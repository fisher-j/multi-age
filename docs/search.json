[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Redwoood multi-age experiment",
    "section": "",
    "text": "1 Introduction\nThis is an analysis of the 10-year remeasure of the Redwood multi-age experiment. It was started by Dr. Pascal Berrill, professor of silviculture at Cal-Poly Humboldt. The experiment is located in the Jackson Demonstration State Forest in Mendocino County, California. The multi-age experiment explores the regeneration response of several species following different harvesting techniques including group selection, aggregated retention, and high/low dispersed retention. The 10-year remeasure data includes surface fuel characterization."
  },
  {
    "objectID": "data_description.html#fuel-data",
    "href": "data_description.html#fuel-data",
    "title": "2  Data description",
    "section": "2.1 Fuel data",
    "text": "2.1 Fuel data\nThe fuel data was gathered in the spring of 2022 and 2023. The Firemon protocol was used as the basis for data collection which in turn employs the use of Brown’s transects, with the addition of duff, litter, and vegetation density sampling stations at two locations along each transect (Section 2.1.3). While the firemon protocol specifies a 2 meter tall imaginary cylinder for quantifying surface fuels, because our resprouting trees were continuous with the ground, we used an imaginary cylinder with a variable height, up to the tallest shrub or sprout fuels that were continuous to the ground.\nTwo 10-meter transects were installed parallel to plot edges at each macro plot corner. Their near ends shared a point inset 10 meters from the corner, towards plot center (Figure 2.1). Downed, woody fuels in four size classes were counted starting from the far end of the transect and each fuel size class was counted for a specified lenght of the total transect (generally longer for larger fuels).\nAt two stations along each transect, duff, litter, live and dead vegetation density (as height and percent cover), and fuel bed depth were assessed. Details of each of these values are described below (Section 2.1.3)\nThe notion of fuel bed depth is notably absent from the Firemon protocol, so we added it by estimating an “average” fuel bed depth within the vegetation sampling cylinder that included litter, and all downed woody debris.\nDominant (well represented) woody speices were also recorded in the vegetation cylinders.\n\n\n\nFigure 2.1: A diagram of the fuel sampling design depicting 0.4 hectare macro plot, fuels transects, transect stations, and vegetation sub-plots. Large green circles are vegetation monitoring plots (which are mostly used for tracking species composition) and the small orange circles are transect stations where duff, litter, and live and dead vegetation density were measured.\n\n\n\n2.1.1 Data entry\nData was entered using Excel and exported as a UTF-8 csv. The file format (Figure 2.2) was defined to resemble the physical datasheets as closely as possible, while maintaining data integretity and facilitating ease of input. There is one file for each macro plot (or replicate = site + treatment combination).\nEach file includes sections for different tables, and these are separated by a line that contains only a single hashtag (e.g., “#duff_litter_fbd”) in lowercase letters with underscores for spaces. In Excel, the #hashtag should go in the first cell of a row, with nothing esle in the row. There should be no blank rows before or after hashtags. The table sections used don’t correlate 1:1 with physical datasheet tables. For instance, columns for “metermeark1” and “metermark2” were added to reflect the fact that initially we sampled duff, litter and vegetation density (transect stations) at 5 and 10 m, but later we moved the stations to 5 and 9 m. The “1” and “2” following repeated variable names refers to one of these locations along the transect. The function used to wrangle these data matches rows based on transect number and columns based on column names. Thus, it is possible to rearrage the rows or columns within each section as long as column names are kept consistent and data for each transect have the same transect number. Following are the section and column labels used for each section of the plot data entry form with any notes regarding their format. A complete description of the data varialbes can be found in Section 2.1.3.\n\n#site_info site treatment\n\ndate\nonehr\ntenhr\nhundhr\nthoushr\ntrans_count notes (site level notes, should not contain commas)\n\n#transects\n\ntransect (number 1 through 8, corresponds with transect column in other sections)\ncorner\nazi\nfwd_crew\nveg_crew (moved from vegetation table to here)\nslope\nmetermark1 (location along transect of first station)\nmetermark2 (location along transect of second station)\nnotes (transect specific notes)\n\n#duff_litter_fbd\n\ntransect (number 1 through 8)\nonehr\ntenhr\nhundhr\nduff_litter1\npct_litter1\nfbd1\nduff_litter2\npct_litter2\nfbd2\n\n#vegetation\n\ntransect (number 1 though 8)\nlive_woody1\ndead_woody1\navg_w_ht1\nlive_herb1\ndead_herb1\navg_h_ht1\nlive_woody2\ndead_woody2\navg_w_ht2\nlive_herb2\ndead_herb2\navg_h_ht2\n\n#woody_species\n\ntransect\nspecies1 (space-delimited list of ‘significant’ species)\nspecies2 (space-delimited list of ‘significant’ species)\n\n#coarse_woody_debris\n\ntransect\ndia\ndecay\n\n\n\n\n\nFigure 2.2: Screenshot of an examle of the datasheet entry format. Each section, defined by a begining hashtag is parsed by an r function as a seperate table.\n\n\nAn R function is used to parse this file into individual tables. As part of this process, all the table sections whose rows have a 1:1 relationship with transects are combined into one wide table, based on the transect column.\n\n\nCode\nsource(\"./scripts/test_funs.r\")\n\n# This function processess one datasheet and returns three tables that can\n# be combined with corresponding tables from other data sheets\nwrangle_datasheet &lt;- function(file) {\n  con &lt;- file(file, encoding = \"UTF-8\")\n  lines &lt;- readLines(con)\n  close(con)\n\n  # Remove BOM &lt;https://stackoverflow.com/a/67906611&gt;\n  lines[1] &lt;- gsub(\"\\\\xef\\\\xbb\\\\xbf\", \"\", lines[1], useBytes = TRUE)\n\n  # These are the sections I want to extract\n  to_get &lt;- c(\n    \"site_data\", \"transects\", \"duff_litter_fbd\", \"vegetation\",\n    \"woody_species\", \"coarse_woody_debris\"\n  )\n  # sections are defined by line with only a hashtage (#section)\n  section_pattern &lt;- \"^#(\\\\w+).*$\"\n\n  # find hashtags and get the data from the next line to the line\n  # before the next hashtag\n  section_breaks &lt;- grep(section_pattern, lines)\n  section_start &lt;- section_breaks + 1\n  section_end &lt;- c(section_breaks[-1], length(lines)) - 1\n  section_names &lt;- gsub(section_pattern, \"\\\\1\", lines[section_breaks])\n\n  sections &lt;- purrr::map2(section_start, section_end, \\(x, y) c(x, y)) |&gt;\n    setNames(section_names) |&gt;\n    (`[`)(to_get) |&gt;\n    purrr::map(\\(x) lines[seq.int(x[1], x[2])]) |&gt;\n    # collapse sections to strings so they can be read as if they were files\n    purrr::map(\\(x) paste(x, collapse = \"\\n\")) |&gt;\n    # leave empty column names so they can be removed\n    purrr::map(\\(x) \n      readr::read_csv(x, show_col_types = FALSE, name_repair = \"minimal\", progress = FALSE)\n    ) |&gt;\n    # Remove empty columns\n    purrr::map(\\(x) x[!names(x) %in% \"\"])\n\n  # I'm going to combine these into a wide table because each row is a transect.\n  # Further data wrangling will require expanding the stations within transecs.\n  # also need to make sure the rows have site data and transect ids for the\n  # coarse woody debris.\n  transect_data &lt;- c(\n    \"transects\", \"duff_litter_fbd\", \"vegetation\", \"woody_species\"\n  )\n\n  transects &lt;- sections |&gt;\n    (`[`)(transect_data) |&gt;\n    purrr::reduce(dplyr::left_join, by = \"transect\") |&gt;\n    # differentiate between transect lenghts and particle counts\n    dplyr::rename_with(\\(x) paste0(x, \"_count\"), ends_with(\"hr\")) |&gt;\n    dplyr::mutate(\n      sections$site_data[c(\"site\", \"treatment\")],\n      .before = corner\n    ) |&gt;\n    dplyr::select(-transect)\n\n  coarse_woody_debris &lt;- sections$coarse_woody_debris |&gt;\n    dplyr::mutate(\n      sections$site_data[c(\"site\", \"treatment\")],\n      .after = transect\n    ) |&gt;\n    dplyr::left_join(\n      sections$transects[c(\"transect\", \"corner\", \"azi\")]\n    ) |&gt;\n    dplyr::select(c(site, treatment, corner, azi, dia, decay))\n  \n  plots &lt;- sections$site_data |&gt;\n    # differentiate between transect lenghts and particle counts\n    dplyr::rename_with(\\(x) paste0(x, \"_length\"), ends_with(\"hr\"))\n\n  # I'll add a check to make sure that all transects and plots are unique\n  warn_duplicates(transects, site, treatment, corner, azi)\n  warn_duplicates(plots, site, treatment)\n\n  # Final output with three tables. These will be combined with corresponding\n  # tables from other datasheets.\n  list(\n    plots = plots,\n    transects = transects,\n    coarse_woody = coarse_woody_debris\n  )\n}\n\ndata_dir &lt;- \"../data\"\n\n# Combine fuels data for each plot\n#\n# This function expects all fuel datasheets to begin with \"fuel\" and end with\n# \"csv\". It loads all matching files in a given folder and returns the same\n# tables as `wrangle_datasheet`, but for all plots combined.\ncombine_fuels_datasheets &lt;- function(data_dir) {\n  files &lt;- list.files(data_dir, pattern = \"^fuel.*csv$\", full.names = TRUE)\n  sheets_list &lt;- purrr::map(files, wrangle_datasheet)\n  table_names &lt;- purrr::set_names(names(sheets_list[[1]]))\n  purrr::map(table_names, \\(x) purrr::list_rbind(purrr::map(sheets_list, x)))\n}\n\n\n# this is how to pivot station data to longer format\n# d |&gt;\n#   tidyr::pivot_longer(\n#     cols = !c(site, treatment, corner, azi),\n#     names_to = \".value\",\n#     names_pattern = \"(\\\\w+)[12]$\"\n#   )\n\n\nThe resulting data looks like this:\n\nwrangle_datasheet(\"../data/fuel_waldon_gs.csv\")\n\nJoining with `by = join_by(transect)`\n\n\n$plots\n# A tibble: 1 × 9\n  site   treatment date   onehr_length tenhr_length hundhr_length thoushr_length\n  &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1 waldon gs        3/15/…            2            2             4             10\n# ℹ 2 more variables: trans_count &lt;dbl&gt;, notes &lt;chr&gt;\n\n$transects\n# A tibble: 8 × 33\n  site   treatment corner   azi fwd_crew veg_crew slope metermark1 metermark2\n  &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 waldon gs        n        135 jf       ac           4          5         10\n2 waldon gs        n        248 jf       ac          19          5         10\n3 waldon gs        w         45 dr       dr           8          5         10\n4 waldon gs        w        135 dr       dr          20          5         10\n5 waldon gs        s         45 dr       jf           5          5         10\n6 waldon gs        s        315 dr       jf           5          5         10\n7 waldon gs        e        225 dr       jf           8          5          9\n8 waldon gs        e        315 dr       jf          13          5          9\n# ℹ 24 more variables: notes &lt;chr&gt;, onehr_count &lt;dbl&gt;, tenhr_count &lt;dbl&gt;,\n#   hundhr_count &lt;dbl&gt;, duff_litter1 &lt;dbl&gt;, pct_litter1 &lt;dbl&gt;, fbd1 &lt;dbl&gt;,\n#   duff_litter2 &lt;dbl&gt;, pct_litter2 &lt;dbl&gt;, fbd2 &lt;dbl&gt;, live_woody1 &lt;dbl&gt;,\n#   dead_woody1 &lt;dbl&gt;, avg_w_ht1 &lt;dbl&gt;, live_herb1 &lt;dbl&gt;, dead_herb1 &lt;dbl&gt;,\n#   avg_h_ht1 &lt;dbl&gt;, live_woody2 &lt;dbl&gt;, dead_woody2 &lt;dbl&gt;, avg_w_ht2 &lt;dbl&gt;,\n#   live_herb2 &lt;dbl&gt;, dead_herb2 &lt;dbl&gt;, avg_h_ht2 &lt;dbl&gt;, species1 &lt;chr&gt;,\n#   species2 &lt;chr&gt;\n\n$coarse_woody\n# A tibble: 20 × 6\n   site   treatment corner   azi   dia decay\n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 waldon gs        n        135     9     5\n 2 waldon gs        n        135    13     2\n 3 waldon gs        n        248    48     3\n 4 waldon gs        n        248    10     5\n 5 waldon gs        w         45    16     5\n 6 waldon gs        w         45    15     5\n 7 waldon gs        w         45    14     5\n 8 waldon gs        w         45    14     5\n 9 waldon gs        w        135    32     3\n10 waldon gs        w        135    12     4\n11 waldon gs        w        135    30     5\n12 waldon gs        s         45    28     4\n13 waldon gs        s         45    42     5\n14 waldon gs        s        315    26     3\n15 waldon gs        e        225    15     4\n16 waldon gs        e        225    13     3\n17 waldon gs        e        225    18     5\n18 waldon gs        e        225    34     3\n19 waldon gs        e        315    26     4\n20 waldon gs        e        315    12     5\n\n\nWhile the wide format for the transect data is not ideal, it is convenient because it reduces the number of tables we are dealing with. This will require extra work at analysis time to expand the data variables which are followed by a “1” or “2”, which represent different stations (Section 2.1.3) on the same transect. To analze these data, they will first need to be pivoted into a longer format, where the station (1 or 2) becomes an explicit column.\n\n\n2.1.2 Data variable descriptions\nThe following is a descripton of the data variables for each of the tables in the data list. So, the $plots heading refers to the table found in data$plots. This data structure is subject to change, but for now, has 3 tables. $plots has plot level data including the lengths of transects (which are the same across all transects, but included for clarity). $transects contains all the fuel data associated with a given transect, including both sampling stations, in wide format (one row for each transect). Finally, $coarse_woody contains coarse woody debris in a long format (multiple rows for each transect).\n\n\n2.1.3 Station sampling cylinder\nReference is made the station sampling cylinder below. It is an imaginary, vertical cylinder with a radius of 1 meter and a variable height equal to the maxiumm height of sprout or shrub vegetation within the cylinders radius. There are two sampling cylinders on each transect and their centers are defined by the transects’ two metermarks. Duff, litter, and fuel bed depth in addition to the vegetation measurements, are all recorded within these cylinders.\n\n\n2.1.4 $plots\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nsite\n\n\n\ntreatment\n\n\n\ndate\n\n\n\nonehr\nDistance from end of transect for which 1-hr fuels were counted\n\n\ntenhr\nDistance from end of transect for which 10-hr fuels were counted\n\n\nhundhr\nDistance from end of transect for which 100-hr fuels were counted\n\n\nthoushr\nDistance from end of transect for which 1000-hr fuels were counted\n\n\ntrans_count\nnumber of transects on macro plot\n\n\nnotes\n\n\n\n\n\n\n2.1.5 $transects\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nsite\nOne of four different sites: whiskey, waldon, waldos, and camp6\n\n\ntreatment\nOne of HD=High density, dispersed retention, LD=Low density dispersed retention, MD=Medium densty dispersed retention, HA=High density aggreagated retention, GS=Group selection opening (1 ha, centered on plot)\n\n\ncorner\nOne of n,s,e,w for “diamond” plost and one of ne, nw, se, sw for “square” plots\n\n\nazi\nactual azimuth from corner to end of fuel transect, deg\n\n\nfwd_crew\nInitials of person performing fuel counts and measuring litter, duff, and FBD\n\n\nveg_crew\nInitials of person estimating vegetation cover\n\n\nslope\nslope in percent, measured with clinometer\n\n\nmetermark1\nlocantion along transect of first veg. station m\n\n\nmetermark2\nlocantion along transect of first veg. station m\n\n\nnotes\nTransect specific notes\n\n\nonehr\nCount of down woody fuels &lt;0.6 cm for the lenght of the 1-hr transect, redwood leaflets less than about 2 mm were not counted as one hour fuels\n\n\ntenhr\nCount of down woody fuels &gt;= 0.6 and &lt; 2.5 cm\n\n\nhundhr\nCount of down woody fuels &gt;= 2.5 and &lt; 8 cm\n\n\nduff_litter1\nCombined duff and litter depth from a representative location within a 1-meter radius circle centered at metermark, cm\n\n\npct_litter1\nPercent of duff_litter depth compose of litter: percent\n\n\nfbd1\nEstimated average height of litter and downed woody debris within the sampling cylinder, 1 m radius, centered at metermark\n\n\nduff_litter2\n\n\n\npct_litter2\n\n\n\nfbd2\n\n\n\nlive_woody1\nTotal projected ground cover of all live woody plant parts within 1-meter-radius sampling cylinder of height equal to the height of the shrub or sprout vegetaion within the cylinders radius, 0-100 percent\n\n\ndead_woody1\nTotal projected ground cover of all dead woody plant parts connected to live or standing dead plants, within the sampling cylinder 0-100 percent\n\n\navg_w_ht1\nAverage maximum height of all live and dead woody plants in sampling cylinder (see Estimating Height in Firemon protocol)\n\n\nlive_herb1\nTotal projected ground cover of live herbs in the sampling cylinder\n\n\ndead_herb1\nTotal projected ground cover of dead herbs in the sampling cylinder\n\n\navg_h_ht1\nAverage maximum height of live and dead herbs in the sampling cylinder\n\n\nlive_woody2\n\n\n\ndead_woody2\n\n\n\navg_w_ht2\n\n\n\nlive_herb2\n\n\n\ndead_herb2\n\n\n\navg_h_ht2\n\n\n\nspecies1\nDominant woody species within sampling cylinder, each species is assumed to occupy an equal portion of volume, under-represented species are ignored\n\n\nspecies2\n\n\n\n\n\n\n2.1.6 $coarse_woody\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nsite\n\n\n\ntreatment\n\n\n\ncorner\n\n\n\nazi\n\n\n\ndia\n\n\n\ndecay\nDecay class 1-5, 1=Fine branches still present, 2=Some branches and bark missing, 3=most branches and much bark missing potentially minor decay, 4=Significant decay, 5=Almost completely rotten"
  },
  {
    "objectID": "calculate_fuel_loading.html#fine-woody-debris",
    "href": "calculate_fuel_loading.html#fine-woody-debris",
    "title": "3  Calculate fuel loading",
    "section": "3.1 Fine woody debris",
    "text": "3.1 Fine woody debris\nThe above information will allow us to calculate fuel loading for fine and coarse woody debris. First we’ll load our data and get the FWD (and the fuel particle parameters) in a long format for easier calculations.\nTwo transects are missing slope. I’m giving them a slope of 0 for now.\n\n\nCode\n# use this to reduce the amount of typing when referring to transects\ntransectid &lt;- c(\"site\", \"treatment\", \"corner\", \"azi\")\n\nfwd &lt;- d$transects |&gt;\n  select(all_of(transectid), slope, matches(\"one|ten|hun\")) |&gt;\n  mutate(slope = if_else(is.na(slope), 0, slope)) |&gt;\n  left_join(select(d$plots, site, treatment, matches(\"one|ten|hun\"))) |&gt;\n  # move onehr, tenhr, etc to new column and create new columns for transect\n  # length and particle counts\n  pivot_longer(\n    matches(\"count|len\"),\n    names_to = c(\"class\", \".value\"),\n    names_sep = \"_\"\n  ) |&gt; \n  left_join(get_particle_params(source = \"glebocki\")) |&gt;\n  mutate(\n    load = simple_load(\n    sum_d2 = count * d2,\n    l = length,\n    percent_slope = slope,\n    G = G,\n    a = a\n    )\n  ) |&gt;\n  select(-c(G, a))"
  },
  {
    "objectID": "calculate_fuel_loading.html#coarse-woody-debris",
    "href": "calculate_fuel_loading.html#coarse-woody-debris",
    "title": "3  Calculate fuel loading",
    "section": "3.2 Coarse woody debris",
    "text": "3.2 Coarse woody debris\nCoarse woody debris is already in a long format so we don’t need to pivot longer, but we will summarize the data for each transect by getting the sum of squared diameters. This differs from the fine woody data because we have actual diameters instead of counts, each diameter corresponds to a single observation.\nWe only have parameters for “sound” and “rotten” particles, so anything over decay class 3 will be considered “rotten”. Finally, we need to join in transect slopes, and transect lengths.\nI’m setting a couple of missing slope to zero.\n\ncwd &lt;- d$coarse_woody |&gt;\n  mutate(class = if_else(decay &gt; 3, \"thoushr_r\", \"thoushr_s\")) |&gt;\n  group_by(site, treatment, corner, azi, class) |&gt;\n  # named count to match fwd table, but these are actually summed d^2\n  summarize(\n    sum_d2 = sum(dia^2),\n    count = n(),\n    med_d = median(dia),\n    .groups = \"drop\"\n  ) |&gt;\n  left_join(d$transects[c(transectid, \"slope\")]) |&gt;\n  mutate(slope = if_else(is.na(slope), 0, slope)) |&gt;\n  left_join(d$plots[c(\"site\", \"treatment\", \"thoushr_length\")]) |&gt;\n  left_join(get_particle_params(source = \"glebocki\")) |&gt;\n  mutate(\n    load = simple_load(\n      sum_d2 = sum_d2,\n      l = thoushr_length,\n      percent_slope = slope,\n      G = G,\n      a = a\n    )\n  ) |&gt;\n  select(-c(G, a, d2), length = thoushr_length)"
  },
  {
    "objectID": "calculate_fuel_loading.html#duff-and-litter",
    "href": "calculate_fuel_loading.html#duff-and-litter",
    "title": "3  Calculate fuel loading",
    "section": "3.3 Duff and litter",
    "text": "3.3 Duff and litter\nWe measured total duff/litter depth, and then estimated a percent of this depth that would be classified as litter. Litter is any leaf material not classified as a 1-hr fuel, that has not yet begun to break down. Particles that were very dark in color and that were broken into smaller pieces than when they had originally fallen were classified as duff.\nDuff and litter were measured at two locations along each transect, for a total of 16 measurements per plot.\nTo convert these depths to load values we use a depth to load equation. Finney and Martin (1993) found a wide variability in the bulk densities of samples, suggesting that simply using the average bulk density should be sufficient, as opposed to calculating bulk densities based on strata depth or differentiating between duff and litter.\n\n\n\n\nTable 3.2: Average depth to load multiplier from different sources in the literature.\n\n\n\n\n\n\n\nSource\nDescription\nLoad Mg ha-1 cm-1\n\n\n\n\nFinney and Martin (1993)\nAnnadel SP & Humboldt Redwoods SP (rw dbh &lt;= 60 in)\n7.15\n\n\nKittredge (1940)\nplantation redwoods\n6.80\n\n\nStuart, J.D. 1985, Unpubl. in Finney and Martin (1993)\nRedwoods SP, (mean of duff & litter)\n9.25\n\n\nJan W. Van Wagtendonk, Benedict, and Sydoriak (1998)\nAvg. for Sierra Nevada conifers\n16.24\n\n\nNives (1989)\nRedwood NP, Lost Man Cr., Redwood Cr.\n2.42\n\n\nKrieger et al. (2020)\nNo ref. cited, 2.75 and 5.5 lbs/ft3 litter and duff resp.\n6.60\n\n\nValachovic et al. (2011)\nTanoak-Douglas-fir, litter only, Humboldt County\n0.93\n\n\n\n\n\n\nOn average, we have about 50% litter and a depth of about 6.2 cm. If we use the mean of the first 3 rows in Table 3.2, an average depth to load multiplier for redwood forests (with 50% litter) is 7.73 Mg ha-1 cm-1.\n\ndufflitter |&gt;\n  group_by(treatment) |&gt;\n  summarize(\n    avg_pct_litter = mean(\n      litter_depth / (duff_depth + litter_depth), \n      na.rm = TRUE\n    ),\n    avg_total_depth = mean(duff_depth + litter_depth),\n)\n\n# A tibble: 4 × 3\n  treatment avg_pct_litter avg_total_depth\n  &lt;chr&gt;              &lt;dbl&gt;           &lt;dbl&gt;\n1 gs                 0.482            6.26\n2 ha                 0.548            5.20\n3 hd                 0.395            7.11\n4 ld                 0.453            5.80\n\ndufflitter &lt;- dufflitter |&gt;\n  mutate(\n    load = (duff_depth + litter_depth) * 7.73\n  )"
  },
  {
    "objectID": "calculate_fuel_loading.html#vegetation",
    "href": "calculate_fuel_loading.html#vegetation",
    "title": "3  Calculate fuel loading",
    "section": "3.4 Vegetation",
    "text": "3.4 Vegetation\nWe based our data collection on the Firemon protocol, which determines vegetative fuel loading by multiplying estimated percent cover by height by a constant bulk densities of 8 and 18 t/ha/m for herbaceous and shrub components, respectively.\nHere I want to standardize the data so that heights are all zero (instead of NA), if percent cover for live and dead were both zero. Also, I want total percent cover, with total proportion dead.\nTheoretically, with the Firemon protocol, total percent cover could be greater than 100, because live and dead percent covers are assessed separately. In practice, sum of live and dead cover was over 100 percent.\n\n\nveg_match &lt;- \"woody|herb|avg_w_ht|avg_h_ht|species\"\n\nveg &lt;- d$transects |&gt;\n  select(all_of(transectid), slope, matches(veg_match)) |&gt;\n  pivot_longer(\n    !c(any_of(transectid), slope),\n    names_to = c(\".value\", \"station\"),\n    names_pattern = \"(\\\\w+)([12])\"\n  ) |&gt; \n  mutate(\n    woody_ht = if_else(live_woody == 0 & dead_woody == 0, 0, avg_w_ht),\n    herb_ht = if_else(live_herb == 0 & dead_herb == 0, 0, avg_h_ht),\n    woody_load = ((live_woody + dead_woody) / 100) * woody_ht * 18,\n    woody_p_dead = if_else(\n      woody_load == 0, 0, dead_woody / (live_woody + dead_woody)\n    ),\n    herb_load = ((live_herb + dead_herb) / 100) * herb_ht * 8,\n    herb_p_dead = if_else(\n      herb_load == 0, 0, dead_herb / (live_herb + dead_herb)\n    )\n  ) |&gt;\n  select(!matches(\"live_|dead_|avg_\"), woody_ht, herb_ht) |&gt;\n  pivot_longer(\n    matches(\"woody|herb\"),\n    names_to = c(\"class\", \".value\"),\n    names_pattern = \"(woody|herb)_(.*)\"\n  )"
  },
  {
    "objectID": "calculate_fuel_loading.html#sec-total-load",
    "href": "calculate_fuel_loading.html#sec-total-load",
    "title": "3  Calculate fuel loading",
    "section": "3.5 Total load",
    "text": "3.5 Total load\nNow we can join results for fine woody debris, coarse woody debris, litter, and duff into a single dataframe.\nVegetation and duff/litter first need to be summarized to the transect level.\n\ndufflitter_load &lt;- dufflitter |&gt; \n  group_by(pick(all_of(transectid))) |&gt; \n  summarize(class = \"dufflitter\", load = mean(load))\n\nveg_load &lt;- veg |&gt;\n  group_by(pick(all_of(transectid)), class) |&gt;\n  summarize(load = mean(load))\n\nfwd_load &lt;- fwd |&gt; select(all_of(transectid), class, load)\ncwd_load &lt;- cwd |&gt; select(all_of(transectid), class, load)\n\ntotal_load &lt;- bind_rows(dufflitter_load, veg_load, fwd_load, cwd_load)|&gt;\n  ungroup()\n\nJohn Stuart measured old-growth forest fuels in in Bull Creek Drainage of Humboldt Redwood State Park, approximately 30 km inland from the coast. Overstory BA was about 66 m2ha-1. Plots were classifed based on their overstory/understory species as one of:\n\nredwood-Douglas-fir/tanoak-evergreen huckleberry\nredwood-Douglas-fir/evergreen huckleberry-tanoak\nDouglas-fir-redwood/evergreen huckleberry\n\nFinney and Martin (1993) measured fuels at two loacations. In Annadel SP with 30-40% slopes and 45 - 60 m2ha-1 of trees approximate 120 years old. At RW SP, BA was between 20 and 60 m2ha-1 that sprouted after harvest and were around 90 years old.\nKittredge (1940) studied duff and litter in redwood plantations with an average total depth of 4 cm.\nValachovic et al. (2011) measured surface fuels in Douglas-fir-tanoak forests from Sonoma to northern Humboldt counteis. The values shown are from across this range.\n\n\nTable 3.3: Summary of fuel loading metrics for this and other studies.\n\n\n\n\n\n\n(a)  \n  \n    \n    \n      \n      source\n      low\n      high\n    \n  \n  \n    litter\nvalachovic\n2.9\n4.7\n    dufflitter\nthisstudy\n40.2\n55.0\n    finney\n29.0\n55.0\n    kittredge\n24.0\n24.0\n    stuart\n35.0\n83.3\n    onehr\nthisstudy\n0.6\n1.2\n    valachovic\n2.0\n3.4\n    stuart\n1.0\n2.4\n    tenhr\nthisstudy\n2.9\n3.7\n    valachovic\n2.5\n6.1\n    stuart\n4.0\n7.3\n    hundhr\nthisstudy\n9.5\n11.8\n    valachovic\n3.1\n7.6\n    stuart\n3.2\n8.5\n    onetenhundhr\nthisstudy\n13.4\n16.1\n    finney\n9.0\n20.0\n    valachovic\n9.0\n15.5\n    stuart\n9.1\n15.1\n  \n  \n  \n\n\n\n\n\n\n\n(b)  \n  \n    \n    \n      \n      source\n      low\n      high\n    \n  \n  \n    thoushr_s\nthisstudy\n27.4\n72.4\n    valachovic\n4.9\n76.7\n    stuart\n5.7\n133.0\n    thoushr_r\nthisstudy\n23.0\n59.0\n    valachovic\n3.1\n35.2\n    stuart\n0.6\n52.9\n    thoushr\nthisstudy\n45.1\n60.6\n    finney\n0.0\n264.0\n    valachovic\n18.3\n85.9\n    stuart\n6.3\n143.1\n    veg_woody\nstuart\n0.1\n6.3\n    NA\nthisstudy\n0.2\n0.5\n    thisstudy\n12.4\n37.8"
  },
  {
    "objectID": "calculate_fuel_loading.html#summary",
    "href": "calculate_fuel_loading.html#summary",
    "title": "3  Calculate fuel loading",
    "section": "3.6 Summary",
    "text": "3.6 Summary\nOur combined duff-litter depths were comparable to other studies, resulting in comparable loading for litter and duff.\nOnehr fuels were lower than then other redwood study and that found in Douglas-fir/tanoak forests, which is somewhat supprising.\nHundhr fuels were higher in our stands. This makes sense given the logging.\nTotal fine fuel loading (onetenhundhr) was similar to other studies, but apprently with more hundhr particles.\nThoushr fuels are notoriously variable. Those on our sites were more consistent and within the middle of the range of other reported values.\nWoody vegetation was much higher than in the one other reported study. That study was in old-growth redwoods. Our values include tree sprout vegetation, which can be several times taller than evergreen huckleberry and much bushier than understory tanoak saplings. Stuart did mention the presence of “nearly inpenetrable [evergreen huckleberry] thickets.” Stuart (1985) found good correlation of live fuels (which included leaves and “twigs”) with basal diamter for both huckleberry and tanoak saplings. The simple scaling factor and/or the occular estimates we used may be biased."
  },
  {
    "objectID": "calculate_fuel_loading.html#save-data",
    "href": "calculate_fuel_loading.html#save-data",
    "title": "3  Calculate fuel loading",
    "section": "3.7 Save data",
    "text": "3.7 Save data\nI’ll Save this data so it can be used in subsequent analysis.\n\ndufflitter, fwd, cwd, and veg, represent mostly raw data in a long format\n\nthis includes two stations per transect for classes for veg and dufflitter\n\ntotal_load is the summarized loading for all classes by transect.\n\n\nsave(dufflitter, fwd, cwd, veg, total_load, file = \"data_long.RData\")\n\n\n\n\n\nBrown, James K. 1974. “Handbook for Inventorying Downed Woody Material.” Gen. Tech. Rep. INT-16. Ogden, UT: US Department of Agriculture, Forest Service, Intermountain Forest and Range Experiment Station. 24 p. 16.\n\n\nFinney, Mark A., and Robert E. Martin. 1993. “Fuel Loading, Bulk Density, and Depth of Forest Floor in Coast Redwood Stands.” Forest Science 39 (3): 617–22.\n\n\nGlebocki, Radoslaw. 2015. “Fuel Loading and Moisture Dynamics in Thinned Coast Redwood Forests in Headwaters Forest Reserve, California.” Master’s thesis, Humboldt State University. https://scholarworks.calstate.edu/concern/theses/ws859j014.\n\n\nKittredge, Joseph. 1940. “A Comparison of Forest Floors from Plantations of the Same Age and Environment.” Journal of Forestry 38 (9): 729–31. https://doi.org/10.1093/jof/38.9.729.\n\n\nKrieger, Raven M., Brian E. Wall, Cody W. Kidd, and John-Pascal Berrill. 2020. “Chronosequence of Fuel Loading and Fuel Depth Following Forest Rehabilitation Frill Treatment of Tanoak to Release Douglas-Fir: A Case Study from Northern California.” Forests 11 (6): 691. https://doi.org/10.3390/f11060691.\n\n\nNives, Sherryl L. 1989. “Fire Behavior on the Forest Floor in Coastal Redwood Forests, Redwood National Park.” PhD thesis, Humboldt State University.\n\n\nStuart, John. 1985. “Redwood Fire Ecology: Final Report Submitted to California Department of Parks and Recreation.” Forestry Department, Humboldt State University.\n\n\nValachovic, Yana S., Christopher A. Lee, Hugh Scanlon, J. Morgan Varner, Radoslaw Glebocki, Bradley D. Graham, and David M. Rizzo. 2011. “Sudden Oak Death-Caused Changes to Surface Fuel Loading and Potential Fire Behavior in Douglas-fir-tanoak Forests.” Forest Ecology and Management 261 (11): 1973–86. https://doi.org/10.1016/j.foreco.2011.02.024.\n\n\nVan Wagner, C. E. 1982. “Practical Aspects of the Line Intersect Method.” PI-X-12. Chalk River, Ontario, Canada: Petawawa National Forestry Institute, Canadian Forestry Service.\n\n\nVan Wagtendonk, J. W., J. M. Benedict, and W. M. Sydoriak. 1996. “Physical Properties of Woody Fuel Particles of Sierra Nevada Conifers.” International Journal of Wildland Fire 6 (3): 117–23. https://doi.org/10.1071/wf9960117.\n\n\nVan Wagtendonk, Jan W., James M. Benedict, and Walter M. Sydoriak. 1998. “Fuel Bed Characteristics of Sierra Nevada Conifers.” Western Journal of Applied Forestry 13 (3): 73–84. https://doi.org/10.1093/wjaf/13.3.73."
  },
  {
    "objectID": "fuel_data_exploration.html#basic-summary",
    "href": "fuel_data_exploration.html#basic-summary",
    "title": "4  Fuel data exploration",
    "section": "4.1 Basic Summary",
    "text": "4.1 Basic Summary\n\n\nCode\n# function to access mostly raw data, but with combined veg and thoushr fuels.\n\nload2 &lt;- function(shape = \"wide\", ...) {\n  load_vars &lt;- c(\"onehr\", \"tenhr\", \"hundhr\", \"dufflitter\", \"thoushr\", \"veg\")\n  tl &lt;- pivot_wider(total_load, names_from = class, values_from = load) |&gt;\n    mutate(\n      thoushr = rowSums(pick(c(thoushr_s, thoushr_r)), na.rm = TRUE),\n      veg = rowSums(pick(c(woody, herb)), na.rm = TRUE),\n      .keep = \"unused\"\n    )\n  if (!missing(...)) tl &lt;- select(tl, ...)  \n  if (shape == \"long\") {\n    tl &lt;- pivot_longer(tl, \n      -any_of(c(\"site\", \"treatment\", \"corner\", \"azi\")),\n      names_to = \"class\", \n      values_to = \"load\"\n    )\n    load_vars &lt;- load_vars[load_vars %in% tl$class]\n    tl &lt;- mutate(tl, class = factor(class, levels = load_vars))\n  }\n  tl\n} \n\n\n\nload2(\"long\", everything()) |&gt;\n  filter(!(load &gt; 190 & class == \"veg\" | load &gt; 400 & class == \"thoushr\")) |&gt;\n  ggplot(aes(treatment, load)) +\n  geom_boxplot() +\n  facet_wrap(~class, scales = \"free\")\n\n\n\n\nFigure 4.1: Two outliers were removed to aide in interpretability–600 and 199 t/ha in ha-thoushr and gs-veg, respectively.\n\n\n\n\nHere is a table summary of our raw data.\n\nspf &lt;- \"%.1f\"\nload2(\"long\", all_of(c(transectid, load_vars))) |&gt;\n  group_by(class, treatment) |&gt;\n  summarize(\n    avg_load = mean(load),\n    sd_load = sd(load)\n  ) |&gt;\n  mutate(\n    load = paste0(sprintf(spf, avg_load), \" (\", sprintf(spf, sd_load), \")\"),\n    .keep = \"unused\"\n  ) |&gt;\n  pivot_wider(names_from = treatment, values_from = load) |&gt;\n  knitr::kable()\n\n`summarise()` has grouped output by 'class'. You can override using the\n`.groups` argument.\n\n\n\n\nTable 4.1: Average (sd) transect level load (Mg ha-1) for six fuel class categories in four different overstory harvest techniques.\n\n\nclass\ngs\nha\nhd\nld\n\n\n\n\nonehr\n0.6 (0.6)\n1.2 (0.8)\n1.0 (0.6)\n0.7 (0.5)\n\n\ntenhr\n3.7 (2.8)\n3.2 (3.2)\n2.9 (1.6)\n3.4 (2.0)\n\n\nhundhr\n11.8 (8.6)\n9.7 (9.0)\n9.5 (7.6)\n9.5 (8.5)\n\n\ndufflitter\n48.4 (31.1)\n40.2 (22.0)\n55.0 (28.9)\n44.9 (18.3)\n\n\nthoushr\n44.9 (57.0)\n41.6 (105.6)\n39.5 (45.1)\n35.3 (52.4)\n\n\nveg\n38.2 (42.3)\n12.9 (14.6)\n17.3 (16.1)\n21.2 (16.0)"
  },
  {
    "objectID": "fuel_data_exploration.html#outliers",
    "href": "fuel_data_exploration.html#outliers",
    "title": "4  Fuel data exploration",
    "section": "4.2 Outliers",
    "text": "4.2 Outliers\n\ncleavland_plot &lt;- function(data, title, load_var = load) {\n  data |&gt;\n    group_by(site, treatment) |&gt;\n    mutate(replicate_mean_load = mean({{load_var}}, na.rm = TRUE)) |&gt;\n    ggplot(\n      aes(\n        {{load_var}},\n        fct_reorder(\n          interaction(site, treatment, sep = \" \"),\n          {{load_var}},\n          .na_rm = TRUE\n        ),\n        color = treatment\n      )\n    ) +\n    geom_jitter(width = 0, height = 0.2) +\n    labs(x = expression(Load~(Mg%.%ha^-1)), y = \"Data order\", title = title)\n}\n\n\n\n\n\n\n\n(a) One hour fuels\n\n\n\n\n\n\n\n(b) Ten hour fuels\n\n\n\n\n\n\n\n(c) Hundred hour fuels\n\n\n\nFigure 4.2: Data distribution of loading for fine woody debris classes. Data are sorted by mean loading within each replicate. Jitter has been added to aid in visual interpretation.\n\n\n\n\n\n\n\nFigure 4.3: Sum of coarse woody (&gt;7.64 cm, sound and rotten wood combined) fuel loading for transects. The y-axis is sorted by mean CWD loading for each replicate.\n\n\n\n\n\n\n\n\n\nFigure 4.4: Combined duff and litter loading at each station along trancects. Y-axis is sorted as in Figure 4.3.\n\n\n\n\n\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n(a) Woody vegetation\n\n\n\n\n\n\n\n(b) herbaceous vegetation\n\n\n\nFigure 4.5: Vegetation fuel loading for each station along transects, including live and dead fuels attached to live vegetation. Y-axis is sorted as in Figure 4.3."
  },
  {
    "objectID": "fuel_data_exploration.html#sec-Normality",
    "href": "fuel_data_exploration.html#sec-Normality",
    "title": "4  Fuel data exploration",
    "section": "4.3 Normality",
    "text": "4.3 Normality\nFor further testing, I will summarize the data somewhat, by combining vegetation loading (woody and herb), and coarse woody loading (sound and rotten) into just two loading metrics. Now we have the following response variables:\n\ndufflitter\nonehr\ntenhr\nhundhr\nthoushr\nveg\n\nWhen using manova to test for difference between groups with multiple response variables, it is important that the response variables are multivariate normally distributed. Unfortunately, it would appear that we have a probelem with normality. The raw data for each loading variable is clearly not normally distributed Figure 4.6.\n\nmyqqplot &lt;- function(data, var) {\n  data |&gt;\n  ggplot(aes(sample = {{ var }})) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_grid(class ~ treatment, scales = \"free\") +\n  labs(\n    x = \"Theoretical quantiles\", y = \"Sample quantiles\",\n    title = \"Normal Q-Q Plot\"\n  )\n}\n\nload2(\"long\", treatment, all_of(load_vars)) |&gt; \nmyqqplot(load)\n\n\n\n\nFigure 4.6: Naive qq plot of loading variables. This doesn’t take into the fact that our data is nested. You could say this is based on a simple model where all observations are independent.\n\n\n\n\n\nbins &lt;- 16\nhist_dat &lt;- load2(\"long\", treatment, all_of(load_vars)) |&gt;\n  drop_na() |&gt;\n  # Facet grid each column (class) has same scale, find limits to calculate bin\n  # width, limits are either implied by the constructed normal curce, or the raw\n  # data\n  group_by(class) |&gt;\n  mutate(\n    xmin = min(c(mean(load) - 3 * sd(load), load)),\n    xmax = max(c(mean(load) + 3 * sd(load), load))\n  ) |&gt;\n  group_by(treatment, class) |&gt;\n  nest(data = load) |&gt;\n  # generate data for a normal curve with mean and sd from observed data to\n  # cover 3 sd.\n  mutate(\n    norm_x = map(data, function (d) {\n      seq(\n        from = mean(d$load) - 3 * sd(d$load), \n        to = mean(d$load) + 3 * sd(d$load), \n        length.out = 100\n      )\n    }),\n    # scale curve to expected binwidth based on plot layout (same x scale across\n    # all fuel classes) multiplied by the number of observations. The histogram\n    # and normal curve should represent the same total area.\n    norm_y = map(data, function (d) {\n      dens &lt;- dnorm(unlist(norm_x), mean = mean(d$load), sd = sd(d$load))\n      dens * ((xmax - xmin) / bins) * nrow(d)\n    })\n  )\n\n\n\n\nCode\n# Break plot into two panels for higher resolution.\nggplot(filter(hist_dat, class %in% load_vars[1:3] )) +\ngeom_histogram(data = \\(x) unnest(x, data), aes(x = load), bins = bins) +\ngeom_line(data = \\(x) unnest(x, c(norm_x, norm_y)), aes(norm_x, norm_y)) +\nfacet_grid(treatment ~ class, scales = \"free\") +\nlabs(y = \"count\", x = \"Load Mg/ha\")\n\nggplot(filter(hist_dat, class %in% load_vars[4:6] )) +\ngeom_histogram(data = \\(x) unnest(x, data), aes(x = load), bins = bins) +\ngeom_line(data = \\(x) unnest(x, c(norm_x, norm_y)), aes(norm_x, norm_y)) +\nfacet_grid(treatment ~ class, scales = \"free\") +\nlabs(y = \"count\", x = \"Load Mg/ha\")\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\nFigure 4.7: Histotrams for the fuel loading response variables. A normal curve with the same mean as sd as the data (and scaled to the same area as that covered by the bins) has been superimposed for reference.\n\n\n\n4.3.1 Box-Cox transformation\nI would like to look at the effect of this transformation on the response data.\n\nboxcox &lt;- function(y, lambda1, lambda2) {\n  if(lambda1 == 0) {\n    ln(y + lambda2)\n  } else {\n    ((y + lambda2)^lambda1 - 1) / lambda1\n  }\n}\n\nload2(\"long\", treatment, all_of(load_vars)) |&gt;\n  group_by(class) |&gt; nest() |&gt; rowwise() |&gt;\n  mutate(\n    lambda = list(suppressMessages(geoR::boxcoxfit(data$load, lambda2 = TRUE)$lambda)),\n    load_bc = list(boxcox(data$load, lambda[1], lambda[2]))\n  ) |&gt;\n  unnest(c(data, load_bc)) |&gt; ungroup() |&gt;\n  myqqplot(load_bc)\n\n\n\n\nWhile, this looks somewhat more normal, the zeros end up being a little strange. I applied a separte transformation for each fuel class, but all treatments within a fuel class have the same transformation.\nFor MANOVA we are concered with the within group multivariate normality, the assumption does not appear to be met here either (Figure 4.8). The code output below indicates the rows with the greatest deviation from normal.\n\n\ntl &lt;- load2(\"wide\", treatment, all_of(load_vars))\nload_mod &lt;- lm(as.matrix(tl[-1]) ~ treatment, data = tl)\nm_dist &lt;- heplots::cqplot(load_mod)\n\nload2(\"wide\", everything())[order(m_dist, decreasing = TRUE)[1:10], ]\n\n# A tibble: 10 × 10\n   site    treatment corner   azi dufflitter onehr  tenhr hundhr thoushr    veg\n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 waldos  ha        nw        90       23.2 1.27  10.7    21.2    593.   16.2 \n 2 waldos  gs        ne       270      131.  1.72   6.46   16.1     36.3 199.  \n 3 whiskey ha        n        225       27.1 2.76  13.3    45.2     32.1   8.5 \n 4 waldos  gs        se       270       54.1 0.504 11.6    34.8    280.    0.9 \n 5 whiskey gs        n        225       27.1 0.124  2.09    8.56     0   121.  \n 6 waldos  ha        se       270      112.  2.32   0.930   2.54     0    46.1 \n 7 waldos  ha        se         0       15.5 3.34   6.53   13.7     17.7   6.31\n 8 waldos  gs        se         0       38.7 1.08   7.31   37.3     17.6   4.23\n 9 waldon  gs        e        225      112.  0.711  8.98   14.7     89.6  34.6 \n10 whiskey ld        s        315       77.3 0.390  5.38   31.9     59.8  74.2 \n\ntreatments = c(\"gs\", \"ha\", \"ld\", \"hd\")\npar(mfrow = c(2, 2))\ninvisible(lapply(treatments, \n  \\(x) heplots::cqplot(filter(tl, treatment == x)[-1], main = x)\n))\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\nFigure 4.8: Plot A assessed the multivariate normality of residuals given the model where all loading variabels are a function of the treatment group.\n\n\nA number of different tests of multivariate normaliy also confirm the lack of evidence for meeting this assumption (Table 4.2).\n\nall_mvn_tests &lt;- function(data) {\n  c(\"mardia\", \"hz\", \"royston\", \"dh\", \"energy\") |&gt;\n  map(\\(x) MVN::mvn(data = data, subset = \"treatment\", mvnTest = x)) |&gt;\n  map(\\(x) x$multivariateNormality) |&gt;\n  map(\\(x) bind_rows(x, .id = \"treatment\")) |&gt;\n  map_dfr(\\(x) \n    filter(x, Test != \"MVN\") |&gt; \n    mutate( across(where(is.factor), \\(f) as.numeric(as.character(f)))) |&gt; \n    select(1:2, statistic = 3, `p value`, Result = last_col())\n  )\n}\n\nall_mvn_tests(load2(\"wide\", treatment, all_of(load_vars))) |&gt;\n  knitr::kable(digits = 4)\n\n\n\nTable 4.2: Several different tests of multivariate normality indicate a lack of evidence to support this assumption.\n\n\ntreatment\nTest\nstatistic\np value\nResult\n\n\n\n\ngs\nMardia Skewness\n156.2386\n0.0000\nNO\n\n\ngs\nMardia Kurtosis\n4.1306\n0.0000\nNO\n\n\nha\nMardia Skewness\n198.9055\n0.0000\nNO\n\n\nha\nMardia Kurtosis\n5.8903\n0.0000\nNO\n\n\nhd\nMardia Skewness\n85.1671\n0.0072\nNO\n\n\nhd\nMardia Kurtosis\n0.7268\n0.4673\nYES\n\n\nld\nMardia Skewness\n122.0012\n0.0000\nNO\n\n\nld\nMardia Kurtosis\n3.2534\n0.0011\nNO\n\n\ngs\nHenze-Zirkler\n1.1979\n0.0000\nNO\n\n\nha\nHenze-Zirkler\n1.2792\n0.0000\nNO\n\n\nhd\nHenze-Zirkler\n1.0978\n0.0001\nNO\n\n\nld\nHenze-Zirkler\n1.1036\n0.0001\nNO\n\n\ngs\nRoyston\n79.7755\n0.0000\nNO\n\n\nha\nRoyston\n105.5921\n0.0000\nNO\n\n\nhd\nRoyston\n51.6106\n0.0000\nNO\n\n\nld\nRoyston\n65.9480\n0.0000\nNO\n\n\ngs\nDoornik-Hansen\n45.8165\n0.0000\nNO\n\n\nha\nDoornik-Hansen\n57.3566\n0.0000\nNO\n\n\nhd\nDoornik-Hansen\n65.4678\n0.0000\nNO\n\n\nld\nDoornik-Hansen\n38.4513\n0.0001\nNO\n\n\ngs\nE-statistic\n1.9038\n0.0000\nNO\n\n\nha\nE-statistic\n2.1409\n0.0000\nNO\n\n\nhd\nE-statistic\n1.6312\n0.0000\nNO\n\n\nld\nE-statistic\n1.7881\n0.0000\nNO\n\n\n\n\n\n\n\n\n4.3.2 Other distributions\nIf our data is not normally distributed, then what distribution is it? I’m going to assume what we are interested in the distribution of data within groups (treatments).\nI attemped to model the distribution of our conditional response data (fuel size class by treatment), but it mostly didn’t work.\nOur data is non-negative (contains zeros) continuous (for the most part) and highly variable in terms of skew and kurtosis. The presence of zeros, makes using the Gamma distribution more difficult. One possibility is a hurdle gamma, or Zero-adjusted Gamma.\n\nd &lt;- load2(\"long\", treatment, all_of(load_vars)) |&gt; \n  split(~class) |&gt; map(~split(.x, ~treatment))\n\n# imap_dfr(d, \\(x, y) tibble(class = y, treatment = names(x) )) |&gt;\n#   with(paste0(\"Treatment \", treatment, \", fuel class \", class)) |&gt;\n#   cat(sep = \"\\n\")\n\n# par(mfrow = c(2,2))\nwalk(d, ~ walk(.x, \\(x) fitdistrplus::descdist(x$load, boot = 111)))\n\n\n\n\n\n\n\n(a) Treatment gs, fuel class onehr\n\n\n\n\n\n\n\n(b) Treatment ha, fuel class onehr\n\n\n\n\n\n\n\n\n\n(c) Treatment hd, fuel class onehr\n\n\n\n\n\n\n\n(d) Treatment ld, fuel class onehr\n\n\n\n\n\n\n\n\n\n(e) Treatment gs, fuel class tenhr\n\n\n\n\n\n\n\n(f) Treatment ha, fuel class tenhr\n\n\n\n\n\n\n\n\n\n(g) Treatment hd, fuel class tenhr\n\n\n\n\n\n\n\n(h) Treatment ld, fuel class tenhr\n\n\n\n\n\n\n\n\n\n(i) Treatment gs, fuel class hundhr\n\n\n\n\n\n\n\n(j) Treatment ha, fuel class hundhr\n\n\n\n\n\n\n\n\n\n(k) Treatment hd, fuel class hundhr\n\n\n\n\n\n\n\n(l) Treatment ld, fuel class hundhr\n\n\n\n\n\n\n\n\n\n(m) Treatment gs, fuel class dufflitter\n\n\n\n\n\n\n\n(n) Treatment ha, fuel class dufflitter\n\n\n\n\n\n\n\n\n\n(o) Treatment hd, fuel class dufflitter\n\n\n\n\n\n\n\n(p) Treatment ld, fuel class dufflitter\n\n\n\n\n\n\n\n\n\n(q) Treatment gs, fuel class thoushr\n\n\n\n\n\n\n\n(r) Treatment ha, fuel class thoushr\n\n\n\n\n\n\n\n\n\n(s) Treatment hd, fuel class thoushr\n\n\n\n\n\n\n\n(t) Treatment ld, fuel class thoushr\n\n\n\n\n\n\n\n\n\n(u) Treatment gs, fuel class veg\n\n\n\n\n\n\n\n(v) Treatment ha, fuel class veg\n\n\n\n\n\n\n\n\n\n(w) Treatment hd, fuel class veg\n\n\n\n\n\n\n\n(x) Treatment ld, fuel class veg\n\n\n\n\nFigure 4.9: Skewness and kurtosis for fuel classes within treatments.\n\n\n\n\n\n4.3.3 Zero-adjusted Gamma\n\n# Zero adjusted Gamma distribution fit to histograms\nplot_zaga &lt;- function(i, d) {\n  m1 &lt;- gamlss::gamlssML(d, family = gamlss.dist::ZAGA())\n  hist(d, prob = TRUE, main = i, breaks = 20)\n  curve(gamlss.dist::dZAGA(x, mu = m1$mu, sigma = m1$sigma, nu = m1$nu), from = min(d), to = max(d), add = TRUE)\n}\n\nplot_zaga_class &lt;- function(data, class_name) {\n  if(missing(class_name)) {\n    class_name &lt;- as.character(substitute(data))\n    class_name &lt;- class_name[length(class_name)]\n  }\n  par(mfrow = c(2,2))\n  iwalk(data, ~plot_zaga(.y, .x))\n  mtext(class_name, cex = 1.6, side = 3, line = -2, outer = TRUE)\n}\n\nd2 &lt;- d |&gt; map(~ map(.x, \"load\"))\niwalk(d2, ~plot_zaga_class(.x, .y))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.4 Poisson fit of count data\nAll of the woody debris can be viewed as count data, and mean diameter. The mean diameter implies a distribution, which we actually have for coarse woody, but not for FWD.\nI wonder If I can model these counts as a Poisson process.\n\ncwd_counts &lt;- cwd |&gt; \n  group_by(site, treatment, corner, azi) |&gt;\n  summarize(count = sum(count), .groups = \"drop\")\n\npar(mfrow = c(2,2))\ncwd_counts |&gt;\n  group_by(treatment) |&gt;\n  group_walk( function (data, group) {\n    n &lt;- data$count\n    hist(n, main = group, prob = TRUE)\n    lines(0:max(n), dpois(0:max(n), mean(n)))\n  })\n\n\n\n\nThat didn’t look so great, so I also tried using a Zero-inflated Poisson distribution, but the model fit an extremely small value for the parameter that controls the probability of zero (referred to here as sigma) and so was effectively the same as the Poisson fit.\n\npar(mfrow = c(2,2))\ncwd_counts |&gt; \n  group_by(treatment) |&gt; nest() |&gt;\n  transmute(\n    sigma = map_dbl(data, \n      ~gamlss::gamlssML(.x$count, family = gamlss.dist::ZIP())$sigma\n    )\n  )\n\n# A tibble: 4 × 2\n# Groups:   treatment [4]\n  treatment    sigma\n  &lt;chr&gt;        &lt;dbl&gt;\n1 gs        2.61e-10\n2 ha        1.61e-10\n3 hd        2.42e-10\n4 ld        2.71e-10"
  },
  {
    "objectID": "fuel_data_exploration.html#sec-homogeneity-of-variance",
    "href": "fuel_data_exploration.html#sec-homogeneity-of-variance",
    "title": "4  Fuel data exploration",
    "section": "4.4 Homogeneity of variance",
    "text": "4.4 Homogeneity of variance\nThere seem to be some pretty big differences in the variance between treatments. This is likely to do with outliers. For linear regression, it is recommended that maximum variance ration should be below 4.\n\nmax_var &lt;- load2(\"long\", everything()) |&gt;\n  group_by(class, treatment) |&gt;\n  summarize(var = sd(load)^2, load = max(load), .groups = \"drop_last\") |&gt;\n  summarize(\n    max_var_rat = paste(\"Max. var. ratio: \", round(max(var) / min(var))),\n    x = 1.1, y = max(load) * 1.1, .groups = \"drop\")\n\nload2(\"long\", everything()) |&gt;\n  ggplot(aes(treatment, load)) +\n  geom_boxplot() +\n  geom_text(data = max_var, aes(x, y, label = max_var_rat, hjust = \"inward\")) +\n  facet_wrap(~class, scales = \"free\")"
  },
  {
    "objectID": "fuel_data_exploration.html#zeros",
    "href": "fuel_data_exploration.html#zeros",
    "title": "4  Fuel data exploration",
    "section": "4.5 Zeros",
    "text": "4.5 Zeros\nWe do have zeros, which is important if we want to employ a glm like Gamma, which is only defined for positive values.\n\nload2(\"long\", everything()) |&gt;\n  group_by(class, treatment) |&gt;\n  summarize(zeros = sum(load == 0), percent = zeros / n(), .groups = \"drop\") |&gt;\n  ggplot(aes(treatment, percent)) +\n  geom_col() +\n  geom_text(\n    aes(label = if_else(zeros == 0, NA, zeros), y = percent / 2),\n    color = \"gray70\", na.rm = TRUE\n  ) +\n  facet_wrap(~class) +\n  scale_y_continuous(labels = scales::percent)"
  },
  {
    "objectID": "fuel_data_exploration.html#correlation-of-response-variables",
    "href": "fuel_data_exploration.html#correlation-of-response-variables",
    "title": "4  Fuel data exploration",
    "section": "4.6 Correlation of response variables",
    "text": "4.6 Correlation of response variables\nI’m not sure if it’s important, but I was curious if the various fuel loading classes were correlated with each other. Either across the board, or within a given treatment.\n\nsuppressMessages(GGally::ggpairs(load2(\"wide\", all_of(load_vars))))\n\n\n\n\nFigure 4.10: Correlation among the response variables (fuel classes)."
  },
  {
    "objectID": "fuel_data_exploration.html#independence",
    "href": "fuel_data_exploration.html#independence",
    "title": "4  Fuel data exploration",
    "section": "4.7 Independence",
    "text": "4.7 Independence\nBecause of how are data were collected they are not independent. The current data is summarized at the transect level. At that level. We have two transects at each corner. Because of spatial autocorrelation, these may be correlated with each other. Corners (and thus transects) are nested within plots, and plots are within treatments. Each plot received a different treatment. What I’m not clear about is: should I include a random variable for plots, if I’m including a fixed effect for treatment?\nThere are also question about at what level to summarize/model the data. I’ve already averaged stations within transects for several variable that were collected at the station level (two within each transect). What are the trade-offs for either averaging at the corner level, or alternatively, analyzing our raw station data instead of averaging.\n\n\n\n\nZuur, Alain F., Elena N. Ieno, and Chris S. Elphick. 2010. “A Protocol for Data Exploration to Avoid Common Statistical Problems.” Methods in Ecology and Evolution 1 (1): 3–14. https://doi.org/10.1111/j.2041-210X.2009.00001.x."
  },
  {
    "objectID": "hypothesis_test.html",
    "href": "hypothesis_test.html",
    "title": "5  Hypothesis testing",
    "section": "",
    "text": "6 TODO:"
  },
  {
    "objectID": "hypothesis_test.html#manova-and-multiple-anovas",
    "href": "hypothesis_test.html#manova-and-multiple-anovas",
    "title": "5  Hypothesis testing",
    "section": "5.1 Manova and multiple anovas",
    "text": "5.1 Manova and multiple anovas\nThe often recommended Pillai’s Trace Test is robust to the normality assumption. Follow up with linear discriminant analysis, or multiple one-way anovas dependidng on research question. Using a Bonferroni correction for rejecting the null of alpha / m, for m hypothesis, we get an alpha of 0.008 for an alpha of 0.05 and 6 tests.\nThis suggests that it is unlikely that all treatemnts are equal.\n\ntl &lt;- load2(\"wide\", treatment, all_of(load_vars))\nmyexpr &lt;- expr(cbind(!!!syms(load_vars)) ~ treatment)\ntest1 &lt;- do.call(\"manova\", list(myexpr, data = quote(tl)))\nsummary(test1)\n\n           Df  Pillai approx F num Df den Df    Pr(&gt;F)    \ntreatment   3 0.35947   2.7454     18    363 0.0001889 ***\nResiduals 124                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "hypothesis_test.html#multiple-one-way-anovas",
    "href": "hypothesis_test.html#multiple-one-way-anovas",
    "title": "5  Hypothesis testing",
    "section": "5.2 Multiple one-way anovas",
    "text": "5.2 Multiple one-way anovas\nOne way anova (using the welch test) can either assume constant variance or not. A levene test (using median) indicates onehr, tenhr, and veg may all have different variances between groups.\nThe one-way anova test results are the same though between equal and unequal variance assumptions. These tests support the notion that we can’t assume that the mean vegetatvie and onehr fuel loading are equal across all treatments, but there isn’t such evidence for the other fuel loading classes.\n\nd &lt;- load2(\"long\", treatment, all_of(load_vars)) |&gt; group_by(class)\n\nd |&gt; nest() |&gt; rowwise() |&gt;\n  transmute(\n    levene = car::leveneTest(load ~ factor(treatment), data)[[3]][1],\n    welch_uneq_var = oneway.test(load ~ treatment, data)$p.value,\n    welch_eq_var = oneway.test(load ~ treatment, var.equal = TRUE, data = data)$p.value,\n  ) |&gt;\n  knitr::kable(digits = 3)\n\n\n\nTable 5.1: Levene tests suggest that variances are unequal across treatments for all fuel loading classes. Welches tests suggest that veg and onehr fuels may have different means among treatments.\n\n\nclass\nlevene\nwelch_uneq_var\nwelch_eq_var\n\n\n\n\nonehr\n0.043\n0.001\n0.000\n\n\ntenhr\n0.020\n0.491\n0.596\n\n\nhundhr\n0.937\n0.648\n0.636\n\n\ndufflitter\n0.118\n0.152\n0.136\n\n\nthoushr\n0.955\n0.919\n0.955\n\n\nveg\n0.006\n0.010\n0.001\n\n\n\n\n\n\nWe can use the Games Howell test for pairwise comparisons to follow up on the welches test for differences between means when there is unequal variance among groups. These p-values provide evidence that for onehr fuels, the mean value of ha is greater than gs and ld, and the mean value for hd is also greater than gs and ld. Also, for vegetation, gs is greater than ha only. While this test is robust to the assumptions of normality, some of our data is highly skewed. Also, because of the nesting of our data, observations are not independent, so our effective sample size is not what is assumed by this test.\n\ngh_test &lt;- d |&gt; rstatix::games_howell_test(load ~ treatment) |&gt;\n  filter(p.adj.signif != \"ns\") |&gt;\n  rstatix::add_y_position(scales = \"free\", step.increase = 0.5)\nggpubr::ggboxplot(d, x = \"treatment\", y = \"load\", facet.by = \"class\") +\nfacet_wrap(~class, scales = \"free\") +\nggpubr::stat_pvalue_manual(gh_test, label = \"p.adj\") +\nscale_y_continuous(expand = expansion(mult = c(0.05, 0.1)))\n\n\n\n\nFigure 5.1: pair-wise tests using Games-Howell, for unequal variances across groups. This shows many statistically significant differences, but the assumption of independence, which is likely to have a significant effect on our effective sample size."
  },
  {
    "objectID": "hypothesis_test.html#sec-multi-level-model",
    "href": "hypothesis_test.html#sec-multi-level-model",
    "title": "5  Hypothesis testing",
    "section": "5.3 Multi-level model",
    "text": "5.3 Multi-level model\nWe have transects nested within plot corners, corners nested within plots, and plots nested within sites. We would like to detect a treatment effect, while accounting for the non-independence of this nested data structure. The following model, I believe, captures this grouping structure.\n\nform &lt;- load ~ treatment + (1 | site/treatment/corner)\n\nThis will estimate a group-wise intercept adjustments for each site, plot, and corner, based on modeled variances for each of these grouping levels.\n\nd &lt;- load2(\"long\", site, treatment, corner, all_of(load_vars)) |&gt; \n  group_by(class)\n\nm1 &lt;- d |&gt; nest() |&gt; rowwise() |&gt;\n  transmute(\n    mod = list(lme4::lmer(form, data = data)),\n    emmeans = list(emmeans::emmeans(mod, \"treatment\")),\n    pairs = list(as_tibble(pairs(emmeans, infer = TRUE)))\n  )\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nPairwise comparisons with Tukey adjustment for each of 6 multilevel models representing different fuel loading classes reveals that the only evidence for differences in means among treatments is with vegetation between the gs and ha treatments. Another sizeable difference in means is between gs and ha for the onehr fuels (Figure 5.2).\n\nselect(m1, pairs) |&gt; unnest(pairs) |&gt;\n  filter(p.value &lt;= 0.05) |&gt;\n  knitr::kable(digits = 3)\n\nAdding missing grouping variables: `class`\n\n\n\n\nTable 5.2: Pairwise comparisons among treatments with p-values &lt; 0.05 for 6 multilevel models. Only Veg, gs-ha comparison is statistically significant.\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nveg\ngs - ha\n25.31\n6.879\n9\n3.835\n46.786\n3.679\n0.022\n\n\n\n\n\n\n\ngroup_map(m1, ~ plot(.x$emmeans[[1]], comparisons = TRUE) + ggtitle(.x$class)) |&gt;\n  patchwork::wrap_plots()\n\n\n\n\nFigure 5.2: 95% confidence intervals and pairwise comparisons of means for 6 mixed models representing different fuel loading classes using package emmeans.\n\n\n\n\nHypothesis testing with multi-level models is not as straight forward with multi-level models. The problem, explained here is two fold. For GLMMs and unbalanced experimental designs, the null distribution for the F-statistic may not be F-distributed.\nFor us, we have a balanced design (I think) and so the F-statistic should be F distributed and degrees of freedom should be clear from the details of the design. Because of our balanced design, the Kenward-Rogers approach and “inner-outer” design approach (which is used by nlme::lme) give the same result of 9.\nUsing the package pbkrtest we can get parametric bootstrap liklihood ratio statistics and test this statistic in a number of different ways. The PBtest should probably be the most reliable, but I’ve included descriptions of the others from the package documentation for reference. I’m also including an F-test in which degrees of freedom are estimated with Kenward-Rogers approach.\n\nLRT\n\nAssuming that LRT has a chi-square distribution.\n\nPBtest\n\nThe fraction of simulated LRT-values that are larger or equal to the observed LRT value.\n\nBartlett\n\nA Bartlett correction is of LRT is calculated from the mean of the simulated LRT-values\n\nGamma\n\nThe reference distribution of LRT is assumed to be a gamma distribution with mean and variance determined as the sample mean and sample variance of the simulated LRT-values.\n\nF\n\nThe LRT divided by the number of degrees of freedom is assumed to be F-distributed, where the denominator degrees of freedom are determined by matching the first moment of the reference distribution.\n\n\n\nd &lt;- load2(\"long\", all_of(c(transectid, load_vars)))\ndd &lt;- ungroup(d) |&gt; split(~class)\n\n\nif(file.exists(\"pmod.Rdata\")) {\n  load(\"pmod.Rdata\")\n} else {\n  cluster &lt;- parallel::makeCluster(rep(\"localhost\", parallel::detectCores()))\n\n  pmod &lt;- imap(dd, function(d, i) {\n    form &lt;- load ~ treatment + (1 | site/treatment/corner)\n    amod &lt;- lme4::lmer(form, d, REML = FALSE)\n    nmod &lt;- update(amod, . ~ . -treatment)\n    krtest &lt;- pbkrtest::KRmodcomp(amod, nmod) |&gt;\n      pluck(\"test\", \\(x) slice(x, 1)) |&gt;\n      rename(df = ndf) |&gt;\n      rownames_to_column(\"test\")\n    pbkrtest::PBmodcomp(amod, nmod, cl = cluster) |&gt;\n      pluck(summary, \"test\") |&gt;\n      rownames_to_column(\"test\") |&gt;\n      bind_rows(krtest) |&gt;\n      mutate(class = i, .before = 1) |&gt;\n      relocate(c(df, ddf, F.scaling), .after = stat)\n  }) |&gt; \n    list_rbind() |&gt;\n    filter(test %in% c(\"LRT\", \"PBtest\"))\n \n  parallel::stopCluster(cluster)\n  save(pmod, file = \"pmod.Rdata\")\n}\n\npmod |&gt; knitr::kable(digits = c(NA, NA, 2, 1, 2, 1, 4))\n\n\n\nTable 5.3: Liklihood ratio tests and parametric boot strap tests of model significance: whether the model with treatment, fits the data better than the intercept only model (adjusting for nesting structure).\n\n\nclass\ntest\nstat\ndf\nddf\nF.scaling\np.value\n\n\n\n\nonehr\nLRT\n7.48\n3\nNA\nNA\n0.0580\n\n\nonehr\nPBtest\n7.48\nNA\nNA\nNA\n0.1608\n\n\ntenhr\nLRT\n0.77\n3\nNA\nNA\n0.8576\n\n\ntenhr\nPBtest\n0.77\nNA\nNA\nNA\n0.8942\n\n\nhundhr\nLRT\n0.70\n3\nNA\nNA\n0.8725\n\n\nhundhr\nPBtest\n0.70\nNA\nNA\nNA\n0.9201\n\n\ndufflitter\nLRT\n6.05\n3\nNA\nNA\n0.1093\n\n\ndufflitter\nPBtest\n6.05\nNA\nNA\nNA\n0.1129\n\n\nthoushr\nLRT\n0.22\n3\nNA\nNA\n0.9739\n\n\nthoushr\nPBtest\n0.22\nNA\nNA\nNA\n0.9747\n\n\nveg\nLRT\n11.70\n3\nNA\nNA\n0.0085\n\n\nveg\nPBtest\n11.70\nNA\nNA\nNA\n0.0380"
  },
  {
    "objectID": "hypothesis_test.html#model-checking",
    "href": "hypothesis_test.html#model-checking",
    "title": "5  Hypothesis testing",
    "section": "5.4 Model checking",
    "text": "5.4 Model checking\nTaking a look at residual vs. fitted and qqplots of the model, it looks like our residuals are not normally distributed and there is not constant variance.\n\n\nCode\n# These are functions to plot for each model, residuals vs fitted and normal\n# quantiles. The third function is a wrapper to do both.\nresid_plot &lt;- function(data) {\n  data |&gt;\n    ggplot(aes(fitted, resid)) +\n    geom_point() +\n    facet_wrap(~class, scales = \"free\") +\n    geom_hline(yintercept = 0)\n}\n\nqq_plot &lt;- function(data) {\n  data |&gt;\n    ggplot(aes(sample = resid)) +\n    stat_qq() +\n    stat_qq_line() +\n    facet_wrap(~class, scales = \"free\")\n}\n\nresid_qq_plot &lt;- function(data) {\n  data &lt;- unnest(data, c(resid, fitted))\n  list(\n    a = resid_plot(data),\n    b = qq_plot(data)\n  )\n}\n\nd &lt;- load2(\"long\", all_of(c(transectid, load_vars))) |&gt;\n  group_by(class) |&gt; nest() |&gt; rowwise()\n\n\n\nform &lt;- load ~ treatment + (1 | site/treatment/corner)\n\nmod1 &lt;- d |&gt;\n  mutate(\n    mod = list(lme4::lmer(form, data)),\n    fitted = list(fitted(mod)),\n    resid = list(resid(mod, type = \"pearson\", scaled = TRUE)),\n    .keep = \"unused\"\n  ) \n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\nresid_qq_plot(mod1) |&gt; patchwork::wrap_plots(ncol = 1)\n\n\n\n\nFigure 5.3: Residual vs fitted and normal quantile-quantile plots for a multi-level model with un-pooled treatment intercepts and partially pooled (random effects) for nested data. Fit using lme4 The residuals are not homogenous.\n\n\n\n\nI’ll try to control the variance by refitting the model with nlme::lme and using the weights argument. I’ll be using the pearson residuals which are corrected for heteroscedasticity.\nI had to use the control argument sigma = 1 for the model to fit. I’m not sure why, I read it in the documentation for nlme::varConstProp. I’m modeling variance as a constant proportion of the fitted values of the model. This seems to have cleaned up the variance.\n\nmod2 &lt;- d |&gt;\n  mutate(\n    mod = list(nlme::lme(\n      fixed = load ~ treatment,\n      random = ~ 1 | site/treatment/corner,\n      data = data,\n      weights = nlme::varConstProp(),\n      control = nlme::lmeControl(sigma = 1)\n    )),\n    fitted = list(fitted(mod)),\n    resid = list(resid(mod, type = \"pearson\")),\n    .keep = \"unused\"\n  )\n\npatchwork::wrap_plots(resid_qq_plot(mod2), ncol = 1)\n\n\n\n\nFigure 5.4: Same as Figure 5.3 but variance is modeled as a function fitted values, assuming a linear relationship. Fit with nlme. The (scaled) residuals are more homogenous now.\n\n\n\n\nI’ll compare AIC of the two models to see if one performs better than the other.\nfirst, I want to see if the models produced by lme and lmer are equivalent\n\nd |&gt;\n  mutate(\n    mod1 = list(lme4::lmer(form, data)),\n    mod2 = list(nlme::lme(\n      fixed = load ~ treatment,\n      random = ~ 1 | site/treatment/corner,\n      data = data\n    )),\n    .keep = \"unused\"\n  ) |&gt;\n  pivot_longer(-class, names_to = \"model\") |&gt;\n  rowwise() |&gt;\n  mutate(s = list(broom.mixed::tidy(value, effect = \"fixed\"))) |&gt;\n  select(class, model, s) |&gt;\n  unnest(everything()) |&gt;\n  arrange(class, term)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\n# A tibble: 48 × 9\n   class model effect term        estimate std.error statistic    df    p.value\n   &lt;fct&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 onehr mod1  fixed  (Intercept)   0.598      0.191     3.13     NA NA        \n 2 onehr mod2  fixed  (Intercept)   0.598      0.191     3.13     64  0.00265  \n 3 onehr mod1  fixed  treatmentha   0.603      0.256     2.35     NA NA        \n 4 onehr mod2  fixed  treatmentha   0.603      0.256     2.35      9  0.0433   \n 5 onehr mod1  fixed  treatmenthd   0.439      0.256     1.71     NA NA        \n 6 onehr mod2  fixed  treatmenthd   0.439      0.256     1.71      9  0.121    \n 7 onehr mod1  fixed  treatmentld   0.0697     0.256     0.272    NA NA        \n 8 onehr mod2  fixed  treatmentld   0.0697     0.256     0.272     9  0.792    \n 9 tenhr mod1  fixed  (Intercept)   3.75       0.784     4.78     NA NA        \n10 tenhr mod2  fixed  (Intercept)   3.75       0.784     4.78     64  0.0000106\n# ℹ 38 more rows\n\n\nthey seem equivalent enough, although the random effects variances estimated by lmer are somewhat smaller. Now, lets compare the two lme models. I’m fitting with REML because I’m not changing the fixed effects structure.\n\nmod_c &lt;- d |&gt;\n  mutate(\n    mod1 = list(nlme::lme(\n      fixed = load ~ treatment,\n      random = ~ 1 | site/treatment/corner,\n      data = data,\n    )),\n    mod2 = list(nlme::lme(\n      fixed = load ~ treatment,\n      random = ~ 1 | site/treatment/corner,\n      data = data,\n      weights = nlme::varConstProp(),\n      control = nlme::lmeControl(sigma = 1),\n    )),\n    .keep = \"unused\"\n  ) \n\nmod_c |&gt;\n  mutate(aic = list(across(starts_with(\"mod\"), ~ AIC(.x))))|&gt;\n  select(aic)|&gt; unnest(aic)\n\nAdding missing grouping variables: `class`\n\n\n# A tibble: 6 × 3\n# Groups:   class [6]\n  class       mod1  mod2\n  &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 onehr       252.  220.\n2 tenhr       590.  485.\n3 hundhr      892.  861.\n4 dufflitter 1180. 1178.\n5 thoushr    1429. 1421.\n6 veg        1171. 1122.\n\n\nThis indicates that the model with modeled variance fits the data better than the model without.\nDoes this change our conclusions about the effect of the treatment?\n\nd &lt;- load2(\"long\", site, treatment, corner, all_of(load_vars)) |&gt; \n  group_by(class)\n\nm2 &lt;- d |&gt; nest() |&gt; rowwise() |&gt;\n  transmute(\n    mod = list(nlme::lme(\n      fixed = load ~ treatment,\n      random = ~ 1 | site/treatment/corner,\n      data = data,\n      weights = nlme::varConstProp(),\n      control = nlme::lmeControl(sigma = 1),\n    )),\n    emmeans = list(emmeans::emmeans(mod, \"treatment\")),\n    pairs = list(as_tibble(pairs(emmeans, infer = TRUE)))\n  )\n\n\nselect(m2, pairs) |&gt; unnest(pairs) |&gt;\n  filter(p.value &lt;= 0.05) |&gt;\n  knitr::kable(digits = 3)\n\nAdding missing grouping variables: `class`\n\n\n\n\nTable 5.4: Pairwise comparisons among treatments with p-values &lt; 0.05 for 6 multilevel models using a model with variance modeled as a linear relationsihp with the fitted value.\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nveg\ngs - ha\n25.31\n7.12\n9\n3.083\n47.538\n3.555\n0.026\n\n\n\n\n\n\n\ngroup_map(m2, ~ plot(.x$emmeans[[1]], comparisons = TRUE) + ggtitle(.x$class)) |&gt;\n  patchwork::wrap_plots()\n\n\n\n\nFigure 5.5: 95% confidence intervals and pairwise comparisons of means for 6 mixed models representing different fuel loading classes using a model with variance modeled as a linear relationsihp with the fitted value.\n\n\n\n\nNo, it doesn’t seem to make much of a difference."
  },
  {
    "objectID": "hypothesis_test.html#other-random-effects-structures",
    "href": "hypothesis_test.html#other-random-effects-structures",
    "title": "5  Hypothesis testing",
    "section": "5.5 Other random effects structures",
    "text": "5.5 Other random effects structures\nI’m not sure I’m using the correct random effects specification. The somewhat confusing thing is that I have a random effects nested above and below my fixed effect. This means that when I specify my random effect using the nesting notation: 1 | site/treatment/corner, I’m estimating a variance for corner:treatment:site, treatment:site, and site. The interaction of treatment and site here is analagous to a plot effect, of which there are 16."
  },
  {
    "objectID": "hypothesis_test.html#bayesian-mode",
    "href": "hypothesis_test.html#bayesian-mode",
    "title": "5  Hypothesis testing",
    "section": "5.6 Bayesian mode",
    "text": "5.6 Bayesian mode\nI’ll use mostly brms defaults (notably, priors, which is not recommended) to do a basic bayesian analysis, using the same formula I used for the lmm above:\n\n5.6.1 Data\nAll the data is nested to facilitate modeling each fuel class separately. We’ll look at the average loading to get an idea of the data.\n\nd &lt;- load2(\"long\", all_of(c(transectid, load_vars))) |&gt;\n  group_by(class) |&gt; nest() |&gt; rowwise()\n\n\n\n5.6.2 Model formula\nWe’re modeling intercepts for each treatment seperately with no baseline intercept. The partilly pooled interecepts (site/treatment/corner) are based on the way the data was collected. Transects within corners, corners within plots (combination of site and treatment) and plots within sites.\n\nform &lt;- load ~ treatment - 1 + (1 | site/treatment/corner)\n\n\n\n5.6.3 Priors\nWe are using mostly uninformative priors for our un-pooled (fixed) estimates of treatment intercepts. They are all set as normal distributions, centered at the median of the fuel load with a sd of 2.5 times the sd of the data. They are contrained to be positive, as it’s not possible to have a negative fuel load.\n\n# add calcualted priors to the data\nd &lt;- mutate(d, \n  priors = list(brms::set_prior(\n    str_glue(\n    \"normal( {round(median(data$load))}, {round(2.5 * sd(data$load))} )\"), \n    lb = 0\n  ))\n)\n\n# Plot the priors\nd |&gt;\n  mutate(\n    prior_dist = list(select(tidybayes::parse_dist(priors), .dist_obj))\n  ) |&gt;\n  unnest(prior_dist) |&gt;\n  ggplot(aes(xdist = .dist_obj)) +\n    tidybayes::stat_halfeye(normalize = \"panels\") +\n    geom_text(aes(label = format(.dist_obj), x = 0, y = 0.97, hjust = 0)) +\n    facet_wrap(~ class, scales = \"free_x\")\n\n\n\n\nFigure 5.6: Normal priors centered at the median and sd of 2.5 times the sd of the data\n\n\n\n\n\n\n5.6.4 Model fitting\nNow we’ll fit the model\n\nif (file.exists(\"bf2.Rdata\")) {\n  load(\"bf2.Rdata\")\n} else {\n  bf2 &lt;- mutate(d,\n    mod = list(brms::brm(form, data,\n      warmup = 3000,\n      iter = 4000,\n      cores = 4,\n      control = list(adapt_delta = 0.99),\n      prior = priors\n    ))\n  )\n  save(bf2, file = \"bf2.Rdata\")\n}\n\n\n\n5.6.5 Expected predictions\n\n\n\n\n5.6.6 Posterior Predictive check (Gaussian)\n\n\nCode\n# this allows to adjust each panels coordinate limits\nsource(\"./scripts/coord_cartesian_panels.r\")\n\nposterior_predictive_check &lt;- function(models) {\n  models &lt;- mutate(models,\n    pred = list(tidybayes::add_predicted_draws(data, mod, ndraws = 15)),\n    lims = list(tibble(\n      xmin = min(\n        quantile(data$load, .0001),\n        quantile(pred$.prediction, .0001)\n      ),\n      xmax = max(\n        quantile(data$load, .9999),\n        quantile(pred$.prediction, .9999)\n      ))\n    )\n  )\n  ggplot() + \n  geom_line(\n    data = unnest(models, data),\n    aes(x = load),\n    stat = \"density\", size = 1\n  ) +\n  geom_line(\n    data = unnest(models, pred),\n    aes(x = .prediction, group = .draw),\n    stat = \"density\", alpha = 0.15, size = 1\n  ) +\n  facet_wrap(~class, scales = \"free\") +\n  coord_cartesian_panels(\n    panel_limits = unnest(select(models, lims), lims)\n  )\n}\n\n\nHere is the default posterior predictive check provided by the bayesplot package. Each light curve is a predicted replication of the original, observed response (a predicted transect).\n\nposterior_predictive_check(bf2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nAdding missing grouping variables: `class`\n\n\n\n\n\nFigure 5.7: Density of the observed data (y) plotted against 10 random draws from the posterior predictive distribution.\n\n\n\n\nThe Gaussian distribution is symmetric and doesn’t capture well the peak near zero and the long right tail of our observed values for most of the fuel classes. It also dramatically overpredicts negative values (which are absent from our data, despite the fact that the density smoothing of the observed values seems to suggest there are some.)\nThe fact that the model predicts negative values suggests that it is not right for our data, and could potentially be biasing comparissons between treatments. A Gamma distribution for the response makes more sense.\n\n\n5.6.7 Gamma model\n\nif (file.exists(\"bf3.Rdata\")) {\n  load(\"bf3.Rdata\")\n} else {\n  bf3 &lt;- mutate(d,\n    mod = list(brms::brm(form, data,\n      warmup = 10000,\n      iter = 11000,\n      cores = 4,\n      control = list(adapt_delta = 0.99),\n      family = brms::hurdle_gamma(),\n      prior = priors\n    ))\n  )\n  save(bf3, file = \"bf3.Rdata\")\n}\n\nWarning: There were 8 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `mod = list(...)`.\nℹ In row 3.\nCaused by warning:\n! There were 2 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\nℹ Run `dplyr::last_dplyr_warnings()` to see the 7 remaining warnings.\n\n\nThe model fit without convergence problems. I used the same priors as before, but with a hurdle gamma response. This fits one model for the zeros and another for the non-zero values, which are expected to be gamma distributed, that is, positive and right skewed.\n\n\n5.6.8 Posterior Predictive check (Gamma)\n\nposterior_predictive_check(bf3)\n\nAdding missing grouping variables: `class`\n\n\n\n\n\nFigure 5.8: Density of the observed data (y) plotted against 10 random draws from the posterior predictive distribution.\n\n\n\n\nThe gamma model fits the data better. There are no predictions below zero anymore. It does seem like the gamma distribution tends to predict higher densities of lower values than we observed, as seen in the plots for the tenhr, thoushr, and veg fuel classes. But generally, the predictions appear to agree with the observed data pretty well."
  },
  {
    "objectID": "hypothesis_test.html#expected-value-of-the-posterior-predictive",
    "href": "hypothesis_test.html#expected-value-of-the-posterior-predictive",
    "title": "5  Hypothesis testing",
    "section": "5.7 Expected value of the posterior predictive",
    "text": "5.7 Expected value of the posterior predictive\n\n# get posterior predictions for treatments, ignoring random effects. These are\n# predictions for the expected value across all sites.\n\npredict_posterior_expected &lt;- function(data, plot = TRUE) {\n  newdata &lt;- expand(data$data[[1]], nesting(treatment))\n  data &lt;- mutate(data,\n    pred = list(\n      tidybayes::epred_draws(mod, newdata, re_formula = NA, value = \"pred\")\n    ),\n    lims = list(\n      tibble(xmin = 0, xmax = quantile(pred$pred, .995))\n    )\n  )\n  if (plot) {\n    p &lt;- data |&gt; unnest(pred) |&gt;\n      ggplot(aes(pred, treatment)) +\n      tidybayes::stat_halfeye(normalize = \"panels\") +\n      facet_wrap(~class, scales = \"free_x\") +\n      coord_cartesian_panels(panel_limits = unnest(select(data, lims), lims))\n    print(p)\n  }\n  invisible(data)\n}\n\n\nexpected_predictions &lt;- predict_posterior_expected(bf3)\n\nAdding missing grouping variables: `class`\n\n\n\n\n\nFigure 5.9: Posterior expected predictions, with no random effects. This reprsents the expected average conditions across all sites. The point estimate is the mode. Units are mg ha-1. Upper and lower limits are the 95% credible intervals.\n\n\n\n\nThese are the expected predictions, or predictions for the mean. It only includes the uncertainty in the mean and not the variance in predictions estimated by the model.\nThere is quite a bit of uncertainty about the mean all around, but there is a notable difference in that uncertainty among treatments for the onehr and veg fuel classes.\nTable Table 5.5 shows these data in a tabular format. I’m using the highest density continuous interval because, while its hard to see in Figure 5.9, the highest desity region is actually slightly discontinuous.\n\nexpected_predictions |&gt;\n  mutate(\n    summary = list(tidybayes::mode_hdci(pred))\n  ) |&gt; select(summary) |&gt; unnest(summary) |&gt;\n  select(treatment, prediction = pred, lower = .lower, upper = .upper) |&gt;\n    gt::gt(groupname_col = \"class\") |&gt;\n    gt::tab_options(row_group.as_column = TRUE) |&gt;\n    gt::fmt_number(decimals = 1)\n\nAdding missing grouping variables: `class`\nAdding missing grouping variables: `class`\n\n\n\n\n\n\nTable 5.5:  Tabular data associated with @fig-expected-predictions.\nUpper and lower pertain to the 95% highest density continuous\ninterval. \n  \n    \n    \n      \n      treatment\n      prediction\n      lower\n      upper\n    \n  \n  \n    onehr\ngs\n1.0\n1.0\n3.2\n    ha\n2.2\n1.0\n6.8\n    hd\n1.9\n1.0\n5.7\n    ld\n1.2\n1.0\n3.8\n    tenhr\ngs\n3.5\n1.8\n6.3\n    ha\n2.8\n1.4\n5.1\n    hd\n2.6\n1.4\n4.6\n    ld\n3.0\n1.2\n5.1\n    hundhr\ngs\n10.7\n5.1\n18.7\n    ha\n8.4\n4.2\n14.8\n    hd\n8.4\n3.7\n14.6\n    ld\n9.4\n4.8\n17.1\n    dufflitter\ngs\n45.8\n24.9\n72.0\n    ha\n37.9\n21.9\n63.2\n    hd\n53.4\n29.1\n84.0\n    ld\n44.3\n23.4\n69.7\n    thoushr\ngs\n31.4\n9.7\n94.0\n    ha\n21.4\n6.5\n61.6\n    hd\n32.3\n9.0\n88.9\n    ld\n22.2\n6.1\n57.6\n    veg\ngs\n25.6\n8.2\n57.9\n    ha\n9.1\n2.6\n21.5\n    hd\n13.9\n5.0\n33.6\n    ld\n17.4\n7.2\n43.2\n  \n  \n  \n\n\n\n\n\n\n\nCode\npredict_expected_contrasts &lt;- function(data, rope_size, plot = TRUE) {\n  # Assume treatment levels are the same for all models: they are.\n  newdata &lt;- expand(data$data[[1]], nesting(treatment))\n  d &lt;- data |&gt;\n    mutate(\n      pred = list(\n        tidybayes::epred_draws(mod, newdata, re_formula = NA, value = \"pred\") |&gt;\n        tidybayes::compare_levels(pred, by = treatment) |&gt;\n        select(contrast = treatment, pred) \n      ),\n      rope = rope_size * sd(data$load),\n      lims = list(\n        tibble(xmin = quantile(pred$pred, .001), xmax = quantile(pred$pred, .999))\n      )\n    )\n  if (plot) {\n    p &lt;- d |&gt;\n      unnest(c(pred)) |&gt;\n      ggplot(aes(x = pred, y = contrast)) +\n      tidybayes::stat_halfeye(normalize = \"panels\") +\n      geom_vline(aes(xintercept = rope))  +\n      geom_vline(aes(xintercept = -rope))  +\n      facet_wrap(~class, scales = \"free_x\") +\n      coord_cartesian_panels(\n        panel_limits = unnest(select(d, lims), lims)\n      )\n    print(p)\n  }\n  invisible(d)\n}\n\n\n\nexpected_contrasts &lt;- predict_expected_contrasts(bf3, 0)\n\nAdding missing grouping variables: `class`\n\n\n\n\n\nFigure 5.10: Differences between expected values for each treatment, with 95% continuous interval shown.\n\n\n\n\n\nexpected_contrasts |&gt;\n  mutate(\n    summary = list(tidybayes::mode_hdci(pred)),\n    pred = list(group_by(pred, contrast)),\n    prob = list(summarize(pred,\n      prob = if_else(median(pred) &gt; 0, \n        mean(pred &gt; rope),\n        mean(pred &lt; rope)\n      )\n    )),\n    summary = list(left_join(summary, prob, by = \"contrast\"))\n  ) |&gt; select(summary) |&gt; unnest(c(summary)) |&gt;\n  select(contrast, prediction = pred, lower = .lower, upper = .upper, prob) |&gt;\n  knitr::kable(digits = 2)\n\nAdding missing grouping variables: `class`\nAdding missing grouping variables: `class`\n\n\n\n\nTable 5.6: Posterior expected predictions of pairwise differences in means, with no random effects. This reprsents the expected average conditions across all sites. Units are Mg ha-1. The point estimate is the mode. Upper and lower limits are the 95% credible intervals. Prob is the probability that the predicted difference matches the sign of its median–the probability that it is not zero.\n\n\nclass\ncontrast\nprediction\nlower\nupper\nprob\n\n\n\n\nonehr\nha - gs\n1.07\n-0.26\n4.23\n0.98\n\n\nonehr\nhd - gs\n0.60\n-0.29\n3.39\n0.96\n\n\nonehr\nhd - ha\n-0.29\n-2.81\n1.46\n0.73\n\n\nonehr\nld - gs\n0.04\n-0.83\n1.48\n0.70\n\n\nonehr\nld - ha\n-0.94\n-3.85\n0.48\n0.96\n\n\nonehr\nld - hd\n-0.49\n-2.90\n0.66\n0.89\n\n\ntenhr\nha - gs\n-0.58\n-3.36\n1.86\n0.75\n\n\ntenhr\nhd - gs\n-0.63\n-3.46\n1.62\n0.81\n\n\ntenhr\nhd - ha\n-0.05\n-2.36\n2.07\n0.58\n\n\ntenhr\nld - gs\n-0.59\n-3.39\n1.78\n0.71\n\n\ntenhr\nld - ha\n0.27\n-2.13\n2.38\n0.55\n\n\ntenhr\nld - hd\n0.27\n-1.96\n2.67\n0.64\n\n\nhundhr\nha - gs\n-1.65\n-8.23\n3.16\n0.83\n\n\nhundhr\nhd - gs\n-2.18\n-8.45\n3.36\n0.84\n\n\nhundhr\nhd - ha\n-0.19\n-5.51\n4.83\n0.52\n\n\nhundhr\nld - gs\n-1.34\n-7.74\n4.61\n0.66\n\n\nhundhr\nld - ha\n0.72\n-4.27\n6.89\n0.70\n\n\nhundhr\nld - hd\n0.82\n-4.21\n7.03\n0.72\n\n\ndufflitter\nha - gs\n-6.44\n-24.08\n8.05\n0.86\n\n\ndufflitter\nhd - gs\n5.27\n-9.25\n27.93\n0.81\n\n\ndufflitter\nhd - ha\n13.61\n-2.98\n34.33\n0.97\n\n\ndufflitter\nld - gs\n-2.18\n-19.57\n13.97\n0.64\n\n\ndufflitter\nld - ha\n3.85\n-9.35\n21.10\n0.78\n\n\ndufflitter\nld - hd\n-6.69\n-27.71\n8.73\n0.88\n\n\nthoushr\nha - gs\n-11.05\n-65.31\n29.97\n0.80\n\n\nthoushr\nhd - gs\n-3.41\n-61.88\n53.59\n0.56\n\n\nthoushr\nhd - ha\n8.15\n-32.20\n63.47\n0.77\n\n\nthoushr\nld - gs\n-11.87\n-70.76\n25.18\n0.82\n\n\nthoushr\nld - ha\n0.94\n-35.74\n36.45\n0.52\n\n\nthoushr\nld - hd\n-9.92\n-67.13\n29.11\n0.78\n\n\nveg\nha - gs\n-16.54\n-42.61\n1.35\n0.99\n\n\nveg\nhd - gs\n-9.13\n-38.84\n7.24\n0.92\n\n\nveg\nhd - ha\n3.86\n-7.69\n21.60\n0.84\n\n\nveg\nld - gs\n-6.39\n-36.03\n13.23\n0.82\n\n\nveg\nld - ha\n6.29\n-4.45\n28.59\n0.94\n\n\nveg\nld - hd\n2.55\n-13.46\n22.90\n0.71"
  },
  {
    "objectID": "work_log.html",
    "href": "work_log.html",
    "title": "6  Work log",
    "section": "",
    "text": "The following is a log of the work completed for this project\n\n2023-07-07\n\nEmmeans for multilevel model\nbootstrap test of treatment significance\ncompare to nlme\ncompare to model addressing heterscedasticity\n\n\n\n2023-07-03\n\nmulti-level model\n\n\n\n2023-06-30\n\nexplore more distributions\n\n\n\n2023-06-29\n\nexplore other distributions, zeros\n\n\n\n2023-06-28\n\nadded histograms + normal curves\n\n\n\n2023-06-27\n\nforgot these files\ntest normality\n\n\n\n2023-06-20\n\nadded captions\nlook at outliers in fuel data\n\n\n\n2023-06-16\n\ninterpret fuel load summary\n\n\n\n2023-06-15\n\nclean up code blocks\nMerge branch ‘main’ of https://github.com/fisher-j/multi-age\nImprove presentation of fuel loading from literature\n\n\n\n2023-06-14\n\nminor website change\nsummarize total fuel loading\n\n\n\n2023-06-13\n\ncomplete vegetation loading\nwebsite\ncomplete dufflitter fuel loading\n\n\n\n2023-06-12\n\nwork on duff/litter, values from lit.\n\n\n\n2023-06-20\n\nBegin fuels data exploration\nLook for outliers\n\n\n\n2023-06-15\n\nImprove presentation of fuel loading values from literature\n\n\n\n2023-14-08\n\nCalculated vegetation fuel loading\ncalculate duff and litter fuel load\n\nsummarize some depth to load values from literature\n\ncalculate vegetation fuel load\n\nresearch vegetation bulk density in literature\n\nwe should have measured shrub basal diameter\n\n\nSummarize total fuel load\n\nsummarize values from literature\n\n\n\n\n2023-06-08\n\nCalculate fine and coarse fuel load\ncreate test for duplicated transect or plot ids\nfix wrong azimuths in datasheet\nstart with duff and litter load\n\n\n\n2023-06-05\n\nStart calculate fuel loading\nsome background research on the theory behind planar intercept\ncompile fuel particle parameters from multiple sources\n\n\n\n2023-06-01\n\nFinish data entry\n\n\n\n2023-05-31\n\nFigure showing fuel (and vegetation) sampling layout\nFunction to combine data from all datasheets\nRefine data description text\ncontinued data entry\n\n\n\n2023-5-26\n\nDeveloped data input format\n\none file for each plot\n\nCreated R function to ingest a datasheet in the specified format\nDocumented the input format and processing function\nFuel data entered for 7 replicates\n\n\n\n2023-05-23\n\nwork on development environment for coding\n\n\n\n2023-05-22\n\nInitialized project as a quarto book\n\nlearning to use Quarto building analysis website\nset up GitHub repository\nstart log to track progress"
  }
]