[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Redwoood multi-age experiment",
    "section": "",
    "text": "1 Introduction\nThis is an analysis of the 10-year remeasure of the Redwood multi-age experiment. It was started by Dr. Pascal Berrill, professor of silviculture at Cal-Poly Humboldt. The experiment is located in the Jackson Demonstration State Forest in Mendocino County, California. The multi-age experiment explores the regeneration response of several species following different harvesting techniques including group selection, aggregated retention, and high/low dispersed retention. The 10-year remeasure data includes surface fuel characterization."
  },
  {
    "objectID": "data_description.html#fuel-data",
    "href": "data_description.html#fuel-data",
    "title": "2  Data description",
    "section": "2.1 Fuel data",
    "text": "2.1 Fuel data\nThe fuel data was gathered in the spring of 2022 and 2023. The Firemon protocol was used as the basis for data collection which in turn employs the use of Brown’s transects, with the addition of duff, litter, and vegetation density sampling stations at two locations along each transect (Section 2.1.3). While the firemon protocol specifies a 2 meter tall imaginary cylinder for quantifying surface fuels, because our resprouting trees were continuous with the ground, we used an imaginary cylinder with a variable height, up to the tallest shrub or sprout fuels that were continuous to the ground.\nTwo 10-meter transects were installed parallel to plot edges at each macro plot corner. Their near ends shared a point inset 10 meters from the corner, towards plot center (Figure 2.1). Downed, woody fuels in four size classes were counted starting from the far end of the transect and each fuel size class was counted for a specified lenght of the total transect (generally longer for larger fuels).\nAt two stations along each transect, duff, litter, live and dead vegetation density (as height and percent cover), and fuel bed depth were assessed. Details of each of these values are described below (Section 2.1.3)\nThe notion of fuel bed depth is notably absent from the Firemon protocol, so we added it by estimating an “average” fuel bed depth within the vegetation sampling cylinder that included litter, and all downed woody debris.\nDominant (well represented) woody speices were also recorded in the vegetation cylinders.\n\n\n\nFigure 2.1: A diagram of the fuel sampling design depicting 0.4 hectare macro plot, fuels transects, transect stations, and vegetation sub-plots. Large green circles are vegetation monitoring plots (which are mostly used for tracking species composition) and the small orange circles are transect stations where duff, litter, and live and dead vegetation density were measured.\n\n\n\n2.1.1 Data entry\nData was entered using Excel and exported as a UTF-8 csv. The file format (Figure 2.2) was defined to resemble the physical datasheets as closely as possible, while maintaining data integretity and facilitating ease of input. There is one file for each macro plot (or replicate = site + treatment combination).\nEach file includes sections for different tables, and these are separated by a line that contains only a single hashtag (e.g., “#duff_litter_fbd”) in lowercase letters with underscores for spaces. In Excel, the #hashtag should go in the first cell of a row, with nothing esle in the row. There should be no blank rows before or after hashtags. The table sections used don’t correlate 1:1 with physical datasheet tables. For instance, columns for “metermeark1” and “metermark2” were added to reflect the fact that initially we sampled duff, litter and vegetation density (transect stations) at 5 and 10 m, but later we moved the stations to 5 and 9 m. The “1” and “2” following repeated variable names refers to one of these locations along the transect. The function used to wrangle these data matches rows based on transect number and columns based on column names. Thus, it is possible to rearrage the rows or columns within each section as long as column names are kept consistent and data for each transect have the same transect number. Following are the section and column labels used for each section of the plot data entry form with any notes regarding their format. A complete description of the data varialbes can be found in Section 2.1.3.\n\n#site_info site treatment\n\ndate\nonehr\ntenhr\nhundhr\nthoushr\ntrans_count notes (site level notes, should not contain commas)\n\n#transects\n\ntransect (number 1 through 8, corresponds with transect column in other sections)\ncorner\nazi\nfwd_crew\nveg_crew (moved from vegetation table to here)\nslope\nmetermark1 (location along transect of first station)\nmetermark2 (location along transect of second station)\nnotes (transect specific notes)\n\n#duff_litter_fbd\n\ntransect (number 1 through 8)\nonehr\ntenhr\nhundhr\nduff_litter1\npct_litter1\nfbd1\nduff_litter2\npct_litter2\nfbd2\n\n#vegetation\n\ntransect (number 1 though 8)\nlive_woody1\ndead_woody1\navg_w_ht1\nlive_herb1\ndead_herb1\navg_h_ht1\nlive_woody2\ndead_woody2\navg_w_ht2\nlive_herb2\ndead_herb2\navg_h_ht2\n\n#woody_species\n\ntransect\nspecies1 (space-delimited list of ‘significant’ species)\nspecies2 (space-delimited list of ‘significant’ species)\n\n#coarse_woody_debris\n\ntransect\ndia\ndecay\n\n\n\n\n\nFigure 2.2: Screenshot of an examle of the datasheet entry format. Each section, defined by a begining hashtag is parsed by an r function as a seperate table.\n\n\nAn R function is used to parse this file into individual tables. As part of this process, all the table sections whose rows have a 1:1 relationship with transects are combined into one wide table, based on the transect column.\n\n\nCode\nsource(\"./scripts/test_funs.r\")\n\n# This function processess one datasheet and returns three tables that can\n# be combined with corresponding tables from other data sheets\nwrangle_datasheet &lt;- function(file) {\n  con &lt;- file(file, encoding = \"UTF-8\")\n  lines &lt;- readLines(con)\n  close(con)\n\n  # Remove BOM &lt;https://stackoverflow.com/a/67906611&gt;\n  lines[1] &lt;- gsub(\"\\\\xef\\\\xbb\\\\xbf\", \"\", lines[1], useBytes = TRUE)\n\n  # These are the sections I want to extract\n  to_get &lt;- c(\n    \"site_data\", \"transects\", \"duff_litter_fbd\", \"vegetation\",\n    \"woody_species\", \"coarse_woody_debris\"\n  )\n  # sections are defined by line with only a hashtage (#section)\n  section_pattern &lt;- \"^#(\\\\w+).*$\"\n\n  # find hashtags and get the data from the next line to the line\n  # before the next hashtag\n  section_breaks &lt;- grep(section_pattern, lines)\n  section_start &lt;- section_breaks + 1\n  section_end &lt;- c(section_breaks[-1], length(lines)) - 1\n  section_names &lt;- gsub(section_pattern, \"\\\\1\", lines[section_breaks])\n\n  sections &lt;- purrr::map2(section_start, section_end, \\(x, y) c(x, y)) |&gt;\n    setNames(section_names) |&gt;\n    (`[`)(to_get) |&gt;\n    purrr::map(\\(x) lines[seq.int(x[1], x[2])]) |&gt;\n    # collapse sections to strings so they can be read as if they were files\n    purrr::map(\\(x) paste(x, collapse = \"\\n\")) |&gt;\n    # leave empty column names so they can be removed\n    purrr::map(\\(x) \n      readr::read_csv(x, show_col_types = FALSE, name_repair = \"minimal\", progress = FALSE)\n    ) |&gt;\n    # Remove empty columns\n    purrr::map(\\(x) x[!names(x) %in% \"\"])\n\n  # I'm going to combine these into a wide table because each row is a transect.\n  # Further data wrangling will require expanding the stations within transecs.\n  # also need to make sure the rows have site data and transect ids for the\n  # coarse woody debris.\n  transect_data &lt;- c(\n    \"transects\", \"duff_litter_fbd\", \"vegetation\", \"woody_species\"\n  )\n\n  transects &lt;- sections |&gt;\n    (`[`)(transect_data) |&gt;\n    purrr::reduce(dplyr::left_join, by = \"transect\") |&gt;\n    # differentiate between transect lenghts and particle counts\n    dplyr::rename_with(\\(x) paste0(x, \"_count\"), ends_with(\"hr\")) |&gt;\n    dplyr::mutate(\n      sections$site_data[c(\"site\", \"treatment\")],\n      .before = corner\n    ) |&gt;\n    dplyr::select(-transect)\n\n  coarse_woody_debris &lt;- sections$coarse_woody_debris |&gt;\n    dplyr::mutate(\n      sections$site_data[c(\"site\", \"treatment\")],\n      .after = transect\n    ) |&gt;\n    dplyr::left_join(\n      sections$transects[c(\"transect\", \"corner\", \"azi\")]\n    ) |&gt;\n    dplyr::select(c(site, treatment, corner, azi, dia, decay))\n  \n  plots &lt;- sections$site_data |&gt;\n    # differentiate between transect lenghts and particle counts\n    dplyr::rename_with(\\(x) paste0(x, \"_length\"), ends_with(\"hr\"))\n\n  # I'll add a check to make sure that all transects and plots are unique\n  warn_duplicates(transects, site, treatment, corner, azi)\n  warn_duplicates(plots, site, treatment)\n\n  # Final output with three tables. These will be combined with corresponding\n  # tables from other datasheets.\n  list(\n    plots = plots,\n    transects = transects,\n    coarse_woody = coarse_woody_debris\n  )\n}\n\ndata_dir &lt;- \"../data\"\n\n# Combine fuels data for each plot\n#\n# This function expects all fuel datasheets to begin with \"fuel\" and end with\n# \"csv\". It loads all matching files in a given folder and returns the same\n# tables as `wrangle_datasheet`, but for all plots combined.\ncombine_fuels_datasheets &lt;- function(data_dir) {\n  files &lt;- list.files(data_dir, pattern = \"^fuel.*csv$\", full.names = TRUE)\n  sheets_list &lt;- purrr::map(files, wrangle_datasheet)\n  table_names &lt;- purrr::set_names(names(sheets_list[[1]]))\n  purrr::map(table_names, \\(x) purrr::list_rbind(purrr::map(sheets_list, x)))\n}\n\n\n# this is how to pivot station data to longer format\n# d |&gt;\n#   tidyr::pivot_longer(\n#     cols = !c(site, treatment, corner, azi),\n#     names_to = \".value\",\n#     names_pattern = \"(\\\\w+)[12]$\"\n#   )\n\n\nThe resulting data looks like this:\n\nwrangle_datasheet(\"../data/fuel_waldon_gs.csv\")\n\nJoining with `by = join_by(transect)`\n\n\n$plots\n# A tibble: 1 × 9\n  site   treatment date   onehr_length tenhr_length hundhr_length thoushr_length\n  &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1 waldon gs        3/15/…            2            2             4             10\n# ℹ 2 more variables: trans_count &lt;dbl&gt;, notes &lt;chr&gt;\n\n$transects\n# A tibble: 8 × 33\n  site   treatment corner   azi fwd_crew veg_crew slope metermark1 metermark2\n  &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 waldon gs        n        135 jf       ac           4          5         10\n2 waldon gs        n        248 jf       ac          19          5         10\n3 waldon gs        w         45 dr       dr           8          5         10\n4 waldon gs        w        135 dr       dr          20          5         10\n5 waldon gs        s         45 dr       jf           5          5         10\n6 waldon gs        s        315 dr       jf           5          5         10\n7 waldon gs        e        225 dr       jf           8          5          9\n8 waldon gs        e        315 dr       jf          13          5          9\n# ℹ 24 more variables: notes &lt;chr&gt;, onehr_count &lt;dbl&gt;, tenhr_count &lt;dbl&gt;,\n#   hundhr_count &lt;dbl&gt;, duff_litter1 &lt;dbl&gt;, pct_litter1 &lt;dbl&gt;, fbd1 &lt;dbl&gt;,\n#   duff_litter2 &lt;dbl&gt;, pct_litter2 &lt;dbl&gt;, fbd2 &lt;dbl&gt;, live_woody1 &lt;dbl&gt;,\n#   dead_woody1 &lt;dbl&gt;, avg_w_ht1 &lt;dbl&gt;, live_herb1 &lt;dbl&gt;, dead_herb1 &lt;dbl&gt;,\n#   avg_h_ht1 &lt;dbl&gt;, live_woody2 &lt;dbl&gt;, dead_woody2 &lt;dbl&gt;, avg_w_ht2 &lt;dbl&gt;,\n#   live_herb2 &lt;dbl&gt;, dead_herb2 &lt;dbl&gt;, avg_h_ht2 &lt;dbl&gt;, species1 &lt;chr&gt;,\n#   species2 &lt;chr&gt;\n\n$coarse_woody\n# A tibble: 20 × 6\n   site   treatment corner   azi   dia decay\n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 waldon gs        n        135     9     5\n 2 waldon gs        n        135    13     2\n 3 waldon gs        n        248    48     3\n 4 waldon gs        n        248    10     5\n 5 waldon gs        w         45    16     5\n 6 waldon gs        w         45    15     5\n 7 waldon gs        w         45    14     5\n 8 waldon gs        w         45    14     5\n 9 waldon gs        w        135    32     3\n10 waldon gs        w        135    12     4\n11 waldon gs        w        135    30     5\n12 waldon gs        s         45    28     4\n13 waldon gs        s         45    42     5\n14 waldon gs        s        315    26     3\n15 waldon gs        e        225    15     4\n16 waldon gs        e        225    13     3\n17 waldon gs        e        225    18     5\n18 waldon gs        e        225    34     3\n19 waldon gs        e        315    26     4\n20 waldon gs        e        315    12     5\n\n\nWhile the wide format for the transect data is not ideal, it is convenient because it reduces the number of tables we are dealing with. This will require extra work at analysis time to expand the data variables which are followed by a “1” or “2”, which represent different stations (Section 2.1.3) on the same transect. To analze these data, they will first need to be pivoted into a longer format, where the station (1 or 2) becomes an explicit column.\n\n\n2.1.2 Data variable descriptions\nThe following is a descripton of the data variables for each of the tables in the data list. So, the $plots heading refers to the table found in data$plots. This data structure is subject to change, but for now, has 3 tables. $plots has plot level data including the lengths of transects (which are the same across all transects, but included for clarity). $transects contains all the fuel data associated with a given transect, including both sampling stations, in wide format (one row for each transect). Finally, $coarse_woody contains coarse woody debris in a long format (multiple rows for each transect).\n\n\n2.1.3 Station sampling cylinder\nReference is made the station sampling cylinder below. It is an imaginary, vertical cylinder with a radius of 1 meter and a variable height equal to the maxiumm height of sprout or shrub vegetation within the cylinders radius. There are two sampling cylinders on each transect and their centers are defined by the transects’ two metermarks. Duff, litter, and fuel bed depth in addition to the vegetation measurements, are all recorded within these cylinders.\n\n\n2.1.4 $plots\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nsite\n\n\n\ntreatment\n\n\n\ndate\n\n\n\nonehr\nDistance from end of transect for which 1-hr fuels were counted\n\n\ntenhr\nDistance from end of transect for which 10-hr fuels were counted\n\n\nhundhr\nDistance from end of transect for which 100-hr fuels were counted\n\n\nthoushr\nDistance from end of transect for which 1000-hr fuels were counted\n\n\ntrans_count\nnumber of transects on macro plot\n\n\nnotes\n\n\n\n\n\n\n2.1.5 $transects\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nsite\nOne of four different sites: whiskey, waldon, waldos, and camp6\n\n\ntreatment\nOne of HD=High density, dispersed retention, LD=Low density dispersed retention, MD=Medium densty dispersed retention, HA=High density aggreagated retention, GS=Group selection opening (1 ha, centered on plot)\n\n\ncorner\nOne of n,s,e,w for “diamond” plost and one of ne, nw, se, sw for “square” plots\n\n\nazi\nactual azimuth from corner to end of fuel transect, deg\n\n\nfwd_crew\nInitials of person performing fuel counts and measuring litter, duff, and FBD\n\n\nveg_crew\nInitials of person estimating vegetation cover\n\n\nslope\nslope in percent, measured with clinometer\n\n\nmetermark1\nlocantion along transect of first veg. station m\n\n\nmetermark2\nlocantion along transect of first veg. station m\n\n\nnotes\nTransect specific notes\n\n\nonehr\nCount of down woody fuels &lt;0.6 cm for the lenght of the 1-hr transect, redwood leaflets less than about 2 mm were not counted as one hour fuels\n\n\ntenhr\nCount of down woody fuels &gt;= 0.6 and &lt; 2.5 cm\n\n\nhundhr\nCount of down woody fuels &gt;= 2.5 and &lt; 8 cm\n\n\nduff_litter1\nCombined duff and litter depth from a representative location within a 1-meter radius circle centered at metermark, cm\n\n\npct_litter1\nPercent of duff_litter depth compose of litter: percent\n\n\nfbd1\nEstimated average height of litter and downed woody debris within the sampling cylinder, 1 m radius, centered at metermark\n\n\nduff_litter2\n\n\n\npct_litter2\n\n\n\nfbd2\n\n\n\nlive_woody1\nTotal projected ground cover of all live woody plant parts within 1-meter-radius sampling cylinder of height equal to the height of the shrub or sprout vegetaion within the cylinders radius, 0-100 percent\n\n\ndead_woody1\nTotal projected ground cover of all dead woody plant parts connected to live or standing dead plants, within the sampling cylinder 0-100 percent\n\n\navg_w_ht1\nAverage maximum height of all live and dead woody plants in sampling cylinder (see Estimating Height in Firemon protocol)\n\n\nlive_herb1\nTotal projected ground cover of live herbs in the sampling cylinder\n\n\ndead_herb1\nTotal projected ground cover of dead herbs in the sampling cylinder\n\n\navg_h_ht1\nAverage maximum height of live and dead herbs in the sampling cylinder\n\n\nlive_woody2\n\n\n\ndead_woody2\n\n\n\navg_w_ht2\n\n\n\nlive_herb2\n\n\n\ndead_herb2\n\n\n\navg_h_ht2\n\n\n\nspecies1\nDominant woody species within sampling cylinder, each species is assumed to occupy an equal portion of volume, under-represented species are ignored\n\n\nspecies2\n\n\n\n\n\n\n2.1.6 $coarse_woody\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nsite\n\n\n\ntreatment\n\n\n\ncorner\n\n\n\nazi\n\n\n\ndia\n\n\n\ndecay\nDecay class 1-5, 1=Fine branches still present, 2=Some branches and bark missing, 3=most branches and much bark missing potentially minor decay, 4=Significant decay, 5=Almost completely rotten"
  },
  {
    "objectID": "calculate_fuel_loading.html#fine-woody-debris",
    "href": "calculate_fuel_loading.html#fine-woody-debris",
    "title": "3  Calculate fuel loading",
    "section": "3.1 Fine woody debris",
    "text": "3.1 Fine woody debris\nThe above information will allow us to calculate fuel loading for fine and coarse woody debris. First we’ll load our data and get the FWD (and the fuel particle parameters) in a long format for easier calculations.\nTwo transects are missing slope. I’m giving them a slope of 0 for now.\n\n\nCode\n# use this to reduce the amount of typing when referring to transects\ntransectid &lt;- c(\"site\", \"treatment\", \"corner\", \"azi\")\n\nfwd &lt;- d$transects |&gt;\n  select(all_of(transectid), slope, matches(\"one|ten|hun\")) |&gt;\n  mutate(slope = if_else(is.na(slope), 0, slope)) |&gt;\n  left_join(select(d$plots, site, treatment, matches(\"one|ten|hun\"))) |&gt;\n  # move onehr, tenhr, etc to new column and create new columns for transect\n  # length and particle counts\n  pivot_longer(\n    matches(\"count|len\"),\n    names_to = c(\"class\", \".value\"),\n    names_sep = \"_\"\n  ) |&gt; \n  left_join(get_particle_params(source = \"glebocki\")) |&gt;\n  mutate(\n    load = simple_load(\n    sum_d2 = count * d2,\n    l = length,\n    percent_slope = slope,\n    G = G,\n    a = a\n    )\n  ) |&gt;\n  select(-c(G, a))"
  },
  {
    "objectID": "calculate_fuel_loading.html#coarse-woody-debris",
    "href": "calculate_fuel_loading.html#coarse-woody-debris",
    "title": "3  Calculate fuel loading",
    "section": "3.2 Coarse woody debris",
    "text": "3.2 Coarse woody debris\nCoarse woody debris is already in a long format so we don’t need to pivot longer, but we will summarize the data for each transect by getting the sum of squared diameters. This differs from the fine woody data because we have actual diameters instead of counts, each diameter corresponds to a single observation.\nWe only have parameters for “sound” and “rotten” particles, so anything over decay class 3 will be considered “rotten”. Finally, we need to join in transect slopes, and transect lengths.\nI’m setting a couple of missing slope to zero.\n\ncwd &lt;- d$coarse_woody |&gt;\n  mutate(class = if_else(decay &gt; 3, \"thoushr_r\", \"thoushr_s\")) |&gt;\n  group_by(site, treatment, corner, azi, class) |&gt;\n  # named count to match fwd table, but these are actually summed d^2\n  summarize(\n    sum_d2 = sum(dia^2),\n    count = n(),\n    med_d = median(dia),\n    .groups = \"drop\"\n  ) |&gt;\n  left_join(d$transects[c(transectid, \"slope\")]) |&gt;\n  mutate(slope = if_else(is.na(slope), 0, slope)) |&gt;\n  left_join(d$plots[c(\"site\", \"treatment\", \"thoushr_length\")]) |&gt;\n  left_join(get_particle_params(source = \"glebocki\")) |&gt;\n  mutate(\n    load = simple_load(\n      sum_d2 = sum_d2,\n      l = thoushr_length,\n      percent_slope = slope,\n      G = G,\n      a = a\n    )\n  ) |&gt;\n  select(-c(G, a, d2), length = thoushr_length)"
  },
  {
    "objectID": "calculate_fuel_loading.html#duff-and-litter",
    "href": "calculate_fuel_loading.html#duff-and-litter",
    "title": "3  Calculate fuel loading",
    "section": "3.3 Duff and litter",
    "text": "3.3 Duff and litter\nWe measured total duff/litter depth, and then estimated a percent of this depth that would be classified as litter. Litter is any leaf material not classified as a 1-hr fuel, that has not yet begun to break down. Particles that were very dark in color and that were broken into smaller pieces than when they had originally fallen were classified as duff.\nDuff and litter were measured at two locations along each transect, for a total of 16 measurements per plot.\nTo convert these depths to load values we use a depth to load equation. Finney and Martin (1993) found a wide variability in the bulk densities of samples, suggesting that simply using the average bulk density should be sufficient, as opposed to calculating bulk densities based on strata depth or differentiating between duff and litter.\n\n\n\n\nTable 3.2: Average depth to load multiplier from different sources in the literature.\n\n\n\n\n\n\n\nSource\nDescription\nLoad Mg ha-1 cm-1\n\n\n\n\nFinney and Martin (1993)\nAnnadel SP & Humboldt Redwoods SP (rw dbh &lt;= 60 in)\n7.15\n\n\nKittredge (1940)\nplantation redwoods\n6.80\n\n\nStuart, J.D. 1985, Unpubl. in Finney and Martin (1993)\nRedwoods SP, (mean of duff & litter)\n9.25\n\n\nJan W. Van Wagtendonk, Benedict, and Sydoriak (1998)\nAvg. for Sierra Nevada conifers\n16.24\n\n\nNives (1989)\nRedwood NP, Lost Man Cr., Redwood Cr.\n2.42\n\n\nKrieger et al. (2020)\nNo ref. cited, 2.75 and 5.5 lbs/ft3 litter and duff resp.\n6.60\n\n\nValachovic et al. (2011)\nTanoak-Douglas-fir, litter only, Humboldt County\n0.93\n\n\n\n\n\n\nOn average, we have about 50% litter and a depth of about 6.2 cm. If we use the mean of the first 3 rows in Table 3.2, an average depth to load multiplier for redwood forests (with 50% litter) is 7.73 Mg ha-1 cm-1.\n\ndufflitter |&gt;\n  group_by(treatment) |&gt;\n  summarize(\n    avg_pct_litter = mean(\n      litter_depth / (duff_depth + litter_depth), \n      na.rm = TRUE\n    ),\n    avg_total_depth = mean(duff_depth + litter_depth),\n)\n\n# A tibble: 4 × 3\n  treatment avg_pct_litter avg_total_depth\n  &lt;chr&gt;              &lt;dbl&gt;           &lt;dbl&gt;\n1 gs                 0.482            6.26\n2 ha                 0.548            5.20\n3 hd                 0.395            7.11\n4 ld                 0.453            5.80\n\ndufflitter &lt;- dufflitter |&gt;\n  mutate(\n    load = (duff_depth + litter_depth) * 7.73\n  )"
  },
  {
    "objectID": "calculate_fuel_loading.html#vegetation",
    "href": "calculate_fuel_loading.html#vegetation",
    "title": "3  Calculate fuel loading",
    "section": "3.4 Vegetation",
    "text": "3.4 Vegetation\nWe based our data collection on the Firemon protocol, which determines vegetative fuel loading by multiplying estimated percent cover by height by a constant bulk densities of 8 and 18 t/ha/m for herbaceous and shrub components, respectively.\nHere I want to standardize the data so that heights are all zero (instead of NA), if percent cover for live and dead were both zero. Also, I want total percent cover, with total proportion dead.\nTheoretically, with the Firemon protocol, total percent cover could be greater than 100, because live and dead percent covers are assessed separately. In practice, sum of live and dead cover was over 100 percent.\n\n\nveg_match &lt;- \"woody|herb|avg_w_ht|avg_h_ht|species\"\n\nveg &lt;- d$transects |&gt;\n  select(all_of(transectid), slope, matches(veg_match)) |&gt;\n  pivot_longer(\n    !c(any_of(transectid), slope),\n    names_to = c(\".value\", \"station\"),\n    names_pattern = \"(\\\\w+)([12])\"\n  ) |&gt; \n  mutate(\n    woody_ht = if_else(live_woody == 0 & dead_woody == 0, 0, avg_w_ht),\n    herb_ht = if_else(live_herb == 0 & dead_herb == 0, 0, avg_h_ht),\n    woody_load = ((live_woody + dead_woody) / 100) * woody_ht * 18,\n    woody_p_dead = if_else(\n      woody_load == 0, 0, dead_woody / (live_woody + dead_woody)\n    ),\n    herb_load = ((live_herb + dead_herb) / 100) * herb_ht * 8,\n    herb_p_dead = if_else(\n      herb_load == 0, 0, dead_herb / (live_herb + dead_herb)\n    )\n  ) |&gt;\n  select(!matches(\"live_|dead_|avg_\"), woody_ht, herb_ht) |&gt;\n  pivot_longer(\n    matches(\"woody|herb\"),\n    names_to = c(\"class\", \".value\"),\n    names_pattern = \"(woody|herb)_(.*)\"\n  )"
  },
  {
    "objectID": "calculate_fuel_loading.html#sec-total-load",
    "href": "calculate_fuel_loading.html#sec-total-load",
    "title": "3  Calculate fuel loading",
    "section": "3.5 Total load",
    "text": "3.5 Total load\nNow we can join results for fine woody debris, coarse woody debris, litter, and duff into a single dataframe.\nVegetation and duff/litter first need to be summarized to the transect level.\n\ndufflitter_load &lt;- dufflitter |&gt; \n  group_by(pick(all_of(transectid))) |&gt; \n  summarize(class = \"dufflitter\", load = mean(load))\n\nveg_load &lt;- veg |&gt;\n  group_by(pick(all_of(transectid)), class) |&gt;\n  summarize(load = mean(load))\n\nfwd_load &lt;- fwd |&gt; select(all_of(transectid), class, load)\ncwd_load &lt;- cwd |&gt; select(all_of(transectid), class, load)\n\ntotal_load &lt;- bind_rows(dufflitter_load, veg_load, fwd_load, cwd_load)|&gt;\n  ungroup()\n\nJohn Stuart measured old-growth forest fuels in in Bull Creek Drainage of Humboldt Redwood State Park, approximately 30 km inland from the coast. Overstory BA was about 66 m2ha-1. Plots were classifed based on their overstory/understory species as one of:\n\nredwood-Douglas-fir/tanoak-evergreen huckleberry\nredwood-Douglas-fir/evergreen huckleberry-tanoak\nDouglas-fir-redwood/evergreen huckleberry\n\nFinney and Martin (1993) measured fuels at two loacations. In Annadel SP with 30-40% slopes and 45 - 60 m2ha-1 of trees approximate 120 years old. At RW SP, BA was between 20 and 60 m2ha-1 that sprouted after harvest and were around 90 years old.\nKittredge (1940) studied duff and litter in redwood plantations with an average total depth of 4 cm.\nValachovic et al. (2011) measured surface fuels in Douglas-fir-tanoak forests from Sonoma to northern Humboldt counteis. The values shown are from across this range.\n\n\nTable 3.3: Summary of fuel loading metrics for this and other studies.\n\n\n\n\n\n\n(a)  \n  \n    \n    \n      \n      source\n      low\n      high\n    \n  \n  \n    litter\nvalachovic\n2.9\n4.7\n    dufflitter\nthisstudy\n40.2\n55.0\n    finney\n29.0\n55.0\n    kittredge\n24.0\n24.0\n    stuart\n35.0\n83.3\n    onehr\nthisstudy\n0.6\n1.2\n    valachovic\n2.0\n3.4\n    stuart\n1.0\n2.4\n    tenhr\nthisstudy\n2.9\n3.7\n    valachovic\n2.5\n6.1\n    stuart\n4.0\n7.3\n    hundhr\nthisstudy\n9.5\n11.8\n    valachovic\n3.1\n7.6\n    stuart\n3.2\n8.5\n    onetenhundhr\nthisstudy\n13.4\n16.1\n    finney\n9.0\n20.0\n    valachovic\n9.0\n15.5\n    stuart\n9.1\n15.1\n  \n  \n  \n\n\n\n\n\n\n\n(b)  \n  \n    \n    \n      \n      source\n      low\n      high\n    \n  \n  \n    thoushr_s\nthisstudy\n27.4\n72.4\n    valachovic\n4.9\n76.7\n    stuart\n5.7\n133.0\n    thoushr_r\nthisstudy\n23.0\n59.0\n    valachovic\n3.1\n35.2\n    stuart\n0.6\n52.9\n    thoushr\nthisstudy\n45.1\n60.6\n    finney\n0.0\n264.0\n    valachovic\n18.3\n85.9\n    stuart\n6.3\n143.1\n    veg_woody\nstuart\n0.1\n6.3\n    NA\nthisstudy\n0.2\n0.5\n    thisstudy\n12.4\n37.8"
  },
  {
    "objectID": "calculate_fuel_loading.html#summary",
    "href": "calculate_fuel_loading.html#summary",
    "title": "3  Calculate fuel loading",
    "section": "3.6 Summary",
    "text": "3.6 Summary\nOur combined duff-litter depths were comparable to other studies, resulting in comparable loading for litter and duff.\nOnehr fuels were lower than then other redwood study and that found in Douglas-fir/tanoak forests, which is somewhat supprising.\nHundhr fuels were higher in our stands. This makes sense given the logging.\nTotal fine fuel loading (onetenhundhr) was similar to other studies, but apprently with more hundhr particles.\nThoushr fuels are notoriously variable. Those on our sites were more consistent and within the middle of the range of other reported values.\nWoody vegetation was much higher than in the one other reported study. That study was in old-growth redwoods. Our values include tree sprout vegetation, which can be several times taller than evergreen huckleberry and much bushier than understory tanoak saplings. Stuart did mention the presence of “nearly inpenetrable [evergreen huckleberry] thickets.” Stuart (1985) found good correlation of live fuels (which included leaves and “twigs”) with basal diamter for both huckleberry and tanoak saplings. The simple scaling factor and/or the occular estimates we used may be biased."
  },
  {
    "objectID": "calculate_fuel_loading.html#save-data",
    "href": "calculate_fuel_loading.html#save-data",
    "title": "3  Calculate fuel loading",
    "section": "3.7 Save data",
    "text": "3.7 Save data\nI’ll Save this data so it can be used in subsequent analysis.\n\ndufflitter, fwd, cwd, and veg, represent mostly raw data in a long format\n\nthis includes two stations per transect for classes for veg and dufflitter\n\ntotal_load is the summarized loading for all classes by transect.\n\n\nsave(dufflitter, fwd, cwd, veg, total_load, file = \"data_long.RData\")\n\n\n\n\n\nBrown, James K. 1974. “Handbook for Inventorying Downed Woody Material.” Gen. Tech. Rep. INT-16. Ogden, UT: US Department of Agriculture, Forest Service, Intermountain Forest and Range Experiment Station. 24 p. 16.\n\n\nFinney, Mark A., and Robert E. Martin. 1993. “Fuel Loading, Bulk Density, and Depth of Forest Floor in Coast Redwood Stands.” Forest Science 39 (3): 617–22.\n\n\nGlebocki, Radoslaw. 2015. “Fuel Loading and Moisture Dynamics in Thinned Coast Redwood Forests in Headwaters Forest Reserve, California.” Master’s thesis, Humboldt State University. https://scholarworks.calstate.edu/concern/theses/ws859j014.\n\n\nKittredge, Joseph. 1940. “A Comparison of Forest Floors from Plantations of the Same Age and Environment.” Journal of Forestry 38 (9): 729–31. https://doi.org/10.1093/jof/38.9.729.\n\n\nKrieger, Raven M., Brian E. Wall, Cody W. Kidd, and John-Pascal Berrill. 2020. “Chronosequence of Fuel Loading and Fuel Depth Following Forest Rehabilitation Frill Treatment of Tanoak to Release Douglas-Fir: A Case Study from Northern California.” Forests 11 (6): 691. https://doi.org/10.3390/f11060691.\n\n\nNives, Sherryl L. 1989. “Fire Behavior on the Forest Floor in Coastal Redwood Forests, Redwood National Park.” PhD thesis, Humboldt State University.\n\n\nStuart, John. 1985. “Redwood Fire Ecology: Final Report Submitted to California Department of Parks and Recreation.” Forestry Department, Humboldt State University.\n\n\nValachovic, Yana S., Christopher A. Lee, Hugh Scanlon, J. Morgan Varner, Radoslaw Glebocki, Bradley D. Graham, and David M. Rizzo. 2011. “Sudden Oak Death-Caused Changes to Surface Fuel Loading and Potential Fire Behavior in Douglas-fir-tanoak Forests.” Forest Ecology and Management 261 (11): 1973–86. https://doi.org/10.1016/j.foreco.2011.02.024.\n\n\nVan Wagner, C. E. 1982. “Practical Aspects of the Line Intersect Method.” PI-X-12. Chalk River, Ontario, Canada: Petawawa National Forestry Institute, Canadian Forestry Service.\n\n\nVan Wagtendonk, J. W., J. M. Benedict, and W. M. Sydoriak. 1996. “Physical Properties of Woody Fuel Particles of Sierra Nevada Conifers.” International Journal of Wildland Fire 6 (3): 117–23. https://doi.org/10.1071/wf9960117.\n\n\nVan Wagtendonk, Jan W., James M. Benedict, and Walter M. Sydoriak. 1998. “Fuel Bed Characteristics of Sierra Nevada Conifers.” Western Journal of Applied Forestry 13 (3): 73–84. https://doi.org/10.1093/wjaf/13.3.73."
  },
  {
    "objectID": "fuel_data_exploration.html#basic-summary",
    "href": "fuel_data_exploration.html#basic-summary",
    "title": "4  Fuel data exploration",
    "section": "4.1 Basic Summary",
    "text": "4.1 Basic Summary\n\n\nCode\n# function to access mostly raw data, but with combined veg and thoushr fuels.\n\nload2 &lt;- function(shape = \"wide\", ...) {\n  load_vars &lt;- c(\"onehr\", \"tenhr\", \"hundhr\", \"dufflitter\", \"thoushr\", \"veg\")\n  tl &lt;- pivot_wider(total_load, names_from = class, values_from = load) |&gt;\n    mutate(\n      thoushr = rowSums(pick(c(thoushr_s, thoushr_r)), na.rm = TRUE),\n      veg = rowSums(pick(c(woody, herb)), na.rm = TRUE),\n      .keep = \"unused\"\n    )\n  if (!missing(...)) tl &lt;- select(tl, ...)  \n  if (shape == \"long\") {\n    tl &lt;- pivot_longer(tl, \n      -any_of(c(\"site\", \"treatment\", \"corner\", \"azi\")),\n      names_to = \"class\", \n      values_to = \"load\"\n    )\n    load_vars &lt;- load_vars[load_vars %in% tl$class]\n    tl &lt;- mutate(tl, class = factor(class, levels = load_vars))\n  }\n  tl\n} \n\n\n\nload2(\"long\", everything()) |&gt;\n  filter(!(load &gt; 190 & class == \"veg\" | load &gt; 400 & class == \"thoushr\")) |&gt;\n  ggplot(aes(treatment, load)) +\n  geom_boxplot() +\n  facet_wrap(~class, scales = \"free\")\n\n\n\n\nFigure 4.1: Two outliers were removed to aide in interpretability–600 and 199 t/ha in ha-thoushr and gs-veg, respectively.\n\n\n\n\nHere is a table summary of our raw data.\n\nspf &lt;- \"%.1f\"\nload2(\"long\", all_of(c(transectid, load_vars))) |&gt;\n  group_by(class, treatment) |&gt;\n  summarize(\n    avg_load = mean(load),\n    sd_load = sd(load)\n  ) |&gt;\n  mutate(\n    load = paste0(sprintf(spf, avg_load), \" (\", sprintf(spf, sd_load), \")\"),\n    .keep = \"unused\"\n  ) |&gt;\n  pivot_wider(names_from = treatment, values_from = load) |&gt;\n  knitr::kable()\n\n`summarise()` has grouped output by 'class'. You can override using the\n`.groups` argument.\n\n\n\n\nTable 4.1: Average (sd) transect level load (Mg ha-1) for six fuel class categories in four different overstory harvest techniques.\n\n\nclass\ngs\nha\nhd\nld\n\n\n\n\nonehr\n0.6 (0.6)\n1.2 (0.8)\n1.0 (0.6)\n0.7 (0.5)\n\n\ntenhr\n3.7 (2.8)\n3.2 (3.2)\n2.9 (1.6)\n3.4 (2.0)\n\n\nhundhr\n11.8 (8.6)\n9.7 (9.0)\n9.5 (7.6)\n9.5 (8.5)\n\n\ndufflitter\n48.4 (31.1)\n40.2 (22.0)\n55.0 (28.9)\n44.9 (18.3)\n\n\nthoushr\n44.9 (57.0)\n41.6 (105.6)\n39.5 (45.1)\n35.3 (52.4)\n\n\nveg\n38.2 (42.3)\n12.9 (14.6)\n17.3 (16.1)\n21.2 (16.0)"
  },
  {
    "objectID": "fuel_data_exploration.html#outliers",
    "href": "fuel_data_exploration.html#outliers",
    "title": "4  Fuel data exploration",
    "section": "4.2 Outliers",
    "text": "4.2 Outliers\n\ncleavland_plot &lt;- function(data, title, load_var = load) {\n  data |&gt;\n    group_by(site, treatment) |&gt;\n    mutate(replicate_mean_load = mean({{load_var}}, na.rm = TRUE)) |&gt;\n    ggplot(\n      aes(\n        {{load_var}},\n        fct_reorder(\n          interaction(site, treatment, sep = \" \"),\n          {{load_var}},\n          .na_rm = TRUE\n        ),\n        color = treatment\n      )\n    ) +\n    geom_jitter(width = 0, height = 0.2) +\n    labs(x = expression(Load~(Mg%.%ha^-1)), y = \"Data order\", title = title)\n}\n\n\n\n\n\n\n\n(a) One hour fuels\n\n\n\n\n\n\n\n(b) Ten hour fuels\n\n\n\n\n\n\n\n(c) Hundred hour fuels\n\n\n\nFigure 4.2: Data distribution of loading for fine woody debris classes. Data are sorted by mean loading within each replicate. Jitter has been added to aid in visual interpretation.\n\n\n\n\n\n\n\nFigure 4.3: Sum of coarse woody (&gt;7.64 cm, sound and rotten wood combined) fuel loading for transects. The y-axis is sorted by mean CWD loading for each replicate.\n\n\n\n\n\n\n\n\n\nFigure 4.4: Combined duff and litter loading at each station along trancects. Y-axis is sorted as in Figure 4.3.\n\n\n\n\n\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n(a) Woody vegetation\n\n\n\n\n\n\n\n(b) herbaceous vegetation\n\n\n\nFigure 4.5: Vegetation fuel loading for each station along transects, including live and dead fuels attached to live vegetation. Y-axis is sorted as in Figure 4.3."
  },
  {
    "objectID": "fuel_data_exploration.html#sec-Normality",
    "href": "fuel_data_exploration.html#sec-Normality",
    "title": "4  Fuel data exploration",
    "section": "4.3 Normality",
    "text": "4.3 Normality\nFor further testing, I will summarize the data somewhat, by combining vegetation loading (woody and herb), and coarse woody loading (sound and rotten) into just two loading metrics. Now we have the following response variables:\n\ndufflitter\nonehr\ntenhr\nhundhr\nthoushr\nveg\n\nWhen using manova to test for difference between groups with multiple response variables, it is important that the response variables are multivariate normally distributed. Unfortunately, it would appear that we have a probelem with normality. The raw data for each loading variable is clearly not normally distributed Figure 4.6.\n\nmyqqplot &lt;- function(data, var) {\n  data |&gt;\n  ggplot(aes(sample = {{ var }})) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_grid(class ~ treatment, scales = \"free\") +\n  labs(\n    x = \"Theoretical quantiles\", y = \"Sample quantiles\",\n    title = \"Normal Q-Q Plot\"\n  )\n}\n\nload2(\"long\", treatment, all_of(load_vars)) |&gt; \nmyqqplot(load)\n\n\n\n\nFigure 4.6: Naive qq plot of loading variables. This doesn’t take into the fact that our data is nested. You could say this is based on a simple model where all observations are independent.\n\n\n\n\n\nbins &lt;- 16\nhist_dat &lt;- load2(\"long\", treatment, all_of(load_vars)) |&gt;\n  drop_na() |&gt;\n  # Facet grid each column (class) has same scale, find limits to calculate bin\n  # width, limits are either implied by the constructed normal curce, or the raw\n  # data\n  group_by(class) |&gt;\n  mutate(\n    xmin = min(c(mean(load) - 3 * sd(load), load)),\n    xmax = max(c(mean(load) + 3 * sd(load), load))\n  ) |&gt;\n  group_by(treatment, class) |&gt;\n  nest(data = load) |&gt;\n  # generate data for a normal curve with mean and sd from observed data to\n  # cover 3 sd.\n  mutate(\n    norm_x = map(data, function (d) {\n      seq(\n        from = mean(d$load) - 3 * sd(d$load), \n        to = mean(d$load) + 3 * sd(d$load), \n        length.out = 100\n      )\n    }),\n    # scale curve to expected binwidth based on plot layout (same x scale across\n    # all fuel classes) multiplied by the number of observations. The histogram\n    # and normal curve should represent the same total area.\n    norm_y = map(data, function (d) {\n      dens &lt;- dnorm(unlist(norm_x), mean = mean(d$load), sd = sd(d$load))\n      dens * ((xmax - xmin) / bins) * nrow(d)\n    })\n  )\n\n\n\n\nCode\n# Break plot into two panels for higher resolution.\nggplot(filter(hist_dat, class %in% load_vars[1:3] )) +\ngeom_histogram(data = \\(x) unnest(x, data), aes(x = load), bins = bins) +\ngeom_line(data = \\(x) unnest(x, c(norm_x, norm_y)), aes(norm_x, norm_y)) +\nfacet_grid(treatment ~ class, scales = \"free\") +\nlabs(y = \"count\", x = \"Load Mg/ha\")\n\nggplot(filter(hist_dat, class %in% load_vars[4:6] )) +\ngeom_histogram(data = \\(x) unnest(x, data), aes(x = load), bins = bins) +\ngeom_line(data = \\(x) unnest(x, c(norm_x, norm_y)), aes(norm_x, norm_y)) +\nfacet_grid(treatment ~ class, scales = \"free\") +\nlabs(y = \"count\", x = \"Load Mg/ha\")\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\nFigure 4.7: Histotrams for the fuel loading response variables. A normal curve with the same mean as sd as the data (and scaled to the same area as that covered by the bins) has been superimposed for reference.\n\n\n\n4.3.1 Box-Cox transformation\nI would like to look at the effect of this transformation on the response data.\n\nboxcox &lt;- function(y, lambda1, lambda2) {\n  if(lambda1 == 0) {\n    ln(y + lambda2)\n  } else {\n    ((y + lambda2)^lambda1 - 1) / lambda1\n  }\n}\n\nload2(\"long\", treatment, all_of(load_vars)) |&gt;\n  group_by(class) |&gt; nest() |&gt; rowwise() |&gt;\n  mutate(\n    lambda = list(suppressMessages(geoR::boxcoxfit(data$load, lambda2 = TRUE)$lambda)),\n    load_bc = list(boxcox(data$load, lambda[1], lambda[2]))\n  ) |&gt;\n  unnest(c(data, load_bc)) |&gt; ungroup() |&gt;\n  myqqplot(load_bc)\n\n\n\n\nWhile, this looks somewhat more normal, the zeros end up being a little strange. I applied a separte transformation for each fuel class, but all treatments within a fuel class have the same transformation.\nFor MANOVA we are concered with the within group multivariate normality, the assumption does not appear to be met here either (Figure 4.8). The code output below indicates the rows with the greatest deviation from normal.\n\n\ntl &lt;- load2(\"wide\", treatment, all_of(load_vars))\nload_mod &lt;- lm(as.matrix(tl[-1]) ~ treatment, data = tl)\nm_dist &lt;- heplots::cqplot(load_mod)\n\nload2(\"wide\", everything())[order(m_dist, decreasing = TRUE)[1:10], ]\n\n# A tibble: 10 × 10\n   site    treatment corner   azi dufflitter onehr  tenhr hundhr thoushr    veg\n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 waldos  ha        nw        90       23.2 1.27  10.7    21.2    593.   16.2 \n 2 waldos  gs        ne       270      131.  1.72   6.46   16.1     36.3 199.  \n 3 whiskey ha        n        225       27.1 2.76  13.3    45.2     32.1   8.5 \n 4 waldos  gs        se       270       54.1 0.504 11.6    34.8    280.    0.9 \n 5 whiskey gs        n        225       27.1 0.124  2.09    8.56     0   121.  \n 6 waldos  ha        se       270      112.  2.32   0.930   2.54     0    46.1 \n 7 waldos  ha        se         0       15.5 3.34   6.53   13.7     17.7   6.31\n 8 waldos  gs        se         0       38.7 1.08   7.31   37.3     17.6   4.23\n 9 waldon  gs        e        225      112.  0.711  8.98   14.7     89.6  34.6 \n10 whiskey ld        s        315       77.3 0.390  5.38   31.9     59.8  74.2 \n\ntreatments = c(\"gs\", \"ha\", \"ld\", \"hd\")\npar(mfrow = c(2, 2))\ninvisible(lapply(treatments, \n  \\(x) heplots::cqplot(filter(tl, treatment == x)[-1], main = x)\n))\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\nFigure 4.8: Plot A assessed the multivariate normality of residuals given the model where all loading variabels are a function of the treatment group.\n\n\nA number of different tests of multivariate normaliy also confirm the lack of evidence for meeting this assumption (Table 4.2).\n\nall_mvn_tests &lt;- function(data) {\n  c(\"mardia\", \"hz\", \"royston\", \"dh\", \"energy\") |&gt;\n  map(\\(x) MVN::mvn(data = data, subset = \"treatment\", mvnTest = x)) |&gt;\n  map(\\(x) x$multivariateNormality) |&gt;\n  map(\\(x) bind_rows(x, .id = \"treatment\")) |&gt;\n  map_dfr(\\(x) \n    filter(x, Test != \"MVN\") |&gt; \n    mutate( across(where(is.factor), \\(f) as.numeric(as.character(f)))) |&gt; \n    select(1:2, statistic = 3, `p value`, Result = last_col())\n  )\n}\n\nall_mvn_tests(load2(\"wide\", treatment, all_of(load_vars))) |&gt;\n  knitr::kable(digits = 4)\n\n\n\nTable 4.2: Several different tests of multivariate normality indicate a lack of evidence to support this assumption.\n\n\ntreatment\nTest\nstatistic\np value\nResult\n\n\n\n\ngs\nMardia Skewness\n156.2386\n0.0000\nNO\n\n\ngs\nMardia Kurtosis\n4.1306\n0.0000\nNO\n\n\nha\nMardia Skewness\n198.9055\n0.0000\nNO\n\n\nha\nMardia Kurtosis\n5.8903\n0.0000\nNO\n\n\nhd\nMardia Skewness\n85.1671\n0.0072\nNO\n\n\nhd\nMardia Kurtosis\n0.7268\n0.4673\nYES\n\n\nld\nMardia Skewness\n122.0012\n0.0000\nNO\n\n\nld\nMardia Kurtosis\n3.2534\n0.0011\nNO\n\n\ngs\nHenze-Zirkler\n1.1979\n0.0000\nNO\n\n\nha\nHenze-Zirkler\n1.2792\n0.0000\nNO\n\n\nhd\nHenze-Zirkler\n1.0978\n0.0001\nNO\n\n\nld\nHenze-Zirkler\n1.1036\n0.0001\nNO\n\n\ngs\nRoyston\n79.7755\n0.0000\nNO\n\n\nha\nRoyston\n105.5921\n0.0000\nNO\n\n\nhd\nRoyston\n51.6106\n0.0000\nNO\n\n\nld\nRoyston\n65.9480\n0.0000\nNO\n\n\ngs\nDoornik-Hansen\n45.8165\n0.0000\nNO\n\n\nha\nDoornik-Hansen\n57.3566\n0.0000\nNO\n\n\nhd\nDoornik-Hansen\n65.4678\n0.0000\nNO\n\n\nld\nDoornik-Hansen\n38.4513\n0.0001\nNO\n\n\ngs\nE-statistic\n1.9038\n0.0000\nNO\n\n\nha\nE-statistic\n2.1409\n0.0000\nNO\n\n\nhd\nE-statistic\n1.6312\n0.0000\nNO\n\n\nld\nE-statistic\n1.7881\n0.0000\nNO\n\n\n\n\n\n\n\n\n4.3.2 Other distributions\nIf our data is not normally distributed, then what distribution is it? I’m going to assume what we are interested in the distribution of data within groups (treatments).\nI attemped to model the distribution of our conditional response data (fuel size class by treatment), but it mostly didn’t work.\nOur data is non-negative (contains zeros) continuous (for the most part) and highly variable in terms of skew and kurtosis. The presence of zeros, makes using the Gamma distribution more difficult. One possibility is a hurdle gamma, or Zero-adjusted Gamma.\n\nd &lt;- load2(\"long\", treatment, all_of(load_vars)) |&gt; \n  split(~class) |&gt; map(~split(.x, ~treatment))\n\n# imap_dfr(d, \\(x, y) tibble(class = y, treatment = names(x) )) |&gt;\n#   with(paste0(\"Treatment \", treatment, \", fuel class \", class)) |&gt;\n#   cat(sep = \"\\n\")\n\n# par(mfrow = c(2,2))\nwalk(d, ~ walk(.x, \\(x) fitdistrplus::descdist(x$load, boot = 111)))\n\n\n\n\n\n\n\n(a) Treatment gs, fuel class onehr\n\n\n\n\n\n\n\n(b) Treatment ha, fuel class onehr\n\n\n\n\n\n\n\n\n\n(c) Treatment hd, fuel class onehr\n\n\n\n\n\n\n\n(d) Treatment ld, fuel class onehr\n\n\n\n\n\n\n\n\n\n(e) Treatment gs, fuel class tenhr\n\n\n\n\n\n\n\n(f) Treatment ha, fuel class tenhr\n\n\n\n\n\n\n\n\n\n(g) Treatment hd, fuel class tenhr\n\n\n\n\n\n\n\n(h) Treatment ld, fuel class tenhr\n\n\n\n\n\n\n\n\n\n(i) Treatment gs, fuel class hundhr\n\n\n\n\n\n\n\n(j) Treatment ha, fuel class hundhr\n\n\n\n\n\n\n\n\n\n(k) Treatment hd, fuel class hundhr\n\n\n\n\n\n\n\n(l) Treatment ld, fuel class hundhr\n\n\n\n\n\n\n\n\n\n(m) Treatment gs, fuel class dufflitter\n\n\n\n\n\n\n\n(n) Treatment ha, fuel class dufflitter\n\n\n\n\n\n\n\n\n\n(o) Treatment hd, fuel class dufflitter\n\n\n\n\n\n\n\n(p) Treatment ld, fuel class dufflitter\n\n\n\n\n\n\n\n\n\n(q) Treatment gs, fuel class thoushr\n\n\n\n\n\n\n\n(r) Treatment ha, fuel class thoushr\n\n\n\n\n\n\n\n\n\n(s) Treatment hd, fuel class thoushr\n\n\n\n\n\n\n\n(t) Treatment ld, fuel class thoushr\n\n\n\n\n\n\n\n\n\n(u) Treatment gs, fuel class veg\n\n\n\n\n\n\n\n(v) Treatment ha, fuel class veg\n\n\n\n\n\n\n\n\n\n(w) Treatment hd, fuel class veg\n\n\n\n\n\n\n\n(x) Treatment ld, fuel class veg\n\n\n\n\nFigure 4.9: Skewness and kurtosis for fuel classes within treatments.\n\n\n\n\n\n4.3.3 Zero-adjusted Gamma\n\n# Zero adjusted Gamma distribution fit to histograms\nplot_zaga &lt;- function(i, d) {\n  m1 &lt;- gamlss::gamlssML(d, family = gamlss.dist::ZAGA())\n  hist(d, prob = TRUE, main = i, breaks = 20)\n  curve(gamlss.dist::dZAGA(x, mu = m1$mu, sigma = m1$sigma, nu = m1$nu), from = min(d), to = max(d), add = TRUE)\n}\n\nplot_zaga_class &lt;- function(data, class_name) {\n  if(missing(class_name)) {\n    class_name &lt;- as.character(substitute(data))\n    class_name &lt;- class_name[length(class_name)]\n  }\n  par(mfrow = c(2,2))\n  iwalk(data, ~plot_zaga(.y, .x))\n  mtext(class_name, cex = 1.6, side = 3, line = -2, outer = TRUE)\n}\n\nd2 &lt;- d |&gt; map(~ map(.x, \"load\"))\niwalk(d2, ~plot_zaga_class(.x, .y))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.4 Poisson fit of count data\nAll of the woody debris can be viewed as count data, and mean diameter. The mean diameter implies a distribution, which we actually have for coarse woody, but not for FWD.\nI wonder If I can model these counts as a Poisson process.\n\ncwd_counts &lt;- cwd |&gt; \n  group_by(site, treatment, corner, azi) |&gt;\n  summarize(count = sum(count), .groups = \"drop\")\n\npar(mfrow = c(2,2))\ncwd_counts |&gt;\n  group_by(treatment) |&gt;\n  group_walk( function (data, group) {\n    n &lt;- data$count\n    hist(n, main = group, prob = TRUE)\n    lines(0:max(n), dpois(0:max(n), mean(n)))\n  })\n\n\n\n\nThat didn’t look so great, so I also tried using a Zero-inflated Poisson distribution, but the model fit an extremely small value for the parameter that controls the probability of zero (referred to here as sigma) and so was effectively the same as the Poisson fit.\n\npar(mfrow = c(2,2))\ncwd_counts |&gt; \n  group_by(treatment) |&gt; nest() |&gt;\n  transmute(\n    sigma = map_dbl(data, \n      ~gamlss::gamlssML(.x$count, family = gamlss.dist::ZIP())$sigma\n    )\n  )\n\n# A tibble: 4 × 2\n# Groups:   treatment [4]\n  treatment    sigma\n  &lt;chr&gt;        &lt;dbl&gt;\n1 gs        2.61e-10\n2 ha        1.61e-10\n3 hd        2.42e-10\n4 ld        2.71e-10"
  },
  {
    "objectID": "fuel_data_exploration.html#sec-homogeneity-of-variance",
    "href": "fuel_data_exploration.html#sec-homogeneity-of-variance",
    "title": "4  Fuel data exploration",
    "section": "4.4 Homogeneity of variance",
    "text": "4.4 Homogeneity of variance\nThere seem to be some pretty big differences in the variance between treatments. This is likely to do with outliers. For linear regression, it is recommended that maximum variance ration should be below 4.\n\nmax_var &lt;- load2(\"long\", everything()) |&gt;\n  group_by(class, treatment) |&gt;\n  summarize(var = sd(load)^2, load = max(load), .groups = \"drop_last\") |&gt;\n  summarize(\n    max_var_rat = paste(\"Max. var. ratio: \", round(max(var) / min(var))),\n    x = 1.1, y = max(load) * 1.1, .groups = \"drop\")\n\nload2(\"long\", everything()) |&gt;\n  ggplot(aes(treatment, load)) +\n  geom_boxplot() +\n  geom_text(data = max_var, aes(x, y, label = max_var_rat, hjust = \"inward\")) +\n  facet_wrap(~class, scales = \"free\")"
  },
  {
    "objectID": "fuel_data_exploration.html#zeros",
    "href": "fuel_data_exploration.html#zeros",
    "title": "4  Fuel data exploration",
    "section": "4.5 Zeros",
    "text": "4.5 Zeros\nWe do have zeros, which is important if we want to employ a glm like Gamma, which is only defined for positive values.\n\nload2(\"long\", everything()) |&gt;\n  group_by(class, treatment) |&gt;\n  summarize(zeros = sum(load == 0), percent = zeros / n(), .groups = \"drop\") |&gt;\n  ggplot(aes(treatment, percent)) +\n  geom_col() +\n  geom_text(\n    aes(label = if_else(zeros == 0, NA, zeros), y = percent / 2),\n    color = \"gray70\", na.rm = TRUE\n  ) +\n  facet_wrap(~class) +\n  scale_y_continuous(labels = scales::percent)"
  },
  {
    "objectID": "fuel_data_exploration.html#correlation-of-response-variables",
    "href": "fuel_data_exploration.html#correlation-of-response-variables",
    "title": "4  Fuel data exploration",
    "section": "4.6 Correlation of response variables",
    "text": "4.6 Correlation of response variables\nI’m not sure if it’s important, but I was curious if the various fuel loading classes were correlated with each other. Either across the board, or within a given treatment.\n\nsuppressMessages(GGally::ggpairs(load2(\"wide\", all_of(load_vars))))\n\n\n\n\nFigure 4.10: Correlation among the response variables (fuel classes)."
  },
  {
    "objectID": "fuel_data_exploration.html#independence",
    "href": "fuel_data_exploration.html#independence",
    "title": "4  Fuel data exploration",
    "section": "4.7 Independence",
    "text": "4.7 Independence\nBecause of how are data were collected they are not independent. The current data is summarized at the transect level. At that level. We have two transects at each corner. Because of spatial autocorrelation, these may be correlated with each other. Corners (and thus transects) are nested within plots, and plots are within treatments. Each plot received a different treatment. What I’m not clear about is: should I include a random variable for plots, if I’m including a fixed effect for treatment?\nThere are also question about at what level to summarize/model the data. I’ve already averaged stations within transects for several variable that were collected at the station level (two within each transect). What are the trade-offs for either averaging at the corner level, or alternatively, analyzing our raw station data instead of averaging.\n\n\n\n\nZuur, Alain F., Elena N. Ieno, and Chris S. Elphick. 2010. “A Protocol for Data Exploration to Avoid Common Statistical Problems.” Methods in Ecology and Evolution 1 (1): 3–14. https://doi.org/10.1111/j.2041-210X.2009.00001.x."
  },
  {
    "objectID": "hypothesis_test.html#frequentist-approach",
    "href": "hypothesis_test.html#frequentist-approach",
    "title": "5  Hypothesis testing",
    "section": "5.1 Frequentist approach",
    "text": "5.1 Frequentist approach\nExploration of frequentist and Bayesian approaches can lead to more robust conclusions.\n\n5.1.1 Manova and multiple anovas\nThe often recommended Pillai’s Trace Test is robust to the normality assumption. Follow up with linear discriminant analysis, or multiple one-way anovas dependidng on research question. Using a Bonferroni correction for rejecting the null of alpha / m, for m hypothesis, we get an alpha of 0.008 for an alpha of 0.05 and 6 tests.\nThis suggests that it is unlikely that all treatemnts are equal.\n\nload_vars &lt;- c(\"onehr\", \"tenhr\", \"hundhr\", \"dufflitter\", \"thoushr\", \"veg\")\ntl &lt;- load2(\"wide\", treatment, all_of(load_vars))\nmyexpr &lt;- expr(cbind(!!!syms(load_vars)) ~ treatment)\ntest1 &lt;- do.call(\"manova\", list(myexpr, data = quote(tl)))\nsummary(test1)\n\n           Df  Pillai approx F num Df den Df    Pr(&gt;F)    \ntreatment   3 0.35947   2.7454     18    363 0.0001889 ***\nResiduals 124                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n5.1.2 Multiple one-way anovas\nOne way anova (using the welch test) can either assume constant variance or not. A levene test (using median) indicates onehr, tenhr, and veg may all have different variances between groups.\nThe one-way anova test results are the same though between equal and unequal variance assumptions. These tests support the notion that we can’t assume that the mean vegetatvie and onehr fuel loading are equal across all treatments, but there isn’t such evidence for the other fuel loading classes.\n\nd &lt;- load2(\"long\", treatment, all_of(load_vars)) |&gt; group_by(class)\n\nd |&gt; nest() |&gt;\n  rowwise() |&gt;\n  transmute(\n    levene = car::leveneTest(load ~ factor(treatment), data)[[3]][1],\n    welch_uneq_var = oneway.test(load ~ treatment, data)$p.value,\n    welch_eq_var = oneway.test(\n      load ~ treatment, var.equal = TRUE, data = data\n    )$p.value,\n  ) |&gt;\n  knitr::kable(digits = 3)\n\n\n\nTable 5.1: Levene tests suggest that variances are unequal across treatments for all fuel loading classes. Welches tests suggest that veg and onehr fuels may have different means among treatments.\n\n\nclass\nlevene\nwelch_uneq_var\nwelch_eq_var\n\n\n\n\nonehr\n0.043\n0.001\n0.000\n\n\ntenhr\n0.020\n0.491\n0.596\n\n\nhundhr\n0.937\n0.648\n0.636\n\n\ndufflitter\n0.118\n0.152\n0.136\n\n\nthoushr\n0.955\n0.919\n0.955\n\n\nveg\n0.006\n0.010\n0.001\n\n\n\n\n\n\nWe can use the Games Howell test for pairwise comparisons to follow up on the welches test for differences between means when there is unequal variance among groups. These p-values provide evidence that for onehr fuels, the mean value of ha is greater than gs and ld, and the mean value for hd is also greater than gs and ld. Also, for vegetation, gs is greater than ha only. While this test is robust to the assumptions of normality, some of our data is highly skewed. Also, because of the nesting of our data, observations are not independent, so our effective sample size is not what is assumed by this test.\n\ngh_test &lt;- d |&gt; rstatix::games_howell_test(load ~ treatment) |&gt;\n  filter(p.adj.signif != \"ns\") |&gt;\n  rstatix::add_y_position(scales = \"free\", step.increase = 0.5)\n\nggpubr::ggboxplot(d, x = \"treatment\", y = \"load\", facet.by = \"class\") +\n  facet_wrap(~class, scales = \"free\") +\n  ggpubr::stat_pvalue_manual(gh_test, label = \"p.adj\") +\n  scale_y_continuous(expand = expansion(mult = c(0.05, 0.1)))\n\n\n\n\nFigure 5.1: pair-wise tests using Games-Howell, for unequal variances across groups. This shows many statistically significant differences, but the assumption of independence, which is likely to have a significant effect on our effective sample size.\n\n\n\n\n\n\n5.1.3 Multi-level model\nWe have transects nested within plot corners, corners nested within plots, and plots nested within sites. We would like to detect a treatment effect, while accounting for the non-independence of this nested data structure. The following model, I believe, captures this grouping structure.\n\nform &lt;- load ~ treatment + (1 | site/treatment/corner)\n\nThis will estimate a group-wise intercept adjustments for each site, plot, and corner, based on modeled variances for each of these grouping levels.\n\nd &lt;- load2(\"long\", site, treatment, corner, all_of(load_vars)) |&gt; \n  group_by(class)\n\nm1 &lt;- d |&gt; nest() |&gt; rowwise() |&gt;\n  transmute(\n    mod = list(lme4::lmer(form, data = data)),\n    emmeans = list(emmeans::emmeans(mod, \"treatment\")),\n    pairs = list(as_tibble(pairs(emmeans, infer = TRUE)))\n  )\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nPairwise comparisons with Tukey adjustment for each of 6 multilevel models representing different fuel loading classes reveals that the only evidence for differences in means among treatments is with vegetation between the gs and ha treatments. Another sizeable difference in means is between gs and ha for the onehr fuels (Figure 5.2).\n\nselect(m1, pairs) |&gt; unnest(pairs) |&gt;\n  filter(p.value &lt;= 0.05) |&gt;\n  knitr::kable(digits = 3)\n\nAdding missing grouping variables: `class`\n\n\n\n\nTable 5.2: Pairwise comparisons among treatments with p-values &lt; 0.05 for 6 multilevel models. Only Veg, gs-ha comparison is statistically significant.\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nveg\ngs - ha\n25.31\n6.879\n9\n3.835\n46.786\n3.679\n0.022\n\n\n\n\n\n\n\ngroup_map(m1, ~ plot(.x$emmeans[[1]], comparisons = TRUE) + ggtitle(.x$class)) |&gt;\n  patchwork::wrap_plots()\n\n\n\n\nFigure 5.2: 95% confidence intervals and pairwise comparisons of means for 6 mixed models representing different fuel loading classes using package emmeans.\n\n\n\n\nHypothesis testing with multi-level models is not as straight forward with multi-level models. The problem, explained here is two fold. For GLMMs and unbalanced experimental designs, the null distribution for the F-statistic may not be F-distributed.\nFor us, we have a balanced design (I think) and so the F-statistic should be F distributed and degrees of freedom should be clear from the details of the design. Because of our balanced design, the Kenward-Rogers approach and “inner-outer” design approach (which is used by nlme::lme) give the same result of 9 DF for all of the pairwise tests.\nUsing the package pbkrtest we can get parametric bootstrap liklihood ratio statistics and test this statistic in a number of different ways. The PBtest should probably be the most reliable, but I’ve included descriptions of the others from the package documentation for reference. I’m also including an F-test in which degrees of freedom are estimated with Kenward-Rogers approach.\n\nLRT\n\nAssuming that LRT has a chi-square distribution.\n\nPBtest\n\nThe fraction of simulated LRT-values that are larger or equal to the observed LRT value.\n\nBartlett\n\nA Bartlett correction is of LRT is calculated from the mean of the simulated LRT-values\n\nGamma\n\nThe reference distribution of LRT is assumed to be a gamma distribution with mean and variance determined as the sample mean and sample variance of the simulated LRT-values.\n\nF\n\nThe LRT divided by the number of degrees of freedom is assumed to be F-distributed, where the denominator degrees of freedom are determined by matching the first moment of the reference distribution.\n\n\n\nd &lt;- load2(\"long\", all_of(c(transectid, load_vars)))\ndd &lt;- ungroup(d) |&gt; split(~class)\n\n\nif(file.exists(\"pmod.Rdata\")) {\n  load(\"pmod.Rdata\")\n} else {\n  cluster &lt;- parallel::makeCluster(rep(\"localhost\", parallel::detectCores()))\n\n  pmod &lt;- imap(dd, function(d, i) {\n    form &lt;- load ~ treatment + (1 | site/treatment/corner)\n    amod &lt;- lme4::lmer(form, d, REML = FALSE)\n    nmod &lt;- update(amod, . ~ . -treatment)\n    krtest &lt;- pbkrtest::KRmodcomp(amod, nmod) |&gt;\n      pluck(\"test\", \\(x) slice(x, 1)) |&gt;\n      rename(df = ndf) |&gt;\n      rownames_to_column(\"test\")\n    pbkrtest::PBmodcomp(amod, nmod, cl = cluster) |&gt;\n      pluck(summary, \"test\") |&gt;\n      rownames_to_column(\"test\") |&gt;\n      bind_rows(krtest) |&gt;\n      mutate(class = i, .before = 1) |&gt;\n      relocate(c(df, ddf, F.scaling), .after = stat)\n  }) |&gt; \n    list_rbind() |&gt;\n    filter(test %in% c(\"LRT\", \"PBtest\"))\n \n  parallel::stopCluster(cluster)\n  save(pmod, file = \"pmod.Rdata\")\n}\n\npmod |&gt; knitr::kable(digits = c(NA, NA, 2, 1, 2, 1, 4))\n\n\n\nTable 5.3: Liklihood ratio tests and parametric boot strap tests of model significance: whether the model with treatment, fits the data better than the intercept only model (adjusting for nesting structure).\n\n\nclass\ntest\nstat\ndf\nddf\nF.scaling\np.value\n\n\n\n\nonehr\nLRT\n7.48\n3\nNA\nNA\n0.0580\n\n\nonehr\nPBtest\n7.48\nNA\nNA\nNA\n0.1608\n\n\ntenhr\nLRT\n0.77\n3\nNA\nNA\n0.8576\n\n\ntenhr\nPBtest\n0.77\nNA\nNA\nNA\n0.8942\n\n\nhundhr\nLRT\n0.70\n3\nNA\nNA\n0.8725\n\n\nhundhr\nPBtest\n0.70\nNA\nNA\nNA\n0.9201\n\n\ndufflitter\nLRT\n6.05\n3\nNA\nNA\n0.1093\n\n\ndufflitter\nPBtest\n6.05\nNA\nNA\nNA\n0.1129\n\n\nthoushr\nLRT\n0.22\n3\nNA\nNA\n0.9739\n\n\nthoushr\nPBtest\n0.22\nNA\nNA\nNA\n0.9747\n\n\nveg\nLRT\n11.70\n3\nNA\nNA\n0.0085\n\n\nveg\nPBtest\n11.70\nNA\nNA\nNA\n0.0380\n\n\n\n\n\n\n\n\n5.1.4 Model checking\nTaking a look at residual vs. fitted and qqplots of the model, it looks like our residuals are not normally distributed and there is not constant variance.\n\n\nCode\n# These are functions to plot for each model, residuals vs fitted and normal\n# quantiles. The third function is a wrapper to do both.\nresid_plot &lt;- function(data) {\n  data |&gt;\n    ggplot(aes(fitted, resid)) +\n    geom_point() +\n    facet_wrap(~class, scales = \"free\") +\n    geom_hline(yintercept = 0)\n}\n\nqq_plot &lt;- function(data) {\n  data |&gt;\n    ggplot(aes(sample = resid)) +\n    stat_qq() +\n    stat_qq_line() +\n    facet_wrap(~class, scales = \"free\")\n}\n\nresid_qq_plot &lt;- function(data) {\n  data &lt;- unnest(data, c(resid, fitted))\n  list(\n    a = resid_plot(data),\n    b = qq_plot(data)\n  )\n}\n\nd &lt;- load2(\"long\", all_of(c(transectid, load_vars))) |&gt;\n  group_by(class) |&gt; nest() |&gt; rowwise()\n\n\n\nform &lt;- load ~ treatment + (1 | site/treatment/corner)\n\nmod1 &lt;- d |&gt;\n  mutate(\n    mod = list(lme4::lmer(form, data)),\n    fitted = list(fitted(mod)),\n    resid = list(resid(mod, type = \"pearson\", scaled = TRUE)),\n    .keep = \"unused\"\n  ) \n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\nresid_qq_plot(mod1) |&gt; patchwork::wrap_plots(ncol = 1)\n\n\n\n\nFigure 5.3: Residual vs fitted and normal quantile-quantile plots for a multi-level model with un-pooled treatment intercepts and partially pooled (random effects) for nested data. Fit using lme4 The residuals are not homogenous.\n\n\n\n\nI’ll try to control the variance by refitting the model with nlme::lme and using the weights argument. I’ll be using the pearson residuals which are corrected for heteroscedasticity.\nI had to use the control argument sigma = 1 for the model to fit. I’m not sure why, I read it in the documentation for nlme::varConstProp. I’m modeling variance as a constant proportion of the fitted values of the model. This seems to have cleaned up the variance.\n\nmod2 &lt;- d |&gt;\n  mutate(\n    mod = list(nlme::lme(\n      fixed = load ~ treatment,\n      random = ~ 1 | site/treatment/corner,\n      data = data,\n      weights = nlme::varConstProp(),\n      control = nlme::lmeControl(sigma = 1)\n    )),\n    fitted = list(fitted(mod)),\n    resid = list(resid(mod, type = \"pearson\")),\n    .keep = \"unused\"\n  )\n\npatchwork::wrap_plots(resid_qq_plot(mod2), ncol = 1)\n\n\n\n\nFigure 5.4: Same as Figure 5.3 but variance is modeled as a function fitted values, assuming a linear relationship. Fit with nlme. The (scaled) residuals are more homogenous now.\n\n\n\n\nI’ll compare AIC of the two models to see if one performs better than the other.\nfirst, I want to see if the models produced by lme and lmer are equivalent\n\nd |&gt;\n  mutate(\n    mod1 = list(lme4::lmer(form, data)),\n    mod2 = list(nlme::lme(\n      fixed = load ~ treatment,\n      random = ~ 1 | site/treatment/corner,\n      data = data\n    )),\n    .keep = \"unused\"\n  ) |&gt;\n  pivot_longer(-class, names_to = \"model\") |&gt;\n  rowwise() |&gt;\n  mutate(s = list(broom.mixed::tidy(value, effect = \"fixed\"))) |&gt;\n  select(class, model, s) |&gt;\n  unnest(everything()) |&gt;\n  arrange(class, term)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\n# A tibble: 48 × 9\n   class model effect term        estimate std.error statistic    df    p.value\n   &lt;fct&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 onehr mod1  fixed  (Intercept)   0.598      0.191     3.13     NA NA        \n 2 onehr mod2  fixed  (Intercept)   0.598      0.191     3.13     64  0.00265  \n 3 onehr mod1  fixed  treatmentha   0.603      0.256     2.35     NA NA        \n 4 onehr mod2  fixed  treatmentha   0.603      0.256     2.35      9  0.0433   \n 5 onehr mod1  fixed  treatmenthd   0.439      0.256     1.71     NA NA        \n 6 onehr mod2  fixed  treatmenthd   0.439      0.256     1.71      9  0.121    \n 7 onehr mod1  fixed  treatmentld   0.0697     0.256     0.272    NA NA        \n 8 onehr mod2  fixed  treatmentld   0.0697     0.256     0.272     9  0.792    \n 9 tenhr mod1  fixed  (Intercept)   3.75       0.784     4.78     NA NA        \n10 tenhr mod2  fixed  (Intercept)   3.75       0.784     4.78     64  0.0000106\n# ℹ 38 more rows\n\n\nthey seem equivalent enough, although the random effects variances estimated by lmer are somewhat smaller. Now, lets compare the two lme models. I’m fitting with REML because I’m not changing the fixed effects structure.\n\nmod_c &lt;- d |&gt;\n  mutate(\n    unweighted = list(nlme::lme(\n      fixed = load ~ treatment,\n      random = ~ 1 | site/treatment/corner,\n      data = data,\n    )),\n    weighted = list(nlme::lme(\n      fixed = load ~ treatment,\n      random = ~ 1 | site/treatment/corner,\n      data = data,\n      weights = nlme::varConstProp(),\n      control = nlme::lmeControl(sigma = 1),\n    )),\n    .keep = \"unused\"\n  ) \n\nmod_c |&gt;\n  mutate(aic = list(across(matches(\"weight\"), ~ AIC(.x))))|&gt;\n  select(aic)|&gt; unnest(aic) |&gt; knitr::kable()\n\nAdding missing grouping variables: `class`\n\n\n\n\nTable 5.4: Comparison of AIC between multilevel models with and without weights to account for heterogeneity of variance. The models with weights have consistently lower AIC, indicating better fit.\n\n\nclass\nunweighted\nweighted\n\n\n\n\nonehr\n251.8197\n220.0447\n\n\ntenhr\n589.9042\n486.5950\n\n\nhundhr\n892.3760\n860.7936\n\n\ndufflitter\n1180.4072\n1177.7730\n\n\nthoushr\n1428.6682\n1421.1023\n\n\nveg\n1171.0855\n1122.1291\n\n\n\n\n\n\nThis indicates that the model with modeled variance fits the data better than the model without.\nDoes this change our conclusions about the effect of the treatment?\n\nd &lt;- load2(\"long\", site, treatment, corner, all_of(load_vars)) |&gt; \n  group_by(class)\n\nm2 &lt;- d |&gt; nest() |&gt; rowwise() |&gt;\n  transmute(\n    mod = list(nlme::lme(\n      fixed = load ~ treatment,\n      random = ~ 1 | site/treatment/corner,\n      data = data,\n      weights = nlme::varConstProp(),\n      control = nlme::lmeControl(sigma = 1),\n    )),\n    emmeans = list(emmeans::emmeans(mod, \"treatment\")),\n    pairs = list(as_tibble(pairs(emmeans, infer = TRUE)))\n  )\n\n\nselect(m2, pairs) |&gt; unnest(pairs) |&gt;\n  filter(p.value &lt;= 0.05) |&gt;\n  knitr::kable(digits = 3)\n\nAdding missing grouping variables: `class`\n\n\n\n\nTable 5.5: Pairwise comparisons of treatments (four levels) with p-values &lt; 0.05 for six multilevel models with variance modeled as a linear relationsihp with the fitted value.\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nveg\ngs - ha\n25.31\n7.12\n9\n3.083\n47.538\n3.555\n0.026\n\n\n\n\n\n\n\ngroup_map(m2, ~ plot(.x$emmeans[[1]], comparisons = TRUE) + ggtitle(.x$class)) |&gt;\n  patchwork::wrap_plots()\n\n\n\n\nFigure 5.5: 95% confidence intervals and pairwise comparisons of means for 6 mixed models representing different fuel loading classes using a model with variance modeled as a linear relationsihp with the fitted value.\n\n\n\n\nNo, it doesn’t seem to make much of a difference.\n\n\n5.1.5 Other random effects structures\nI’m not sure I’m using the correct random effects specification. The somewhat confusing thing is that I have a random effects nested above and below my fixed effect. This means that when I specify my random effect using the nesting notation: 1 | site/treatment/corner, I’m estimating a variance for corner:treatment:site, treatment:site, and site. The interaction of treatment and site here is analagous to a plot effect, of which there are 16."
  },
  {
    "objectID": "hypothesis_test.html#bayesian-mode",
    "href": "hypothesis_test.html#bayesian-mode",
    "title": "5  Hypothesis testing",
    "section": "5.2 Bayesian mode",
    "text": "5.2 Bayesian mode\nI’ll use brms with the same formula I used for the lmm above. First we’ll reload our data.\nAll the data is nested to facilitate modeling each fuel class separately. We’ll look at the average loading to get an idea of the data.\n\nd &lt;- load2(\"long\", all_of(c(transectid, load_vars))) |&gt;\n  group_by(class) |&gt; nest() |&gt; rowwise()\n\n\n5.2.1 Gaussian model\nI started out using a Gaussian model for load, with mostly default priors on the random effects. This was for convenience more than for anything. A model with positive support only, and more approriate priors would improve the model. This parameterization also results in the first treatment level (gs) being interpreted as the global intercept. This implies the unfortunate assumption that the other levels entail more variability as their priors result from the combination of the intercept prior as well as the treatment prior, wheres the prior for the first level only contains the variability in the intercept. Thus, parameterizing the model by removing the intercept for the main effect would allow equivalent interpretations of all four treatment levels. I use this approach below in the Gamma model.\n\n5.2.1.1 Priors\nWe are using mostly uninformative priors for our un-pooled (fixed effect) estimates of treatment intercepts. They are all set as normal distributions, centered at the median of the fuel load with a sd of 2.5 times the sd of the data. While it is not possible to have negative values here, a current limitation of the brms package is that you can’t put bounds on individual coefficients, and we would not want to contstrain our treatment effects to be positive, as they are centered around the mean and will be both positive and negative.\n\n# add calcualted priors to the data\nlibrary(ggdist)\n\nd &lt;- mutate(d, \n  priors = list(\n    brms::set_prior(\n      str_glue(\n        \"normal( {round(median(data$load))}, {round(2.5 * sd(data$load))} )\"\n      ), \n      # lb = 0, \n      class = \"b\", coef = \"Intercept\"\n    ) +\n    brms::set_prior(\n      str_glue(\n        \"normal( 0, {round(1.5 * sd(data$load))} )\"\n      ), \n      class = \"b\"\n    )\n    # brms::set_prior(\"student_t(3, 0, 2.5)\",  lb = 0, class = \"sd\")\n  )\n)\n\n# Plot the priors\nd |&gt;\n  mutate(\n    prior_dist = list(tidybayes::parse_dist(priors) |&gt; \n    rename(dist_class = class))\n  ) |&gt;\n  unnest(prior_dist) |&gt;\n  ggplot(aes(ydist = .dist_obj, color = interaction(coef, dist_class))) +\n    stat_slab(normalize = \"panels\", fill = NA) +\n    stat_pointinterval(position = position_dodge(width = 0.3, preserve = \"single\")) +\n    geom_text(\n      aes(label = format(.dist_obj), y = mean(.dist_obj), x = 0.97, vjust = 0),\n      position = position_dodge(width = 0.3), show.legend = FALSE\n    ) +\n    coord_flip() +\n    facet_wrap(~ class, scales = \"free_x\") +\n    scale_color_hue(name = \"prior\", labels = c(\"treatment\", \"intercept\", \"sd\"))\n\n\n\n\nFigure 5.6: Normal priors for the intercept are centered at the median and sd of 2.5 times the sd of the data. For the fixed effect treatment, they are centered at zero with the a standard deviation equal to 1.5 sd of the data. Fixed treatments are relative to the first treatment level (GS) which is used as the intercept.\n\n\n\n\n\n\n5.2.1.2 Model fitting\nNow we’ll fit the model.\n\nform &lt;- load ~  0 + Intercept + treatment + (1 | site/treatment/corner)\n\nbf2 &lt;- mutate(d,\n  mod = list(brms::brm(form, data,\n    warmup = 3000,\n    iter = 4000,\n    cores = 4,\n    control = list(adapt_delta = 0.99),\n    prior = priors,\n    sample_prior = TRUE,\n    file = paste0(\"fits/bf2_\", class)\n  ))\n)\n\n\n\n5.2.1.3 Prior check\n\n\nCode\n# Plod density of model priors against posterior to get a sense of the\n# informativeness of the prior. Average over all fixed effects.\n\nplot_pri_post &lt;- function(mod) {\n  tidy_draws(mod) |&gt; \n  select(c(matches(\"b_\"), matches(\"sd\"), matches(\"sigma\"), matches(\"hu\"))) |&gt;\n  pivot_longer(everything()) |&gt;\n  mutate(\n    name = if_else(\n      str_starts(name, \"prior\"),\n      name, \n      paste0(\"posterior_\", name)\n    )\n  ) |&gt;\n  separate_wider_regex(name, \n      c(phase = \"prior|posterior\", \"_\", name = \".*\")\n  ) |&gt;\n  mutate(name = str_remove(name, \"__.*\")) |&gt;\n  ggplot(aes(x = value, color = phase, fill = phase)) +\n  stat_slab(normalize = \"panels\", alpha = 3/4) +\n  stat_pointinterval(\n    position = position_dodge(width = 0.3, preserve = \"single\")\n  ) +\n  facet_wrap(~ name, scales = \"free\", labeller = fuel_class_labeller) +\n  labs(x = \"Parameter value\", y = \"Density\") +\n  scale_fill_manual(values = c(\"gray60\", \"gray30\")) +\n  scale_color_manual(values = c(\"gray60\", \"gray30\"))\n}\n\n\n\n\n\n\n\nSampled prior and posterior distributions for Gaussian model variables that have priors, for one fuel class. These include the sd for the random effects as well as sigma: the model residuals, and the fixed effect of treatment. The plots for the other fuel classes look similar.\n\n\n\n\n\n\n5.2.1.4 Posterior Predictive check (Gaussian)\n\n\nCode\nposterior_predictive_check &lt;- function(models) {\n  models &lt;- mutate(models,\n    pred = list(tidybayes::add_predicted_draws(data, mod, ndraws = 15)),\n    lims = list(tibble(\n      xmin = min(\n        quantile(data$load, .001),\n        quantile(pred$.prediction, .001)\n      ),\n      xmax = max(\n        quantile(data$load, .999),\n        quantile(pred$.prediction, .999)\n      ))\n    )\n  )\n  ggplot() + \n  geom_line(\n    data = unnest(models, data),\n    aes(x = load),\n    stat = \"density\", size = 1\n  ) +\n  geom_line(\n    data = unnest(models, pred),\n    aes(x = .prediction, group = .draw),\n    stat = \"density\", alpha = 0.15, size = 1\n  ) +\n  facet_wrap(~class, scales = \"free\", labeller = fuel_class_labeller) +\n  coord_cartesian_panels(\n    panel_limits = unnest(select(models, lims), lims)\n  )\n}\n\n\nHere is a posterior predictive check.\n\nposterior_predictive_check(bf2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nAdding missing grouping variables: `class`\n\n\n\n\n\nFigure 5.7: Density of the observed data (y) plotted against 10 random draws from the posterior predictive distribution.\n\n\n\n\nThe Gaussian distribution is symmetric and doesn’t capture well the peak near zero and the long right tail of our observed values for most of the fuel classes. It also dramatically overpredicts negative values (which are absent from our data, despite the fact that the density smoothing of the observed values seems to suggest there are some.)\nThe fact that the model predicts negative values suggests that it is not right for our data, and could potentially be biasing comparissons between treatments. A Gamma distribution for the response makes more sense.\n\n\n\n5.2.2 Gamma model\nThe gamma model has support for only postive values, which makes sense for our weight per area data. Initial data exploration also revealed that the gamma appears to be a good fit for our data. This makes sense, as our data is fundamentally transformed count data, and the gamma distribution is the continuous generalization of the negative binomial, which is used for modeling count data.\n\n5.2.2.1 Formula\nI’m assuming the outcome is hurdle gamma distributed.\n\\[\n\\operatorname{HurdleGamma}(y \\mid \\mu, \\text{shape}, \\text{hu}) =\n\\begin{cases}\n\\text{hu}\n  &\\quad\\text{if } y = 0, \\text{ and}\\\\\n(1 - \\text{hu}) \\operatorname{Gamma}(y \\mid \\alpha, \\beta)\n  &\\quad\\text{if } y &gt; 0,\n\\end{cases}\n\\]\nWhere \\(\\mu = \\frac{\\alpha}{\\beta}\\), and \\(\\text{shape} = \\beta\\). This is a mixture model where the proportion of zeros are estimated and the rest of the observations are assumed to be gamma distributed. Because the gamma distribution doesn’t incldue support for zero, we don’t have to worry about any “overlapping” zero predictions from the gamma portion of the model.\nOur model form is as follows:\n\\[  \n\\begin{align*}\ny_i &\\sim \\operatorname{HurdleGamma}(\\mu, \\text{shape}, \\text{hu})\\\\\n\\log(\\mu_i) &= \\bar{\\alpha} +\n  U_{\\text{site}[i]}\\sigma_{u} +\n  V_{\\text{plot}[i]}\\sigma_{v} +\n  W_{\\text{corner}[i]}\\sigma_{w} +\n  \\beta_{treatment[i]}\\\\\n\\bar{\\alpha} &\\sim \\operatorname{normal}(M, S)\\\\\nU_j &\\sim \\operatorname{normal}(0, 1) \\qquad \\text{for } j=1 \\dots 4\\\\\nV_j &\\sim \\operatorname{normal}(0, 1) \\qquad \\text{for } j=1 \\dots 16\\\\\nW_j &\\sim \\operatorname{normal}(0, 1) \\qquad \\text{for } j=1 \\dots 64\\\\\n\\sigma_u &\\sim \\operatorname{normal^+}(0, 1)\\\\\n\\sigma_v &\\sim \\operatorname{normal^+}(0, 1)\\\\\n\\sigma_w &\\sim \\operatorname{normal^+}(0, 1)\\\\\n\\beta_j &\\sim \\operatorname{normal}(0, 1) \\qquad \\text{for } j=1 \\dots 4\\\\\n\\text{shape} &\\sim \\operatorname{Gamma}(0.01, 0.01)\\\\\n\\text{hu} &\\sim \\operatorname{beta}(1, 1)\n\\end{align*}\n\\tag{5.1}\\]\nHere is an interpretation for each line of the above model.\n\nObservations of transects are assumed to be hurdle gamma distributed given the fixed and random intercept, grouping structure.\nThe log fuel loading for a transect is a function of the grand mean plus random effects for site, plot, and corner, and fixed effect of treatment. Each \\(U\\), \\(V\\), and \\(W\\), is accompanied by a \\(\\sigma\\) parameter. This is the non-centered parameterization and is equivalent to just defining a distribution with a scale of \\(\\sigma\\), but factoring out the scale parameter leads to better numerical stability in the Markov-chain Monte Carlos algorithm.\nThe prior for bar-a, the grand mean, is assumed to be normally distributed with mean and SD of \\(M\\) and \\(S\\), repsectively. These are chosen based on the mean and 2.5 times the SD of the data converted to produce a log normal distribution on the log scale. This should be a minimally informative prior.\nEach of the four unique site effects come from a normal distribution centered on zero.\nSame as above for 16 plot\nSame as above for 64 corners.\nThe site effects come from a normal distribution with a SD that is assumed to be from a postive constrained, standard normal distribution. On the response scale, about 95% of the mass of the joint distribution of this line and line 4 (\\(U_j\\sigma_u\\) is in the interval \\([0.11,8.9]\\)), which implies that we would be supprised to see any site with a load more than 9 times any other.\nSame as 7 for plots.\nSame as 7 for corners.\nThe fixed effect of treatment on the log scale is assumed to be from a standard normal distribution. On the response scale, we would be supprised if the effect of any treatment was more than seven times any other.\nThis is the default prior for the shape parameter of the gamma distribution in brms. It is chosen to be minimally informative.\nOur prior assumption about the proportion of zeros is also minimally informative, it is mostly uniform between 0 and 1.\n\nOne problem with these independent prior assumptions is that the multiplicative effect of the terms on the response scale (due to exponentiation) can lead to impossibly large predictiontions, particularly when multiple random or fixed effect parameters are sampled simultaneosly in their upper tails. For instance, if the effects of site, plot, and corner where sampled at 2 (on the log scale) and the effect of treatment, at 1.8, this could lead to a predicted \\(\\mu\\) more than 2,400 times greater than the grand mean.\nA better choice would be to use a joint prior, like the Dirichlet, so a prior could be set on the total variance and the individual component variances would vary (in either a eqaul, or unequally weighted fashioin) in their repsective proportions of that total.\nIn this regard, with either independent, or independet priors, further work could be done on establshing the relative importance of each variance component and setting priors that reflect these differences. For our nesting structure, this means defining the length scale over which fuels are expected to vary most/least, from 10 m to 50 m to &gt; 100 m, to thousands of meters.\n\n\n5.2.2.2 Compute priors\nFirst I’ll set my prior. This is complicated because we are now working with a log link. According to Solomon Kurz, this is how to transform your mean and sd for a normal distribution on the identity scale, to the equivalent normal distribution on the log scale (a lognormal distribution). This is what we’ll use for our prior on the grand mean (\\(\\bar{\\alpha}\\)).\n\nd &lt;- load2(\"long\", all_of(c(transectid, load_vars))) |&gt;\n  group_by(class) |&gt; nest() |&gt; rowwise()\n\nlnp &lt;- function(data) {\n  # Desired values\n  m &lt;- mean(data)\n  s &lt;- 2 * sd(data)\n  # use the equations\n  mu    &lt;- log(m / sqrt(s^2 / m^2 + 1))\n  sigma &lt;- sqrt(log(s^2 / m^2 + 1))\n  # output mu and sigma on lognormals own scale\n  list(mu = round(mu, 2), sigma = round(sigma, 2))\n}\n\n\n# data frame to plot the priors\npd &lt;- d |&gt;\n  mutate(\n    priors = list(brms::set_prior(\n      str_glue(\"normal({mu}, {sigma})\", .envir = lnp(data$load))\n    )),\n    prior_dist = list( tidybayes::parse_dist(priors)),\n    lims = list(\n      tibble(\n        xmin = quantile(exp(prior_dist$.dist_obj), .01),\n        xmax = quantile(exp(prior_dist$.dist_obj), .99)\n      )\n    )\n  ) |&gt;\n  rename(fuel_class = class)\n\nggplot(data = unnest(pd, prior_dist, names_repair = \"universal\")) +\ntidybayes::stat_halfeye(\n  aes(xdist = exp(.dist_obj)), \n  normalize = \"panels\"\n) +\ngeom_text(aes(label = format(.dist_obj), x = mean(exp(.dist_obj)), y = 0.97, hjust = 0)) +\nfacet_wrap(~ fuel_class, scales = \"free_x\", labeller = fuel_class_labeller) +\ncoord_cartesian_panels(panel_limits = unnest(select(pd, lims), lims)) +\nlabs(x = expression(exp(bar(alpha))))\n\n\n\n\nFigure 5.8: Normal priors for the grand mean (global intercept) centered at the median and sd of 2.5 times the sd of the data.\n\n\n\n\n\n\n5.2.2.3 Prior predictive check\nThe below code ran with very many divergent transitions and the results did not really make sense. This may have to do with a comment I found here: independent priors specified on the log link usually don’t sample.\nAn alternative is refitting the whole model with different priors and comparing the output to infer the prior influence. I’m not going to do that right now.\nI will though, sample from the priors while fitting the model in order to plot those along with the posterior for our variables of interest.\n\n# prior_only_bf4a &lt;- mutate(bf4a, \n#   mod = list(update(mod, \n#     sample_prior = \"only\", \n#     cores = 4,\n#     warmup = 4000,\n#     iter = 5000,\n#     control = list(adapt_delta = .99)\n#   ))\n# )\n\n\n\n5.2.2.4 Model fitting\nHere I actually fit several models. They were each explored to some degree in an iterative process of model fitting and checking. For the sake of brevity, I I have focussed on just one of them (bf4a) for displaying final results. This model is the one described mathematically above.\n\nbf4a &lt;- mutate(d,\n  priors = list(\n    set_prior(\n      str_glue(\"normal({mu}, {sigma})\", .envir = lnp(data$load)),\n      nlpar = \"a\", coef = \"Intercept\"\n    ) +\n    set_prior(\"normal(0, 1)\", nlpar = \"a\", class = \"sd\") +\n    set_prior(\"normal(0, 1)\", nlpar = \"b\", coef = \"treatmentgs\") +\n    set_prior(\"normal(0, 1)\", nlpar = \"b\", coef = \"treatmentha\") +\n    set_prior(\"normal(0, 1)\", nlpar = \"b\", coef = \"treatmenthd\") +\n    set_prior(\"normal(0, 1)\", nlpar = \"b\", coef = \"treatmentld\")\n  ),\n  mod = list(brms::brm(\n    brms::bf(\n      load ~ a + b,\n      a ~ 1 + (1 | site/treatment/corner),\n      b ~ 0 + treatment,\n      nl = TRUE),\n    data,\n    warmup = 4000,\n    iter = 5000,\n    cores = 4,\n    control = list(adapt_delta = 0.99),\n    family = brms::hurdle_gamma(),\n    prior = priors,\n    sample_prior = TRUE,\n    file = paste0(\"fits/bf4a_\", class)\n  ))\n)\n\n\n\nCode\nbf3 &lt;- mutate(d,\n  priors = list(brms::set_prior(\n    str_glue(\"normal({mu}, {sigma})\", .envir = lnp(data$load))\n  )),\n  mod = list(brms::brm(form, data,\n    warmup = 5000,\n    iter = 6000,\n    cores = 4,\n    control = list(adapt_delta = 0.99),\n    family = brms::hurdle_gamma(),\n    prior = priors,\n    file = paste0(\"fits/bf3_\", class)\n  ))\n)\n\nbf4 &lt;- mutate(d,\n  priors = list(\n    set_prior(\n      str_glue(\"normal({mu}, {sigma})\", .envir = lnp(data$load)),\n      nlpar = \"a\", coef = \"Intercept\"\n    ) +\n      set_prior(\"student_t(3, 0, 0.35)\", nlpar = \"a\", class = \"sd\") +\n      set_prior(\"student_t(3, 0, 0.35)\", nlpar = \"b\", coef = \"treatmentgs\") +\n      set_prior(\"student_t(3, 0, 0.35)\", nlpar = \"b\", coef = \"treatmentha\") +\n      set_prior(\"student_t(3, 0, 0.35)\", nlpar = \"b\", coef = \"treatmenthd\") +\n      set_prior(\"student_t(3, 0, 0.35)\", nlpar = \"b\", coef = \"treatmentld\")\n  ),\n  mod = list(brms::brm(\n    brms::bf(\n      load ~ a + b,\n      a ~ 1 + (1 | site / treatment / corner),\n      b ~ 0 + treatment,\n      nl = TRUE\n    ),\n    data,\n    warmup = 4000,\n    iter = 5000,\n    cores = 4,\n    control = list(adapt_delta = 0.99),\n    family = brms::hurdle_gamma(),\n    prior = priors,\n    sample_prior = TRUE,\n    file = paste0(\"fits/bf4_\", class)\n  ))\n)\n\n\nbf5 &lt;- mutate(d,\n  priors = list(brms::set_prior(\n    str_glue(\"normal({mu}, {sigma})\", .envir = lnp(data$load))\n  )),\n  mod = list(brms::brm(\n    brms::bf(load ~ 0 + Intercept + (1 | site / treatment / corner) + (1 | treatment)),\n    data,\n    warmup = 4000,\n    iter = 5000,\n    cores = 4,\n    control = list(adapt_delta = 0.99),\n    family = brms::hurdle_gamma(),\n    prior = priors,\n    file = paste0(\"fits/bf5_\", class)\n  ))\n)\n\n\n\n\n5.2.2.5 Model summaries\n\nmy_summary &lt;- function(mod) {\n  as_draws_df(mod) |&gt;\n    select(starts_with(c(\"b_\", \"sd_\")), shape, hu) |&gt;\n    posterior::summarize_draws(\n      \"mean\", ~posterior::quantile2(.x, c(0.05, .5, .95)), \"rhat\", \"ess_bulk\",\n      \"ess_tail\"\n    )\n}\n\nbf4a |&gt;\n  mutate(\n    summary = list(my_summary(mod))\n  ) |&gt;\n  select(summary) |&gt;\n  unnest(summary) |&gt;\n  mutate(\n    variable = str_remove_all(variable, \"b_._|_a_\")\n  ) |&gt;\n  knitr::kable(digits = c(NA, NA, 2, 2, 2, 2, 2, 0, 0))\n\n\n\nTable 5.6: Summaries of parameters and dianostics. Rhat is a measure of model convergence, well mixed chains have an Rhat of 1, bulk and tail ESS are measures of effective sample size.\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass\nvariable\nmean\nq5\nq50\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nonehr\nIntercept\n-0.32\n-1.13\n-0.31\n0.52\n1\n1579\n2281\n\n\nonehr\ntreatmentgs\n-0.31\n-1.13\n-0.31\n0.49\n1\n1613\n1992\n\n\nonehr\ntreatmentha\n0.43\n-0.41\n0.44\n1.24\n1\n1603\n2087\n\n\nonehr\ntreatmenthd\n0.27\n-0.57\n0.27\n1.07\n1\n1588\n1945\n\n\nonehr\ntreatmentld\n-0.13\n-0.96\n-0.14\n0.68\n1\n1625\n2167\n\n\nonehr\nsd_site_Intercept\n0.37\n0.04\n0.31\n0.90\n1\n1171\n1686\n\n\nonehr\nsd_site:treatment_Intercept\n0.35\n0.14\n0.34\n0.61\n1\n1042\n1240\n\n\nonehr\nsd_site:treatment:corner_Intercept\n0.13\n0.01\n0.11\n0.28\n1\n1169\n1998\n\n\nonehr\nshape\n2.37\n1.89\n2.35\n2.90\n1\n2911\n2432\n\n\nonehr\nhu\n0.01\n0.00\n0.01\n0.02\n1\n4988\n2128\n\n\ntenhr\nIntercept\n1.09\n0.28\n1.10\n1.88\n1\n1251\n1705\n\n\ntenhr\ntreatmentgs\n0.25\n-0.57\n0.24\n1.08\n1\n1324\n1775\n\n\ntenhr\ntreatmentha\n0.04\n-0.77\n0.04\n0.87\n1\n1383\n1839\n\n\ntenhr\ntreatmenthd\n-0.02\n-0.86\n-0.03\n0.82\n1\n1356\n2047\n\n\ntenhr\ntreatmentld\n0.09\n-0.73\n0.08\n0.93\n1\n1351\n1885\n\n\ntenhr\nsd_site_Intercept\n0.25\n0.02\n0.18\n0.73\n1\n1433\n1921\n\n\ntenhr\nsd_site:treatment_Intercept\n0.39\n0.18\n0.38\n0.65\n1\n1012\n1022\n\n\ntenhr\nsd_site:treatment:corner_Intercept\n0.11\n0.01\n0.10\n0.26\n1\n1475\n1607\n\n\ntenhr\nshape\n2.56\n2.02\n2.55\n3.16\n1\n2892\n2390\n\n\ntenhr\nhu\n0.06\n0.03\n0.06\n0.10\n1\n4985\n2464\n\n\nhundhr\nIntercept\n2.25\n1.42\n2.24\n3.05\n1\n1269\n1604\n\n\nhundhr\ntreatmentgs\n0.25\n-0.56\n0.25\n1.07\n1\n1232\n2008\n\n\nhundhr\ntreatmentha\n0.02\n-0.77\n0.02\n0.84\n1\n1272\n1943\n\n\nhundhr\ntreatmenthd\n0.02\n-0.77\n0.02\n0.82\n1\n1265\n1928\n\n\nhundhr\ntreatmentld\n0.14\n-0.65\n0.14\n0.95\n1\n1283\n1958\n\n\nhundhr\nsd_site_Intercept\n0.30\n0.03\n0.24\n0.79\n1\n1390\n1726\n\n\nhundhr\nsd_site:treatment_Intercept\n0.25\n0.06\n0.25\n0.48\n1\n1041\n1190\n\n\nhundhr\nsd_site:treatment:corner_Intercept\n0.15\n0.01\n0.13\n0.32\n1\n1168\n1580\n\n\nhundhr\nshape\n2.81\n2.18\n2.79\n3.55\n1\n3125\n2709\n\n\nhundhr\nhu\n0.12\n0.08\n0.12\n0.17\n1\n5853\n2515\n\n\ndufflitter\nIntercept\n3.72\n2.99\n3.73\n4.47\n1\n1553\n1901\n\n\ndufflitter\ntreatmentgs\n0.11\n-0.62\n0.13\n0.82\n1\n1532\n2141\n\n\ndufflitter\ntreatmentha\n-0.05\n-0.77\n-0.04\n0.65\n1\n1535\n2291\n\n\ndufflitter\ntreatmenthd\n0.24\n-0.49\n0.25\n0.95\n1\n1508\n2087\n\n\ndufflitter\ntreatmentld\n0.06\n-0.66\n0.07\n0.77\n1\n1538\n2153\n\n\ndufflitter\nsd_site_Intercept\n0.30\n0.07\n0.24\n0.72\n1\n1543\n1678\n\n\ndufflitter\nsd_site:treatment_Intercept\n0.10\n0.01\n0.08\n0.25\n1\n1634\n1813\n\n\ndufflitter\nsd_site:treatment:corner_Intercept\n0.10\n0.01\n0.09\n0.22\n1\n1376\n1644\n\n\ndufflitter\nshape\n3.94\n3.16\n3.91\n4.82\n1\n4235\n3308\n\n\ndufflitter\nhu\n0.01\n0.00\n0.01\n0.02\n1\n4313\n1896\n\n\nthoushr\nIntercept\n3.62\n2.70\n3.63\n4.51\n1\n1973\n2171\n\n\nthoushr\ntreatmentgs\n0.33\n-0.61\n0.34\n1.25\n1\n2220\n2403\n\n\nthoushr\ntreatmentha\n-0.07\n-1.00\n-0.08\n0.87\n1\n2346\n2997\n\n\nthoushr\ntreatmenthd\n0.26\n-0.67\n0.25\n1.20\n1\n2520\n2661\n\n\nthoushr\ntreatmentld\n-0.11\n-1.03\n-0.11\n0.83\n1\n2219\n2642\n\n\nthoushr\nsd_site_Intercept\n0.41\n0.04\n0.34\n1.04\n1\n2147\n2423\n\n\nthoushr\nsd_site:treatment_Intercept\n0.57\n0.20\n0.56\n0.95\n1\n826\n737\n\n\nthoushr\nsd_site:treatment:corner_Intercept\n0.39\n0.07\n0.40\n0.68\n1\n900\n1562\n\n\nthoushr\nshape\n1.42\n1.07\n1.40\n1.83\n1\n2304\n2895\n\n\nthoushr\nhu\n0.25\n0.19\n0.25\n0.32\n1\n7275\n2908\n\n\nveg\nIntercept\n2.76\n1.87\n2.75\n3.67\n1\n1302\n1939\n\n\nveg\ntreatmentgs\n0.52\n-0.37\n0.53\n1.39\n1\n1445\n2172\n\n\nveg\ntreatmentha\n-0.41\n-1.28\n-0.41\n0.48\n1\n1560\n2223\n\n\nveg\ntreatmenthd\n-0.01\n-0.90\n0.00\n0.86\n1\n1516\n2153\n\n\nveg\ntreatmentld\n0.20\n-0.68\n0.20\n1.06\n1\n1422\n2117\n\n\nveg\nsd_site_Intercept\n0.47\n0.07\n0.40\n1.11\n1\n1248\n1128\n\n\nveg\nsd_site:treatment_Intercept\n0.38\n0.06\n0.37\n0.71\n1\n640\n711\n\n\nveg\nsd_site:treatment:corner_Intercept\n0.50\n0.24\n0.51\n0.73\n1\n637\n787\n\n\nveg\nshape\n1.97\n1.46\n1.94\n2.56\n1\n1160\n1893\n\n\nveg\nhu\n0.02\n0.00\n0.01\n0.03\n1\n5969\n2096\n\n\n\n\n\n\n\n\n5.2.2.6 Prior check\nWhile it was not possible to sample from the prior using the MCMC sampler which was likely due to the extremely long tails implied by our exponentiated, independent priors, another way of describing priors is simply sampling random from random number generators. The results of these random samples are plotted against the posterior distributions for the variables that we set priors for.\n\nplot_pri_post(bf4a$mod[[1]]) + ggtitle(\"1-hr\")\n# plot_pri_post(bf4a$mod[[2]]) + ggtitle(\"10-hr\")\n# plot_pri_post(bf4a$mod[[3]]) + ggtitle(\"100-hr\")\n# plot_pri_post(bf4a$mod[[4]]) + ggtitle(\"duff & litter\")\n# plot_pri_post(bf4a$mod[[5]]) + ggtitle(\"1000-hr\")\n# plot_pri_post(bf4a$mod[[6]]) + ggtitle(\"Vegetation\")\n\n\n\n\nFigure 5.9: Posterior and prior distributions for variables we set priors for, for one fuel class model.\n\n\n\n\n\n\n5.2.2.7 Posterior predictive check\n\n\nAdding missing grouping variables: `class`\n\n\n\n\n\nFigure 5.10: Density of the observed data (y) plotted against 10 random draws from the posterior predictive distribution.\n\n\n\n\nThe gamma model fits the data better. There are no predictions below zero anymore. It does seem like the gamma distribution tends to predict higher densities of lower values than we observed, as seen in the plots for the tenhr, thoushr, and veg fuel classes. But generally, the predictions appear to agree with the observed data pretty well.\n\n\n5.2.2.8 Expected value of the posterior predictive\n\n# get posterior predictions for treatments for models for all fuel classes, \n# ignoring random effects. These are predictions for the expected value across\n# all sites.\npredict_posterior_expected &lt;- function(data, plot = TRUE, re_formula = NA) {\n  newdata &lt;- tidyr::expand(data$data[[1]], nesting(treatment))\n  data &lt;- mutate(data,\n    pred = list(\n      tidybayes::epred_draws(mod, newdata, re_formula = re_formula, \n        value = \"pred\")\n    ),\n    lims = list(\n      tibble(xmin = 0, xmax = quantile(pred$pred, .995))\n    )\n  )\n  if (plot) {\n    p &lt;- data |&gt; unnest(pred) |&gt;\n      ggplot(aes(pred, treatment)) +\n      tidybayes::stat_halfeye(normalize = \"panels\") +\n      facet_wrap(~class, scales = \"free_x\", labeller = fuel_class_labeller) +\n      coord_cartesian_panels(panel_limits = unnest(select(data, lims), lims)) +\n      scale_y_discrete(labels = toupper) +\n      labs(x = expression(Load~(Mg%.%ha^-1)), y = \"Treatment\")\n    print(p)\n  }\n  invisible(data)\n}\n\n\n\nAdding missing grouping variables: `class`\n\n\n\n\n\nFigure 5.11: Posterior expected predictions, with no random effects. This reprsents the expected average conditions across all sites. The point estimate is the mode. Units are mg ha-1. Upper and lower limits are the 95% credible intervals.\n\n\n\n\nThese are the expected predictions, or predictions for the mean. It only includes the uncertainty in the mean and not the variance in predictions estimated by the model.\nThere is quite a bit of uncertainty about the mean all around, but there is a notable difference in that uncertainty among treatments for the onehr and veg fuel classes.\nTable Table 5.7 shows these data in a tabular format. I’m using the highest density continuous interval because, while its hard to see in Figure 5.11, the highest desity region is actually slightly discontinuous.\n\nexpected_predictions |&gt;\n  mutate(\n    summary = list(tidybayes::mode_hdci(pred))\n  ) |&gt; select(summary) |&gt; unnest(summary) |&gt;\n  select(treatment, prediction = pred, lower = .lower, upper = .upper) |&gt;\n  knitr::kable(digits = 1)\n\nAdding missing grouping variables: `class`\nAdding missing grouping variables: `class`\n\n\n\n\nTable 5.7: Tabular data associated with Figure 5.11. Upper and lower pertain to the 95% highest density continuous interval.\n\n\nclass\ntreatment\nprediction\nlower\nupper\n\n\n\n\nonehr\ngs\n0.5\n0.2\n0.9\n\n\nonehr\nha\n1.1\n0.5\n1.9\n\n\nonehr\nhd\n0.9\n0.4\n1.6\n\n\nonehr\nld\n0.6\n0.3\n1.1\n\n\ntenhr\ngs\n3.5\n1.8\n5.9\n\n\ntenhr\nha\n2.9\n1.4\n4.7\n\n\ntenhr\nhd\n2.6\n1.3\n4.4\n\n\ntenhr\nld\n3.0\n1.5\n4.9\n\n\nhundhr\ngs\n10.4\n5.5\n17.2\n\n\nhundhr\nha\n7.9\n4.6\n13.7\n\n\nhundhr\nhd\n8.3\n4.1\n13.1\n\n\nhundhr\nld\n9.5\n5.0\n15.4\n\n\ndufflitter\ngs\n46.4\n28.4\n67.0\n\n\ndufflitter\nha\n38.7\n24.0\n56.1\n\n\ndufflitter\nhd\n52.2\n32.0\n76.4\n\n\ndufflitter\nld\n42.2\n28.3\n64.3\n\n\nthoushr\ngs\n33.8\n12.6\n80.4\n\n\nthoushr\nha\n22.8\n8.7\n55.2\n\n\nthoushr\nhd\n30.6\n12.1\n75.8\n\n\nthoushr\nld\n23.0\n8.1\n52.2\n\n\nveg\ngs\n23.8\n9.8\n49.3\n\n\nveg\nha\n9.1\n3.7\n19.7\n\n\nveg\nhd\n14.2\n6.0\n29.7\n\n\nveg\nld\n17.0\n7.4\n35.9\n\n\n\n\n\n\n\n\nCode\npredict_expected_contrasts &lt;- function(data, rope_size, plot = TRUE, \n                                       re_formula = NA) {\n  # Assume treatment levels are the same for all models: they are.\n  newdata &lt;- expand(data$data[[1]], nesting(treatment))\n  d &lt;- data |&gt;\n    mutate(\n      pred = list(\n        tidybayes::epred_draws(mod, newdata, re_formula = NA, value = \"pred\") |&gt;\n        tidybayes::compare_levels(pred, by = treatment) |&gt;\n        select(contrast = treatment, pred) \n      ),\n      rope = rope_size * sd(data$load),\n      lims = list(\n        tibble(xmin = quantile(pred$pred, .001), xmax = quantile(pred$pred, .999))\n      )\n    )\n  if (plot) {\n    p &lt;- d |&gt;\n      unnest(c(pred)) |&gt;\n      ggplot(aes(x = pred, y = contrast)) +\n      tidybayes::stat_halfeye(normalize = \"panels\") +\n      geom_vline(aes(xintercept = rope))  +\n      geom_vline(aes(xintercept = -rope))  +\n      facet_wrap(~class, scales = \"free_x\", labeller = fuel_class_labeller) +\n      coord_cartesian_panels(\n        panel_limits = unnest(select(d, lims), lims)\n      ) +\n      scale_y_discrete(labels = toupper) +\n      labs(x = expression(Load~(Mg%.%ha^-1)), y = \"Treatment\")\n    print(p)\n  }\n  invisible(d)\n}\n\n\n\n\nAdding missing grouping variables: `class`\n\n\n\n\n\nFigure 5.12: Differences between expected values for each treatment, with 95% continuous interval shown.\n\n\n\n\n\n\nAdding missing grouping variables: `class`\nAdding missing grouping variables: `class`\n\n\n\n\nTable 5.8: Posterior expected predictions of pairwise differences in means, with no random effects. This reprsents the expected average conditions across all sites. Units are Mg ha-1. The point estimate is the mode. Upper and lower limits are the 95% credible intervals. Prob is the probability that the predicted difference matches the sign of its median–the probability that it is not zero.\n\n\nclass\ncontrast\nprediction\nlower\nupper\nprob\n\n\n\n\nonehr\nha - gs\n0.53\n0.05\n1.30\n0.99\n\n\nonehr\nhd - gs\n0.34\n-0.09\n1.02\n0.96\n\n\nonehr\nhd - ha\n-0.12\n-1.00\n0.45\n0.72\n\n\nonehr\nld - gs\n0.11\n-0.31\n0.49\n0.73\n\n\nonehr\nld - ha\n-0.39\n-1.16\n0.07\n0.97\n\n\nonehr\nld - hd\n-0.27\n-0.90\n0.19\n0.91\n\n\ntenhr\nha - gs\n-0.57\n-3.07\n1.55\n0.74\n\n\ntenhr\nhd - gs\n-0.86\n-3.30\n1.23\n0.81\n\n\ntenhr\nhd - ha\n-0.37\n-2.27\n1.74\n0.59\n\n\ntenhr\nld - gs\n-0.47\n-2.95\n1.86\n0.70\n\n\ntenhr\nld - ha\n0.18\n-2.00\n2.27\n0.55\n\n\ntenhr\nld - hd\n0.23\n-1.63\n2.32\n0.65\n\n\nhundhr\nha - gs\n-1.80\n-7.61\n3.24\n0.82\n\n\nhundhr\nhd - gs\n-1.83\n-7.75\n2.59\n0.83\n\n\nhundhr\nhd - ha\n-0.01\n-5.09\n4.37\n0.50\n\n\nhundhr\nld - gs\n-0.91\n-7.04\n4.55\n0.68\n\n\nhundhr\nld - ha\n0.89\n-3.55\n6.26\n0.69\n\n\nhundhr\nld - hd\n1.05\n-3.85\n6.35\n0.70\n\n\ndufflitter\nha - gs\n-6.71\n-21.39\n7.10\n0.86\n\n\ndufflitter\nhd - gs\n6.29\n-10.83\n22.19\n0.80\n\n\ndufflitter\nhd - ha\n12.54\n-1.55\n29.50\n0.97\n\n\ndufflitter\nld - gs\n-2.12\n-17.44\n12.04\n0.63\n\n\ndufflitter\nld - ha\n5.30\n-8.69\n18.54\n0.78\n\n\ndufflitter\nld - hd\n-6.91\n-23.86\n7.31\n0.88\n\n\nthoushr\nha - gs\n-11.26\n-54.82\n21.81\n0.79\n\n\nthoushr\nhd - gs\n-1.92\n-46.60\n40.29\n0.57\n\n\nthoushr\nhd - ha\n5.37\n-26.35\n48.01\n0.76\n\n\nthoushr\nld - gs\n-11.58\n-54.68\n19.50\n0.82\n\n\nthoushr\nld - ha\n-0.55\n-32.13\n27.97\n0.54\n\n\nthoushr\nld - hd\n-9.08\n-53.36\n18.89\n0.78\n\n\nveg\nha - gs\n-13.93\n-36.87\n-0.75\n0.99\n\n\nveg\nhd - gs\n-9.94\n-30.76\n5.90\n0.93\n\n\nveg\nhd - ha\n3.77\n-5.20\n18.13\n0.86\n\n\nveg\nld - gs\n-5.21\n-27.92\n11.34\n0.82\n\n\nveg\nld - ha\n7.07\n-3.94\n23.16\n0.94\n\n\nveg\nld - hd\n3.25\n-11.88\n17.88\n0.73\n\n\n\n\n\n\n\n\n5.2.2.9 R squared\n\nbf4a |&gt;\nmutate(\n  R_squared = list(as_tibble(bayes_R2(mod)))\n) |&gt;\nselect(R_squared) |&gt;\nunnest(R_squared) |&gt;\nknitr::kable(digits = 2)\n\nAdding missing grouping variables: `class`\n\n\n\n\nTable 5.9: This version of R2 is recommended by Gelman et al. (2019). It is defined as the variance in predictions divided by the variance in predictions plus the expected variance of the errors.\n\n\nclass\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nonehr\n0.36\n0.08\n0.21\n0.51\n\n\ntenhr\n0.24\n0.08\n0.10\n0.39\n\n\nhundhr\n0.18\n0.07\n0.07\n0.34\n\n\ndufflitter\n0.19\n0.07\n0.07\n0.34\n\n\nthoushr\n0.17\n0.08\n0.06\n0.37\n\n\nveg\n0.44\n0.10\n0.23\n0.62"
  },
  {
    "objectID": "hypothesis_test.html#final",
    "href": "hypothesis_test.html#final",
    "title": "5  Hypothesis testing",
    "section": "5.3 Final",
    "text": "5.3 Final\n\n5.3.1 Model formula and prior explanation\nThe final model is described in Equation 5.1. The description of the meaning of the priors in the following list should probably be included for explanation.\n\n\n5.3.2 fiting and diagnostics\n\nThe actual code used to fit the model in brms should be included in an appendix.\nFigure 5.9 should be included as an example of the connection between our priors and posterior distributions.\nWe should summarize rhat and tail ess for all models as shown in Table 5.6\n\nR-hat values were all below 1.005\nBulk and tail effective sample sizes were generally above 1000, with the exception of plot and corner SD’s for 1000-hr and Live vegetation fuels, which were above 640.\nThese should be adequate effective sample sizes for robust inference.\n\nFigure 5.10 should be included to show model fit\nTable 5.9 gives an interpretation of R-squared for bayesian models for each of our models.\n\n\n\n5.3.3 Results\n\n5.3.3.1 Predicted means\n\nThe posterior effect of treatment is summarized in Figure 5.11.\nFor 1-hr fuels, treatments HD and HA show patterns of higher loading. This may have to do with more fine fuel inputs from higher retention. This could lead to increased fire behavior, which could be of benefit if attempting to burn under marginal conditions.\nThe HD treatment shows the potential for higher loading of duff and litter. This could be due to the same effect as for 1-hr fuels, but the HA treatement does not show a similar trend in this case. This could be explained by a higher probability of sampling near a trees base, where duff and litter load are known to be higher.\nAn obvious trend in live vegetation is revealed in the GS treatment, where tree sprouts and brush was able to regenerate without any competition. The potentially reduced live vegetation in the HA treatment is unexpected.\n\n\n\n5.3.3.2 Predicted contrasts\n\nPairwise comparison of treatment efffect is sumarized in Figure 5.12\nContrasts take into account how expected predictions co-vary and is a better indicato of potential differences between treatments.\nIf we take the 95% credible interval as the measure of “significant differences” between treatments, then the HA treatment would be considered to result in greater fuel load than the GS treatment, and the opposite trend would be true for live vegetation: HA is greater than GS.\nThe posterior distributions of contrasts reveal more potential differences with somewhat reduced certainty for 1-hr, Duff & litter, and Live vegetation.\n\nIt is also likely that the LD treatment has lower 1-hr fuel loading than HA.\nSimilar to the HA treatment, the HD treatment also appears to have greater loading than GS.\nFor Duff & litter, the HD treatment may have greater load than HA\nFor live vegetation, LD may have greater load than HA.\n\nFor 10, 100, and 1000-hr fuels, there is no real detectable differences between treatments."
  },
  {
    "objectID": "hypothesis_test.html#todo",
    "href": "hypothesis_test.html#todo",
    "title": "5  Hypothesis testing",
    "section": "5.4 TODO:",
    "text": "5.4 TODO:\n\nInter-class correlation: ICC\nmultivariate response to accont for correlations between fuel classes\nDirichlet, or similiar joint priors on random group SD’s to control exponential increase due to multiple independent priors\nInclude model for measurement error\nmodel stations along transects\n\nthis requires some fuel classes being modeled at an added heirarchical level (station) while the others only get modeled to the transect level\n\nInclude model for deriving biomass from counts, heights, and percent cover estiamtes\n\nvariability + uncertainty about particle density\nvariability + uncertainty about diameter distributions\nvariability + uncertainty about relationship between vegetation sampling cylinder observations and biomass.\n\nSample size calculations"
  },
  {
    "objectID": "work_log.html",
    "href": "work_log.html",
    "title": "6  Work log",
    "section": "",
    "text": "The following is a log of the work completed for this project\n\n2023-07-07\n\nEmmeans for multilevel model\nbootstrap test of treatment significance\ncompare to nlme\ncompare to model addressing heterscedasticity\n\n\n\n2023-07-03\n\nmulti-level model\n\n\n\n2023-06-30\n\nexplore more distributions\n\n\n\n2023-06-29\n\nexplore other distributions, zeros\n\n\n\n2023-06-28\n\nadded histograms + normal curves\n\n\n\n2023-06-27\n\nforgot these files\ntest normality\n\n\n\n2023-06-20\n\nadded captions\nlook at outliers in fuel data\n\n\n\n2023-06-16\n\ninterpret fuel load summary\n\n\n\n2023-06-15\n\nclean up code blocks\nMerge branch ‘main’ of https://github.com/fisher-j/multi-age\nImprove presentation of fuel loading from literature\n\n\n\n2023-06-14\n\nminor website change\nsummarize total fuel loading\n\n\n\n2023-06-13\n\ncomplete vegetation loading\nwebsite\ncomplete dufflitter fuel loading\n\n\n\n2023-06-12\n\nwork on duff/litter, values from lit.\n\n\n\n2023-06-20\n\nBegin fuels data exploration\nLook for outliers\n\n\n\n2023-06-15\n\nImprove presentation of fuel loading values from literature\n\n\n\n2023-14-08\n\nCalculated vegetation fuel loading\ncalculate duff and litter fuel load\n\nsummarize some depth to load values from literature\n\ncalculate vegetation fuel load\n\nresearch vegetation bulk density in literature\n\nwe should have measured shrub basal diameter\n\n\nSummarize total fuel load\n\nsummarize values from literature\n\n\n\n\n2023-06-08\n\nCalculate fine and coarse fuel load\ncreate test for duplicated transect or plot ids\nfix wrong azimuths in datasheet\nstart with duff and litter load\n\n\n\n2023-06-05\n\nStart calculate fuel loading\nsome background research on the theory behind planar intercept\ncompile fuel particle parameters from multiple sources\n\n\n\n2023-06-01\n\nFinish data entry\n\n\n\n2023-05-31\n\nFigure showing fuel (and vegetation) sampling layout\nFunction to combine data from all datasheets\nRefine data description text\ncontinued data entry\n\n\n\n2023-5-26\n\nDeveloped data input format\n\none file for each plot\n\nCreated R function to ingest a datasheet in the specified format\nDocumented the input format and processing function\nFuel data entered for 7 replicates\n\n\n\n2023-05-23\n\nwork on development environment for coding\n\n\n\n2023-05-22\n\nInitialized project as a quarto book\n\nlearning to use Quarto building analysis website\nset up GitHub repository\nstart log to track progress"
  },
  {
    "objectID": "sprouts_import_data.html",
    "href": "sprouts_import_data.html",
    "title": "7  Data import and description",
    "section": "",
    "text": "First, we’ll load some libraries and our data.\n\nsuppressWarnings(suppressPackageStartupMessages(library(tidyverse)))\nlibrary(ggridges)\nlibrary(ggokabeito)\n\nsprouts &lt;- readxl::read_excel(\"../data/Sprouts10yr.xlsx\")\n\nset.seed(743)\nslice_sample(sprouts, n = 4) |&gt; knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSite\nPlot\nTrtmt\nTree\nSpecies\nHT1yr_m\nHT5yr_m\nHT10yr_m\nHTI1-5yr\nHTI5-10yr\nDBH10yr_cm\nLCBH10yr\nCR10yr\nHD10yr\nSDIinit\nAggregatedYN\nResidRWonClumpYN\nHT10rank\n\n\n\n\nWaldo North\n2\nGS\n285\nLIDE\n1.72\n3.55\n6.30\n0.4575\n0.550\nNA\nNA\nNA\nNA\n0.00\n0\nNA\n4\n\n\nWaldo South\n4\nGS\n13\nSESE\n1.14\n3.11\n6.00\n0.4925\n0.578\n6.6\n0.60\n0.900000\n90.90909\n0.00\n0\n0\n21\n\n\nWhiskey Springs\n4\nHA\n1904\nLIDE\n0.29\n0.96\n1.94\n0.1675\n0.196\nNA\nNA\nNA\nNA\n536.33\n1\nNA\n24\n\n\nWhiskey Springs\n2\nHD\n1405\nSESE\n1.83\n5.21\n9.20\n0.8450\n0.798\n12.5\n0.01\n0.998913\n73.60000\n509.90\n0\n0\n6\n\n\n\n\n\nThe data is in a wide format, variables are as follows:\n\nvariable_descriptions &lt;- list(\n  Site = \"One of four sites where treatments were replicated. Sites were located\n  on similar slope positions, but across a range of aspects.\",\n  Plot = \"There were 4 plots at each site and each was randomly assigned a\n  treatment\",\n  Trtmt = \"Treatemnt type: GS = group selection, which is basically a small\n  clearing, LD = low density--fewer trees remaining; HD = high density--more\n  trees remain and they are dispersed; HA = high density aggregated--more trees\n  remain and they are grouped into clumps.\",\n  Tree = \"Unique sprout ID.\",\n  Species = \"SESE = coast redwood, and LIDE = tanoak.\",\n  HT1yr_m = \"Sprout height one year after treatment.\",\n  HT5yr_m = \"Same as above, but for year 5\",\n  HT10yr_m = \"Same as above, but for year 10\",\n  `HT1-5yr` = \"Height growth between years 1 and 5\",\n  `HT5-10yr` = \"Height growth between years 5 and 10\",\n  DBH10yr_cm = \"Diameter at breast height in cm at year 10. Only collected for\n  redwood\",\n  LCBH10yr = \"Live crown base height (height to first live branch) at year 10. \n  Only collected for redwood.\",\n  CR10yr = \"Crown ratio (live crown length / total height) at year 10, only for\n  redwood\",\n  HD10yr = \"Unknown.\",\n  SDIinit = \"Stand density index of plot immediately after treatment.\",\n  AggregatedYN = \"Indicator for treatment HA.\",\n  ResidRWonClumpYN = \"Unknown.\",\n  HT10rank = \"Trees species specific height ranking within plot.\"\n)\n\nvariable_descriptions |&gt; \n  imap(\\(x, y) tibble(Variable = y, Description = gsub(\"[[:space:]]{2,}\", \" \", x))) |&gt;\n  list_rbind(names_to = \"Variable\") |&gt;\n  knitr::kable()\n\n\n\nTable 7.1: Descriptions of variables in dataset.\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nSite\nOne of four sites where treatments were replicated. Sites were located on similar slope positions, but across a range of aspects.\n\n\nPlot\nThere were 4 plots at each site and each was randomly assigned a treatment\n\n\nTrtmt\nTreatemnt type: GS = group selection, which is basically a small clearing, LD = low density–fewer trees remaining; HD = high density–more trees remain and they are dispersed; HA = high density aggregated–more trees remain and they are grouped into clumps.\n\n\nTree\nUnique sprout ID.\n\n\nSpecies\nSESE = coast redwood, and LIDE = tanoak.\n\n\nHT1yr_m\nSprout height one year after treatment.\n\n\nHT5yr_m\nSame as above, but for year 5\n\n\nHT10yr_m\nSame as above, but for year 10\n\n\nHT1-5yr\nHeight growth between years 1 and 5\n\n\nHT5-10yr\nHeight growth between years 5 and 10\n\n\nDBH10yr_cm\nDiameter at breast height in cm at year 10. Only collected for redwood\n\n\nLCBH10yr\nLive crown base height (height to first live branch) at year 10. Only collected for redwood.\n\n\nCR10yr\nCrown ratio (live crown length / total height) at year 10, only for redwood\n\n\nHD10yr\nUnknown.\n\n\nSDIinit\nStand density index of plot immediately after treatment.\n\n\nAggregatedYN\nIndicator for treatment HA.\n\n\nResidRWonClumpYN\nUnknown.\n\n\nHT10rank\nTrees species specific height ranking within plot.\n\n\n\n\n\n\n\nsprouts |&gt;\n  pivot_longer(\n    matches(\"HT..?yr_m\"),\n    names_to = \"year\", values_to = \"height\",\n    names_transform = \\(x) as.integer(str_extract(x, \"\\\\d+\"))\n  ) |&gt;\n  ggplot(aes(height, fct_rev(factor(year)), color = Trtmt)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  scale_x_continuous(expand = c(0, 0)) +\n  coord_cartesian(clip = \"off\") +\n  geom_density_ridges(alpha = .3, size = 1) +\n  facet_wrap(~Species) +\n  labs(x = \"Height (m)\", y = \"Year\") +\n  scale_color_okabe_ito()\n\n\n\n\nFigure 7.1: Height distributions for tanoak and redwood over time for each of four treatments. Treatments GS and LD have higher proportions of taller trees. Years refers to number of years after treatment.\n\n\n\n\n\nsprouts |&gt;\n  pivot_longer(\n    matches(\"HT..?yr_m\"),\n    names_to = \"year\", values_to = \"height\",\n    names_transform = \\(x) as.integer(str_extract(x, \"\\\\d+\"))\n  ) |&gt;\n  ggplot(aes(\n    x = height,\n    y = factor(Trtmt,levels = c(\"GS\", \"LD\", \"HA\", \"HD\")),\n    color = factor(Species, levels = c(\"SESE\", \"LIDE\"))\n  )) +\n  scale_y_discrete(expand = c(0, 0)) +\n  scale_x_continuous(expand = c(0, 0)) +\n  coord_cartesian(clip = \"off\") +\n  geom_density_ridges(alpha = .3, size = 1) +\n  facet_grid(year ~ ., labeller = label_both) +\n  labs(x = \"Height (m)\", y = \"Treatment\") +\n  scale_color_okabe_ito(name = \"Species\")\n\n\n\n\nFigure 7.2: Similar to Figure 7.1, but with an emphasis on differences between speceis responses across treatments. Comparisons are made for each year. The GS treatment appears to favor redwood response the most, but all treatments show redwoods are taller than tanoak. Years refers to number of years after treatment."
  }
]