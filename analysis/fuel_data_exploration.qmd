---
title: Fuel data exploration
---

```{r init}
#| echo: false
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
load("./data_long.RData")
transectid <- c("site", "treatment", "corner", "azi")
```

Our main questions regarding fuel loading are:

1. Are there differences in fuel loading between treatments?
2. If so, what fuel loading components differ, and in which treatments?
3. What is the magnitude of the difference (for each fuel class) by treatment?

We have a range of response variables that include loading in several different
classes of surface fuels. Some of these may be correlated with each other, which
may have an effect on our interpretation of differences between treatments. For
instance, if duff-litter load is negatively correlated with vegetation, even if
we don't see differences between treatment in either of these variables
separately, it might be found that for any given level of vegetation loading, one
treatment, or another may have consistently higher levels of duff-litter.
Capturing differences between treatments in terms of interactions like these
requires different tools than assessing any one variable on its own.

In order to assess potential outliers, co-linearity, interactions, and other
problems with statistical tests, we'll first conduct some data exploration as
outlined by @zuurProtocolDataExploration2010.

For reference, here is a list of the fuel loading (response) variables of
interest.

1. duff/litter load
2. woody vegetation
3. herbaceous vegetation
4. one hr fuels
5. ten hr fuels
6. hundred hr fuels
7. fine woody debris (one, ten, and hundred hr fuels)
8. coarse woody debris

It's important to note that variables 1-3 were measured twice per transect and
the rest were measured once per transect.

## Outliers

```{r outlier-cleveland-func}

cleavland_plot <- function(data, title, load_var = load) {
  data |>
    group_by(site, treatment) |>
    mutate(replicate_mean_load = mean({{load_var}}, na.rm = TRUE)) |>
    ggplot(
      aes(
        {{load_var}},
        fct_reorder(
          interaction(site, treatment, sep = " "),
          {{load_var}},
          .na_rm = TRUE
        ),
        color = treatment
      )
    ) +
    geom_jitter(width = 0, height = 0.2) +
    labs(x = expression(Load~(Mg%.%ha^-1)), y = "Data order", title = title)
}

```

```{r outlier-plot-fwd-sep}
#| echo: false
#| label: fig-fwd
#| fig-cap: >
#|   Data distribution of loading for fine woody debris classes. Data are
#|   sorted by mean loading within each replicate. Jitter has been added to aid
#|   in visual interpretation.
#| fig-subcap:
#|   - One hour fuels
#|   - Ten hour fuels
#|   - Hundred hour fuels

fwd |> 
  mutate(class = factor(class, levels = c("onehr", "tenhr", "hundhr"))) |>
  group_by(class) |> 
  group_walk(\(data, groups) print(cleavland_plot(data, groups$class)))

```

```{r outlier-plot-fwd-all}
#| echo: false
#| label: fig-fwd-all
#| fig-cap: >
#|   Combined fuel loading in one, ten, and hundred hour classes. Data are
#|   sorted as described in @fig-fwd.

fwd |> summarize(.by = all_of(transectid), load = sum(load)) |>
cleavland_plot("One, ten, and hund. hr")

```

```{r outlier-plot-cwd}
#| echo: false
#| label: fig-cwd
#| fig-cap: >
#|   Sum of coarse woody (>7.64 cm, sound and rotten wood combined) fuel loading
#|   for transects. The y-axis is sorted by mean CWD loading for each replicate.

cwd |>
  summarize( load = sum(load, na.rm = TRUE), .by = all_of(transectid)) |> 
  cleavland_plot("CWD")

```

```{r outlier-plot-dufflitter}
#| echo: false
#| label: fig-dufflitter
#| fig-cap: >
#|   Combined duff and litter loading at each station along trancects. Y-axis is
#|   sorted as in @fig-cwd.

dufflitter |>
  cleavland_plot("Duff/litter")

```

```{r outlier-plot-veg}
#| echo: false
#| label: fig-veg
#| fig-cap: !expr caption
#| fig-subcap:
#|   - Woody vegetation
#|   - herbaceous vegetation

caption <- "Vegetation fuel loading for each station along transects, including
  live and dead fuels attached to live vegetation. Y-axis is sorted as in
  @fig-cwd."

veg |>
  mutate(class = factor(class, levels = c("woody", "herb"))) |>
  group_by(class) |>
  group_walk(
    \(data, groups) print(cleavland_plot(data, paste(groups$class, "veg")))
  )

```

## Normality {#sec-Normality}

For further testing, I will summarize the data somewhat, by combining vegetation
loading (woody and herb), and coarse woody loading (sound and rotten) into just
two loading metrics. Now we have the following response variables:

1. dufflitter
2. onehr
3. tenhr
4. hundhr
5. thoushr
6. veg

```{r reduce-vars}
#| code-fold: true

load2 <- function(shape = "wide", ...) {
  load_vars <- c("onehr", "tenhr", "hundhr", "dufflitter", "thoushr", "veg")
  tl <- pivot_wider(total_load, names_from = class, values_from = load) |>
    mutate(
      thoushr = rowSums(pick(c(thoushr_s, thoushr_r)), na.rm = TRUE),
      veg = rowSums(pick(c(woody, herb)), na.rm = TRUE),
      .keep = "unused"
    )
  if (!missing(...)) tl <- select(tl, ...)  
  if (shape == "long") {
    tl <- pivot_longer(tl, 
      -any_of(c("site", "treatment", "corner", "azi")),
      names_to = "class", 
      values_to = "load"
    )
    load_vars <- load_vars[load_vars %in% tl$class]
    tl <- mutate(tl, class = factor(class, levels = load_vars))
  }
  tl
} 

```

When using manova to test for difference between groups with multiple response
variables, it is important that the response variables are multivariate normally
distributed. Unfortunately, it would appear that we have a probelem with
normality. The raw data for each loading variable is clearly not normally
distributed @fig-qqplot-naive.

```{r qq-plot}
#| label: fig-qqplot-naive
#| fig-heigth: 8
#| fig-cap: >
#|   Naive qq plot of loading variables. This doesn't take into account
#|   the fact that we are interested in within group normality.

load_vars <- c("onehr", "tenhr", "hundhr", "dufflitter", "thoushr", "veg")

myqqplot <- function(data, var) {
  data |>
  ggplot(aes(sample = {{ var }})) +
  stat_qq() +
  stat_qq_line() +
  facet_grid(class ~ treatment, scales = "free") +
  labs(
    x = "Theoretical quantiles", y = "Sample quantiles",
    title = "Normal Q-Q Plot"
  )
}

load2("long", treatment, all_of(load_vars)) |> 
myqqplot(load)

```

```{r hist-data}

bins <- 16
hist_dat <- load2("long", treatment, all_of(load_vars)) |>
  drop_na() |>
  # Facet grid each column (class) has same scale, find limits to calculate bin
  # width, limits are either implied by the constructed normal curce, or the raw
  # data
  group_by(class) |>
  mutate(
    xmin = min(c(mean(load) - 3 * sd(load), load)),
    xmax = max(c(mean(load) + 3 * sd(load), load))
  ) |>
  group_by(treatment, class) |>
  nest(data = load) |>
  # generate data for a normal curve with mean and sd from observed data to
  # cover 3 sd.
  mutate(
    norm_x = map(data, function (d) {
      seq(
        from = mean(d$load) - 3 * sd(d$load), 
        to = mean(d$load) + 3 * sd(d$load), 
        length.out = 100
      )
    }),
    # scale curve to expected binwidth based on plot layout (same x scale across
    # all fuel classes) multiplied by the number of observations. The histogram
    # and normal curve should represent the same total area.
    norm_y = map(data, function (d) {
      dens <- dnorm(unlist(norm_x), mean = mean(d$load), sd = sd(d$load))
      dens * ((xmax - xmin) / bins) * nrow(d)
    })
  )

```

```{r}
#| code-fold: true
#| label: fig-hist
#| fig-cap: Histotrams for the fuel loading response variables. A normal curve
#|   with the same mean as sd as the data (and scaled to the same area as that
#|   covered by the bins) has been superimposed for reference.
#| fig-subcap:
#|   - ""
#|   - ""


# Break plot into two panels for higher resolution.
ggplot(filter(hist_dat, class %in% load_vars[1:3] )) +
geom_histogram(data = \(x) unnest(x, data), aes(x = load), bins = bins) +
geom_line(data = \(x) unnest(x, c(norm_x, norm_y)), aes(norm_x, norm_y)) +
facet_grid(treatment ~ class, scales = "free") +
labs(y = "count", x = "Load Mg/ha")

ggplot(filter(hist_dat, class %in% load_vars[4:6] )) +
geom_histogram(data = \(x) unnest(x, data), aes(x = load), bins = bins) +
geom_line(data = \(x) unnest(x, c(norm_x, norm_y)), aes(norm_x, norm_y)) +
facet_grid(treatment ~ class, scales = "free") +
labs(y = "count", x = "Load Mg/ha")

```

### Box-Cox transformation {#sec-box-cox-transformation}

I would like to look at the effect of this transformation on the response data.

```{r}

boxcox <- function(y, lambda1, lambda2) {
  if(lambda1 == 0) {
    ln(y + lambda2)
  } else {
    ((y + lambda2)^lambda1 - 1) / lambda1
  }
}

load2("long", treatment, all_of(load_vars)) |>
  group_by(class) |> nest() |> rowwise() |>
  mutate(
    lambda = list(suppressMessages(geoR::boxcoxfit(data$load, lambda2 = TRUE)$lambda)),
    load_bc = list(boxcox(data$load, lambda[1], lambda[2]))
  ) |>
  unnest(c(data, load_bc)) |> ungroup() |>
  myqqplot(load_bc)

```

While, this looks somewhat more normal, the zeros end up being a little strange.
I applied a separte transformation for each fuel class, but all treatments
within a fuel class have the same transformation.

For MANOVA we are concered with the within group multivariate normality, the
assumption does not appear to be met here either (@fig-qqplot-model). The code
output below indicates the rows with the greatest deviation from normal.

```{r qq-resid}
#| label: fig-qqplot-model
#| fig-cap: >
#|   Plot A assessed the multivariate normality of residuals given the model
#|   where all loading variabels are a function of the treatment group.
#| fig-subcap:
#|   - ""
#|   - ""

tl <- load2("wide", treatment, all_of(load_vars))
load_mod <- lm(as.matrix(tl[-1]) ~ treatment, data = tl)
m_dist <- heplots::cqplot(load_mod)

load2("wide", everything())[order(m_dist, decreasing = TRUE)[1:10], ]

treatments = c("gs", "ha", "ld", "hd")
par(mfrow = c(2, 2))
invisible(lapply(treatments, 
  \(x) heplots::cqplot(filter(tl, treatment == x)[-1], main = x)
))

```

A number of different tests of multivariate normaliy also confirm the lack of
evidence for meeting this assumption (@tbl-norm-test).

```{r norm-test}
#| label: tbl-norm-test
#| tbl-cap: >
#|   Several different tests of multivariate normality indicate a lack of
#|   evidence to support this assumption.

all_mvn_tests <- function(data) {
  c("mardia", "hz", "royston", "dh", "energy") |>
  map(\(x) MVN::mvn(data = data, subset = "treatment", mvnTest = x)) |>
  map(\(x) x$multivariateNormality) |>
  map(\(x) bind_rows(x, .id = "treatment")) |>
  map_dfr(\(x) 
    filter(x, Test != "MVN") |> 
    mutate( across(where(is.factor), \(f) as.numeric(as.character(f)))) |> 
    select(1:2, statistic = 3, `p value`, Result = last_col())
  )
}

all_mvn_tests(load2("wide", treatment, all_of(load_vars))) |>
  knitr::kable(digits = 4)

```

### Other distributions {#sec-other-distributions}

If our data is not normally distributed, then what distribution is it? I'm going
to assume what we are interested in the distribution of data within groups
(treatments).

I attemped to model the distribution of our conditional response data (fuel size
class by treatment), but it mostly didn't work.

Our data is non-negative (contains zeros) continuous (for the most part) and highly variable in
terms of skew and kurtosis. The presence of zeros, makes using the Gamma
distribution

```{r}
#| label: fig-skew-kurt
#| fig-cap: Skewness and kurtosis for fuel classes within treatments.
#| fig-subcap:
#|   - Treatment gs, fuel class onehr      
#|   - Treatment ha, fuel class onehr      
#|   - Treatment hd, fuel class onehr      
#|   - Treatment ld, fuel class onehr      
#|   - Treatment gs, fuel class tenhr      
#|   - Treatment ha, fuel class tenhr      
#|   - Treatment hd, fuel class tenhr      
#|   - Treatment ld, fuel class tenhr      
#|   - Treatment gs, fuel class hundhr     
#|   - Treatment ha, fuel class hundhr     
#|   - Treatment hd, fuel class hundhr     
#|   - Treatment ld, fuel class hundhr     
#|   - Treatment gs, fuel class dufflitter 
#|   - Treatment ha, fuel class dufflitter 
#|   - Treatment hd, fuel class dufflitter 
#|   - Treatment ld, fuel class dufflitter 
#|   - Treatment gs, fuel class thoushr    
#|   - Treatment ha, fuel class thoushr    
#|   - Treatment hd, fuel class thoushr    
#|   - Treatment ld, fuel class thoushr    
#|   - Treatment gs, fuel class veg        
#|   - Treatment ha, fuel class veg        
#|   - Treatment hd, fuel class veg        
#|   - Treatment ld, fuel class veg        
#| layout-ncol: 2
#| column: page

d <- load2("long", treatment, all_of(load_vars)) |> 
  split(~class) |> map(~split(.x, ~treatment))

# imap_dfr(d, \(x, y) tibble(class = y, treatment = names(x) )) |>
#   with(paste0("Treatment ", treatment, ", fuel class ", class)) |>
#   cat(sep = "\n")

# par(mfrow = c(2,2))
walk(d, ~ walk(.x, \(x) fitdistrplus::descdist(x$load, boot = 111)))

```

### Zero-adjusted Gamma

```{r}

# Zero adjusted Gamma distribution fit to histograms
plot_zaga <- function(i, d) {
  m1 <- gamlss::gamlssML(d, family = gamlss.dist::ZAGA())
  hist(d, prob = TRUE, main = i, breaks = 20)
  curve(gamlss.dist::dZAGA(x, mu = m1$mu, sigma = m1$sigma, nu = m1$nu), from = min(d), to = max(d), add = TRUE)
}

plot_zaga_class <- function(data, class_name) {
  if(missing(class_name)) {
    class_name <- as.character(substitute(data))
    class_name <- class_name[length(class_name)]
  }
  par(mfrow = c(2,2))
  iwalk(data, ~plot_zaga(.y, .x))
  mtext(class_name, cex = 1.6, side = 3, line = -2, outer = TRUE)
}

d2 <- d |> map(~ map(.x, "load"))
iwalk(d2, ~plot_zaga_class(.x, .y))

```

### Poisson fit of count data {#sec-poisson-fit-of-count-data}

All of the woody debris can be viewed as count data, and mean diameter. The mean
diameter implies a distribution, which we actually have for coarse woody, but
not for FWD.

I wonder If I can model these counts as a Poisson process.

```{r}

cwd_counts <- cwd |> 
  group_by(site, treatment, corner, azi) |>
  summarize(count = sum(count), .groups = "drop")

par(mfrow = c(2,2))
cwd_counts |>
  group_by(treatment) |>
  group_walk( function (data, group) {
    n <- data$count
    hist(n, main = group, prob = TRUE)
    lines(0:max(n), dpois(0:max(n), mean(n)))
  })

```

That didn't look so great, so I also tried using a Zero-inflated Poisson
distribution, but the model fit an extremely small value for the parameter that
controls the probability of zero (referred to here as sigma) and so was
effectively the same as the Poisson fit.

```{r}

par(mfrow = c(2,2))
cwd_counts |> 
  group_by(treatment) |> nest() |>
  transmute(
    sigma = map_dbl(data, 
      ~gamlss::gamlssML(.x$count, family = gamlss.dist::ZIP())$sigma
    )
  )

```

## Homogeneity of variance {#sec-homogeneity-of-variance}

There seem to be some pretty big differences in the variance between treatments.
This is likely to do with outliers. For linear regression, it is recommended
that maximum variance ration should be below 4.

```{r}

max_var <- load2("long", everything()) |>
  group_by(class, treatment) |>
  summarize(var = sd(load)^2, load = max(load), .groups = "drop_last") |>
  summarize(
    max_var_rat = paste("Max. var. ratio: ", round(max(var) / min(var))),
    x = 1.1, y = max(load) * 1.1, .groups = "drop")

load2("long", everything()) |>
  ggplot(aes(treatment, load)) +
  geom_boxplot() +
  geom_text(data = max_var, aes(x, y, label = max_var_rat, hjust = "inward")) +
  facet_wrap(~class, scales = "free")

```

## Zeros

We do have zeros, which is important if we want to employ a glm like Gamma,
which is only defined for positive values.

```{r}

load2("long", everything()) |>
  group_by(class, treatment) |>
  summarize(zeros = sum(load == 0), percent = zeros / n(), .groups = "drop") |>
  ggplot(aes(treatment, percent)) +
  geom_col() +
  geom_text(
    aes(label = if_else(zeros == 0, NA, zeros), y = percent / 2),
    color = "gray70", na.rm = TRUE
  ) +
  facet_wrap(~class) +
  scale_y_continuous(labels = scales::percent)

```

## Correlation of response variables

I'm not sure if it's important, but I was curious if the various fuel loading
classes were correlated with each other. Either across the board, or within a
given treatment.

```{r}
#| label: fig-response-corr
#| fig-cap: Correlation among the response variables (fuel classes).

suppressMessages(GGally::ggpairs(load2("wide", all_of(load_vars))))

```

## Independence

Because of how are data were collected they are not independent. The current data
is summarized at the transect level. At that level. We have two transects at
each corner. Because of spatial autocorrelation, these may be correlated with
each other. Corners (and thus transects) are nested within plots, and plots are
within treatments. Each plot received a different treatment. What I'm not clear
about is: should I include a random variable for plots, if I'm including a
fixed effect for treatment?

There are also question about at what level to summarize/model the data. I've
already averaged stations within transects for several variable that were
collected at the station level (two within each transect). What are the
trade-offs for either averaging at the corner level, or alternatively,
analyzing our raw station data instead of averaging.





