---
title: Fuel data exploration
---

```{r init}
#| echo: false
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
load("./data_long.RData")
transectid <- c("site", "treatment", "corner", "azi")
```

Our main questions regarding fuel loading are:

1. Are there differences in fuel loading between treatments?
2. If so, what fuel loading components differ, and in which treatments?
3. What is the magnitude of the difference by treatment?

We have a range of response variables that include loading in several different
classes of surface fuels. Some of these may be correlated with each other, which
may have an effect on our interpretation of differences between treatments. For
instance, if duff-litter load is negatively correlated with vegetation, even if
we don't see differences between treatment in either of these variables
separately, it might be found that for any given level of vegetation loading, one
treatment, or another may have consistently higher levels of duff-litter.
Capturing differences between treatments in terms of interactions like these
requires different tools than assessing any one variable on its own.

In order to assess potential outliers, co-linearity, interactions, and other
problems with statistical tests, we'll first conduct some data exploration as
outlined by @zuurProtocolDataExploration2010.

For reference, here is a list of the fuel loading (response) variables of
interest.

1. duff/litter load
2. woody vegetation
3. herbaceous vegetation
4. one hr fuels
5. ten hr fuels
6. hundred hr fuels
7. fine woody debris (one, ten, and hundred hr fuels)
8. coarse woody debris

It's important to note that variables 1-3 were measured twice per transect and
the rest were measured once per transect.

## Outliers

```{r outlier-cleveland-func}

cleavland_plot <- function(data, title, load_var = load) {
  data |>
    group_by(site, treatment) |>
    mutate(replicate_mean_load = mean({{load_var}}, na.rm = TRUE)) |>
    ggplot(
      aes(
        {{load_var}},
        fct_reorder(
          interaction(site, treatment, sep = " "),
          {{load_var}},
          .na_rm = TRUE
        ),
        color = treatment
      )
    ) +
    geom_jitter(width = 0, height = 0.2) +
    labs(x = expression(Load~(Mg%.%ha^-1)), y = "Data order", title = title)
}

```

```{r outlier-plot-fwd-sep}
#| echo: false
#| label: fig-fwd
#| fig-cap: >
#|   Data distribution of loading for fine woody debris classes. Data are
#|   sorted by mean loading within each replicate. Jitter has been added to aid
#|   in visual interpretation.
#| fig-subcap:
#|   - One hour fuels
#|   - Ten hour fuels
#|   - Hundred hour fuels

fwd |> 
  mutate(class = factor(class, levels = c("onehr", "tenhr", "hundhr"))) |>
  group_by(class) |> 
  group_walk(\(data, groups) print(cleavland_plot(data, groups$class)))

```

```{r outlier-plot-fwd-all}
#| echo: false
#| label: fig-fwd-all
#| fig-cap: >
#|   Combined fuel loading in one, ten, and hundred hour classes. Data are
#|   sorted as described in @fig-fwd.

fwd |> summarize(.by = all_of(transectid), load = sum(load)) |>
cleavland_plot("One, ten, and hund. hr")

```

```{r outlier-plot-cwd}
#| echo: false
#| label: fig-cwd
#| fig-cap: >
#|   Sum of coarse woody (>7.64 cm, sound and rotten wood combined) fuel loading
#|   for transects. The y-axis is sorted by mean CWD loading for each replicate.

cwd |>
  summarize( load = sum(load, na.rm = TRUE), .by = all_of(transectid)) |> 
  cleavland_plot("CWD")

```

```{r outlier-plot-dufflitter}
#| echo: false
#| label: fig-dufflitter
#| fig-cap: >
#|   Combined duff and litter loading at each station along trancects. Y-axis is
#|   sorted as in @fig-cwd.

dufflitter |>
  cleavland_plot("Duff/litter")

```

```{r outlier-plot-veg}
#| echo: false
#| label: fig-veg
#| fig-cap: !expr caption
#| fig-subcap:
#|   - Woody vegetation
#|   - herbaceous vegetation

caption <- "Vegetation fuel loading for each station along transects, including
  live and dead fuels attached to live vegetation. Y-axis is sorted as in
  @fig-cwd."

veg |>
  mutate(class = factor(class, levels = c("woody", "herb"))) |>
  group_by(class) |>
  group_walk(
    \(data, groups) print(cleavland_plot(data, paste(groups$class, "veg")))
  )

```

## Normality {#sec-Normality}

For further testing, I will summarize the data somewhat, by combining vegetation
loading (woody and herb), and coarse woody loading (sound and rotten) into just
two loading metrics. Now we have the following response variables:

1. dufflitter
2. onehr
3. tenhr
4. hundhr
5. thoushr
6. veg

```{r reduce-vars}
#| code-fold: true

load2 <- function(shape = "wide", ...) {
  load_vars <- c("onehr", "tenhr", "hundhr", "dufflitter", "thoushr", "veg")
  tl <- pivot_wider(total_load, names_from = class, values_from = load) |>
    mutate(
      thoushr = rowSums(pick(c(thoushr_s, thoushr_r)), na.rm = TRUE),
      veg = rowSums(pick(c(woody, herb)), na.rm = TRUE),
      .keep = "unused"
    )
  if (!missing(...)) tl <- select(tl, ...)  
  if (shape == "long") {
    tl <- pivot_longer(tl, 
      -any_of(c("site", "treatment", "corner", "azi")),
      names_to = "class", 
      values_to = "load"
    )
    load_vars <- load_vars[load_vars %in% tl$class]
    tl <- mutate(tl, class = factor(class, levels = load_vars))
  }
  tl
} 

```

When using manova to test for difference between groups with multiple response
variables, it is important that the response variables are multivariate normally
distributed. Unfortunately, it would appear that we have a probelem with
normality. The raw data for each loading variable is clearly not normally
distributed @fig-qqplot-naive.

```{r qq-plot}
#| label: fig-qqplot-naive
#| fig-cap: >
#|   Naive qq plot of loading variables. This doesn't take into account
#|   the fact that we are interested in within group normality.

load_vars <- c("onehr", "tenhr", "hundhr", "dufflitter", "thoushr", "veg")

load2("long", treatment, all_of(load_vars)) |> 
  ggplot(aes(sample = load)) +
  stat_qq() +
  stat_qq_line() +
  facet_wrap(~class, scales = "free")

```

```{r hist-data}

bins <- 16
hist_dat <- load2("long", treatment, all_of(load_vars)) |>
  drop_na() |>
  # Facet grid each column (class) has same scale, find limits to calculate bin
  # width, limits are either implied by the constructed normal curce, or the raw
  # data
  group_by(class) |>
  mutate(
    xmin = min(c(mean(load) - 3 * sd(load), load)),
    xmax = max(c(mean(load) + 3 * sd(load), load))
  ) |>
  group_by(treatment, class) |>
  nest(data = load) |>
  # generate data for a normal curve with mean and sd from observed data to
  # cover 3 sd.
  mutate(
    norm_x = map(data, function (d) {
      seq(
        from = mean(d$load) - 3 * sd(d$load), 
        to = mean(d$load) + 3 * sd(d$load), 
        length.out = 100
      )
    }),
    # scale curve to expected binwidth based on plot layout (same x scale across
    # all fuel classes) multiplied by the number of observations. The histogram
    # and normal curve should represent the same total area.
    norm_y = map(data, function (d) {
      dens <- dnorm(unlist(norm_x), mean = mean(d$load), sd = sd(d$load))
      dens * ((xmax - xmin) / bins) * nrow(d)
    })
  )

```

```{r}
#| code-fold: true
#| label: fig-hist
#| fig-cap: Histotrams for the fuel loading response variables. A normal curve
#|   with the same mean as sd as the data (and scaled to the same area as that
#|   covered by the bins) has been superimposed for reference.
#| fig-subcap:
#|   - ""
#|   - ""


# Break plot into two panels for higher resolution.
ggplot(filter(hist_dat, class %in% load_vars[1:3] )) +
geom_histogram(data = \(x) unnest(x, data), aes(x = load), bins = bins) +
geom_line(data = \(x) unnest(x, c(norm_x, norm_y)), aes(norm_x, norm_y)) +
facet_grid(treatment ~ class, scales = "free") +
labs(y = "count", x = "Load Mg/ha")

ggplot(filter(hist_dat, class %in% load_vars[4:6] )) +
geom_histogram(data = \(x) unnest(x, data), aes(x = load), bins = bins) +
geom_line(data = \(x) unnest(x, c(norm_x, norm_y)), aes(norm_x, norm_y)) +
facet_grid(treatment ~ class, scales = "free") +
labs(y = "count", x = "Load Mg/ha")

```

For MANOVA we are concered with the within group multivariate normality, the
assumption does not appear to be met here either (@fig-qqplot-model). The code
output below indicates the rows with the greatest deviation from normal.

```{r qq-resid}
#| label: fig-qqplot-model
#| fig-cap: >
#|   Plot A assessed the multivariate normality of residuals given the model
#|   where all loading variabels are a function of the treatment group.
#| fig-subcap:
#|   - ""
#|   - ""

tl <- load2("wide", treatment, all_of(load_vars))
load_mod <- lm(as.matrix(tl[-1]) ~ treatment, data = tl)
m_dist <- heplots::cqplot(load_mod)

load2("wide", everything())[order(m_dist, decreasing = TRUE)[1:10], ]

treatments = c("gs", "ha", "ld", "hd")
par(mfrow = c(2, 2))
invisible(lapply(treatments, 
  \(x) heplots::cqplot(filter(tl, treatment == x)[-1], main = x)
))

```

A number of different tests of multivariate normaliy also confirm the lack of
evidence for meeting this assumption (@tbl-norm-test).

```{r norm-test}
#| label: tbl-norm-test
#| tbl-cap: >
#|   Several different tests of multivariate normality indicate a lack of
#|   evidence to support this assumption.

all_mvn_tests <- function(data) {
  c("mardia", "hz", "royston", "dh", "energy") |>
  map(\(x) MVN::mvn(data = data, subset = "treatment", mvnTest = x)) |>
  map(\(x) x$multivariateNormality) |>
  map(\(x) bind_rows(x, .id = "treatment")) |>
  map_dfr(\(x) 
    filter(x, Test != "MVN") |> 
    mutate( across(where(is.factor), \(f) as.numeric(as.character(f)))) |> 
    select(1:2, statistic = 3, `p value`, Result = last_col())
  )
}

all_mvn_tests(load2("wide", treatment, all_of(load_vars))) |>
  knitr::kable(digits = 4)

```

### Other distributions {#sec-other-distributions}

If our data is not normally distributed, then what distribution is it? I'm going
to assume what we are interested in the distribution of data within groups
(treatments).

I attemped to model the distribution of our conditional response data (fuel size
class by treatment), but it mostly didn't work.

Our data is positive, and mostly highly skewed.

```{r}
#| label: fig-skew-kurt
#| fig-cap: Skewness and kurtosis for fuel classes within treatments.
#| fig-subcap:
#|   - Treatment gs, fuel class onehr      
#|   - Treatment ha, fuel class onehr      
#|   - Treatment hd, fuel class onehr      
#|   - Treatment ld, fuel class onehr      
#|   - Treatment gs, fuel class tenhr      
#|   - Treatment ha, fuel class tenhr      
#|   - Treatment hd, fuel class tenhr      
#|   - Treatment ld, fuel class tenhr      
#|   - Treatment gs, fuel class hundhr     
#|   - Treatment ha, fuel class hundhr     
#|   - Treatment hd, fuel class hundhr     
#|   - Treatment ld, fuel class hundhr     
#|   - Treatment gs, fuel class dufflitter 
#|   - Treatment ha, fuel class dufflitter 
#|   - Treatment hd, fuel class dufflitter 
#|   - Treatment ld, fuel class dufflitter 
#|   - Treatment gs, fuel class thoushr    
#|   - Treatment ha, fuel class thoushr    
#|   - Treatment hd, fuel class thoushr    
#|   - Treatment ld, fuel class thoushr    
#|   - Treatment gs, fuel class veg        
#|   - Treatment ha, fuel class veg        
#|   - Treatment hd, fuel class veg        
#|   - Treatment ld, fuel class veg        
#| layout-ncol: 2
#| column: page

d <- load2("long", treatment, all_of(load_vars)) |> 
  split(~class) |> map(~split(.x, ~treatment))

# imap_dfr(d, \(x, y) tibble(class = y, treatment = names(x) )) |>
#   with(paste0("Treatment ", treatment, ", fuel class ", class)) |>
#   cat(sep = "\n")

# par(mfrow = c(2,2))
walk(d, ~ walk(.x, \(x) fitdistrplus::descdist(x$load, boot = 111)))

# fit_exp <- map(d, ~fitdistrplus::fitdist(.x, "exp"))

# walk(fit_exp, ~plot(.x))

# map(d, \(x) gamlss::fitDist(x, k = 2, type = "realplus")$fits) |>
#   bind_rows(.id = "trt")

```

## Homogeneity of variance {#sec-homogeneity-of-variance}

There seem to be some pretty big differences in the variance between treatments.
This is likely to do with outliers. For linear regression, it is recommended
that maximum variance ration should be below 4.

```{r}

max_var <- load2("long", everything()) |>
  group_by(class, treatment) |>
  summarize(var = sd(load)^2, load = max(load)) |>
  summarize(max_var_rat = round(max(var) / min(var)), x = 3, y = max(load))


load2("long", everything()) |>
  ggplot(aes(treatment, load)) +
  geom_boxplot() +
  geom_text(data = max_var, aes(x, y, label = paste("Maximum variance ratio: ", max_var_rat))) +
  facet_wrap(~class, scales = "free")

```

## Zeros

We do have zeros, which is importnat if we want to employ a glm like Gamma,
which is only defined for positive values.

```{r}

load2("long", everything()) |>
  group_by(class, treatment) |>
  summarize(zeros = sum(load == 0), percent = zeros / n()) |>
  ggplot(aes(treatment, percent)) +
  geom_col() +
  geom_text(aes(label = if_else(zeros == 0, NA, zeros), y = percent / 2), color = "gray70") +
  facet_wrap(~class) +
  scale_y_continuous(labels = scales::percent)

```

## Correlation of response variables

I'm not sure if it's important, but I was curious if the various fuel loading
classes were correlated with eachother. Either across the board, or within a
givin treatment.

```{r}
#| label: fig-response-corr
#| fig-cap: Correlation among the response variables (fuel classes).

GGally::ggpairs(load2("wide", all_of(load_vars)))

```

## Independence

Because of how are data were collected they are not indepdent. The current data
is summarized at the transect level. At that level. We have two transects at
each corner. Because of spatial autocorrelation, these may be correlated with
eachother. Corners (and thus transects) are nested within plots, and plots are
within treatments. Each plot received a different treatment. What I'm not clear
about is: should I incldude a random variable for plots, if i'm including a
fixed effect for treatment?

There are also question about at what level to summarize/model the data. I've
already averaged stations within transects for several variable that were
collected at the station level (two within each transect). What are the
tradeoffs for either averaging at the the corner level, or alternatively,
analyzing our raw station data instead of averaging.



