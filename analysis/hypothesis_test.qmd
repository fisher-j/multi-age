---
title: Hypothesis testing
---

```{r init}

#| echo: false
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
load("./data_long.RData")
source("./scripts/utils.r")
transectid <- c("site", "treatment", "corner", "azi")
load_vars <- c("onehr", "tenhr", "hundhr", "dufflitter", "thoushr", "veg")

```

## Manova and multiple anovas

The often recommended Pillai's Trace Test is robust to the normality assumption.
Follow up with linear discriminant analysis, or multiple one-way anovas
dependidng on research question. Using a Bonferroni correction for rejecting the
null of alpha / m, for m hypothesis, we get an alpha of 0.008 for an alpha of
0.05 and 6 tests.

This suggests that it is unlikely that all treatemnts are equal.

```{r}

tl <- load2("wide", treatment, all_of(load_vars))
myexpr <- expr(cbind(!!!syms(load_vars)) ~ treatment)
test1 <- do.call("manova", list(myexpr, data = quote(tl)))
summary(test1)

```

## Levene test for homogeneity of variance {#sec-levene-test-for-homogeneity-of-variance}

From the looks of the levene test (using median), onehr, tenhr, and veg may all
have different *variances* between groups. 

The one-way anova test results are the same though between equal and unequal
variance assumptions. These tests support the notion that we can't assume that
the mean vegetatvie and onehr fuel loading are equal across all treatments, but
there isn't such evidence for the other fuel loading classes.

```{r}

d <- load2("long", treatment, all_of(load_vars)) |> group_by(class)

d |> nest() |> rowwise() |>
  transmute(
    levene = car::leveneTest(load ~ factor(treatment), data)[[3]][1],
    welch_uneq_var = oneway.test(load ~ treatment, data)$p.value,
    welch_eq_var = oneway.test(load ~ treatment, var.equal = TRUE, data = data)$p.value,
  ) |>
  knitr::kable(digits = 3)

```

We can use the Games Howell test for pairwise comparisons to follow up on the
welches test for differences between means when there is unequal variance among
groups. These p-values provide evidence that for onehr fuels, the mean value of
ha is greater than gs and ld, and the mean value for hd is also greater than gs
and ld. Also, for vegetation, gs is greater than ha only.

```{r}

gh_test <- d |> rstatix::games_howell_test(load ~ treatment) |>
  filter(p.adj.signif != "ns") |>
  rstatix::add_y_position(scales = "free", step.increase = 0.5)
ggpubr::ggboxplot(d, x = "treatment", y = "load", facet.by = "class") +
facet_wrap(~class, scales = "free") +
ggpubr::stat_pvalue_manual(gh_test, label = "p.adj")

```

## Multi-level model {#sec-multi-level-model}

We have transects nested within plot corners, corners nested within plots, and
plots nested within sites. We would like to detect a treatment effect, while
accounting for the non-independence of this nested data structure. The following
model, I believe, captures this grouping structure.

```{r}

form <- load ~ treatment + (1 | site/treatment/corner)

```

```{r}

d <- load2("long", site, treatment, corner, all_of(load_vars)) |> group_by(class)

m1 <- d |> nest() |> rowwise() |>
  transmute(
    mod = list(lme4::lmer(form, data = data)),
    emmeans = list(emmeans::emmeans(mod, "treatment")),
    pairs = list(as_tibble(pairs(emmeans, infer = TRUE)))
  )

```

Pairwise comparisons with Tukey adjustment for each of 6 multilevel models
representing different fuel loading classes reveals that the only evidence for
differences in means among treatments is with vegetation between the gs and ha
treatments. Another sizeable difference in means is between gs and ha for the
onehr fuels (@fig-multilevel-p-value).

```{r}

#| label: tbl-multilevel-p-values
#| tbl-cap: Pairwise comparisons among treatments with p-values < 0.05 for 6
#|   multilevel models.

select(m1, pairs) |> unnest(pairs) |>
  filter(p.value <= 0.05) |>
  knitr::kable(digits = 3)

```

```{r}
#| label: fig-multilevel-p-value
#| fig-cap: 95% confidence intervals and pairwise comparisons of means for 6
#|   mixed models representing different fuel loading classes using package
#|   emmeans.

group_map(m1, ~ plot(.x$emmeans[[1]], comparisons = TRUE) + ggtitle(.x$class)) |>
  patchwork::wrap_plots()

```

Hypothesis testing with multi-level models is not as straight forward with
multi-level models. The problem, [explained here](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#inference-and-confidence-intervals)
is two fold. For GLMMs and unbalanced experimental designs, the null
distribution for the F-statistic may not be F-distributed.

For us, we have a balanced design (I think) and so the F-statistic should be F
distributed *and* degrees of freedom should be clear from the details of the
design. Because of our balanced design, the Kenward-Rogers approach and
"inner-outer" design approach (which is used by `nlme::lme`) give the same
result of 9.

Using the package `pbkrtest` we can get parametric bootstrap liklihood ratio
statistics and test this statistic in a number of different ways. The PBtest
should probably be the most reliable, but I've included descriptions of the
others from the package documentation for reference. I'm also including an
F-test in which degrees of freedom are estimated with Kenward-Rogers approach.

LRT
: Assuming that LRT has a chi-square distribution.

PBtest
: The fraction of simulated LRT-values that are larger or equal to the observed
LRT value.

Bartlett
: A Bartlett correction is of LRT is calculated from the mean of the simulated
LRT-values

Gamma
: The reference distribution of LRT is assumed to be a gamma distribution with
mean and variance determined as the sample mean and sample variance of the
simulated LRT-values.

F
: The LRT divided by the number of degrees of freedom is assumed to be
F-distributed, where the denominator degrees of freedom are determined by
matching the first moment of the reference distribution. 

```{r}
#| cache: true

d <- load2("long", all_of(c(transectid, load_vars)))
dd <- ungroup(d) |> split(~class)

cluster <- parallel::makeCluster(rep("localhost", parallel::detectCores()))
pmod <- imap(dd, function(d, i) {
  form <- load ~ treatment + (1 | site/treatment/corner)
  amod <- lme4::lmer(form, d, REML = FALSE)
  nmod <- update(amod, . ~ . -treatment)
  krtest <- pbkrtest::KRmodcomp(amod, nmod) |>
    pluck("test", \(x) slice(x, 1)) |>
    rename(df = ndf) |>
    rownames_to_column("test")
  pbkrtest::PBmodcomp(amod, nmod, cl = cluster) |>
    pluck(summary, "test") |>
    rownames_to_column("test") |>
    bind_rows(krtest) |>
    mutate(class = i, .before = 1) |>
    relocate(c(df, ddf, F.scaling), .after = stat)
}) |> 
  list_rbind()
parallel::stopCluster(cluster)

pmod |> knitr::kable(digits = c(NA, NA, 2, 1, 2, 1, 4))

```

## Model checking

Taking a look at residual vs. fitted and qqplots of the model, it looks like our
residuals are not normally distributed and there is not constant variance.

```{r}
#| code-fold: true

resid_plot <- function(data) {
  data |>
    ggplot(aes(fitted, resid)) +
    geom_point() +
    facet_wrap(~class, scales = "free") +
    geom_hline(yintercept = 0)
}

qq_plot <- function(data) {
  data |>
    ggplot(aes(sample = resid)) +
    stat_qq() +
    stat_qq_line() +
    facet_wrap(~class, scales = "free")
}

resid_qq_plot <- function(data) {
  data <- unnest(data, c(resid, fitted))
  list(
    a = resid_plot(data),
    b = qq_plot(data)
  )
}

d <- load2("long", all_of(c(transectid, load_vars))) |>
  group_by(class) |> nest() |> rowwise()

```

```{r}
#| label: fig-mod1-diagnostic1
#| fig-height: 10

form <- load ~ treatment + (1 | site/treatment/corner)

mod1 <- d |>
  mutate(
    mod = list(lme4::lmer(form, data)),
    fitted = list(fitted(mod)),
    resid = list(resid(mod, type = "pearson", scaled = TRUE)),
    .keep = "unused"
  ) 
  
resid_qq_plot(mod1) |> patchwork::wrap_plots(ncol = 1)

```

I'll try to control the variance by refitting the model with `nlme::lme` and
using the weights argument. I'll be using the pearson residuals which are
[corrected for heteroscedasticity](https://stackoverflow.com/a/24108158).

I had to use the control argument `sigma = 1` for the model to fit. I'm not sure
why, I read it in the documentation for `nlme::varConstProp`. I'm modeling
variance as a constant proportion of the fitted values of the model. This seems
to have cleaned up the variance.

```{r}
#| label: fig-mod1-diagnostic2
#| fig-height: 10

mod2 <- d |>
  mutate(
    mod = list(nlme::lme(
      fixed = load ~ treatment,
      random = ~ 1 | site/treatment/corner,
      data = data,
      weights = nlme::varConstProp(),
      control = nlme::lmeControl(sigma = 1)
    )),
    fitted = list(fitted(mod)),
    resid = list(resid(mod, type = "pearson")),
    .keep = "unused"
  )

patchwork::wrap_plots(resid_qq_plot(mod2), ncol = 1)

```

I'll compare AIC of the two models to see if one performs better than the other.


first, I want to see if the models produced by lme and lmer are equivalent

```{r}

d |>
  mutate(
    mod1 = list(lme4::lmer(form, data)),
    mod2 = list(nlme::lme(
      fixed = load ~ treatment,
      random = ~ 1 | site/treatment/corner,
      data = data
    )),
    .keep = "unused"
  ) |>
  pivot_longer(-class, names_to = "model") |>
  rowwise() |>
  mutate(s = list(broom.mixed::tidy(value, effect = "fixed"))) |>
  select(class, model, s) |>
  unnest(everything()) |>
  arrange(class, term)

```

they seem equivalent enough, although the random effects variances estimated by
`lmer` are somewhat smaller. Now, lets compare the two `lme` models. I'm fitting
with REML because I'm not changing the fixed effects structure.

```{r}

mod_c <- d |>
  mutate(
    mod2 = list(nlme::lme(
      fixed = load ~ treatment,
      random = ~ 1 | site/treatment/corner,
      data = data,
    )),
    mod3 = list(nlme::lme(
      fixed = load ~ treatment,
      random = ~ 1 | site/treatment/corner,
      data = data,
      weights = nlme::varConstProp(),
      control = nlme::lmeControl(sigma = 1),
    )),
    .keep = "unused"
  ) 

mod_c |>
  mutate(aic = list(across(starts_with("mod"), ~ AIC(.x))))|>
  select(aic)|> unnest(aic)

```

## Other random effects structures

I'm not sure I'm using the correct random effects specification. The somewhat
confusing thing is that I have a random effects nested above *and* below my
fixed effect. This means that when I specify my random effect using the nesting
notation: `1 | site/treatment/corner`, I'm estimating a variance for
`corner:treatment:site`, `treatment:site`, and `site`. The interaction of
treatment and site here is analagous to a plot effect, of which there are 16.

## Bayesian mode

I'll use mostly `brms` defaults (notably, priors) to do a basic bayesian
analysis, using the same formula I used for the lmm above:

```{r}
form <- load ~ treatment + (1 | site/treatment/corner)
```

All the data is nested to facilitate modeling each fuel class separately. We'll
look at the average loading to get an idea of the data.

```{r}

d <- load2("long", all_of(c(transectid, load_vars))) |>
  group_by(class) |> nest() |> rowwise()

unnest(d, data) |> 
  group_by(class, treatment) |>
  summarize(avg_load = mean(load)) |>
  ggplot(aes(treatment, avg_load)) +
    geom_col() +
    facet_wrap(~class, scales = "free_y")
```

Now we'll the model

```{r}
#| message: false
#| warning: false
#| cache: true

bf <- d |>
  mutate(
    mod = list(brms::brm(form, data,
      warmup = 2000,
      iter = 4000,
      cores = 4,
      control = list(adapt_delta = 0.99)
    ))
  )

```

Now I'll add global expected predictions. This represents global expected effect
size, as opposed to the prediction for a new site, or a new plot, or a new
transect. I'm also establishing a region of practical equivalence (ROPE) as 0.2
* sd(load) for each fuel class, that is, I'm suggesting that a difference of
  less than 0.2sd(load) is not enough to consider significant.

```{r}
  
global_mean_epred_contrats <- function(mod) {
  treat_lvls <- mod$data["treatment"] |> unique()
  mod |>
    tidybayes::epred_draws(treat_lvls, re_formula = NA, value = "epred") |>
    tidybayes::compare_levels(epred, by = treatment) |>
    select(contrast = treatment, epred) 
}

rope_size <- 0.2
bf <- bf |>
  mutate(
    epred = list(global_mean_epred_contrats(mod)),
    rope = list(c(-rope_size * sd(data$load), rope_size * sd(data$load)))
  ) 

```

Finally, I'll plot the distributions of the pairwise differences between each
treatment, for each fuel class, with bars to indicate the 95% highest density
region and vertical lines around the ROPE.

```{r}


unnest(bf, c(epred)) |>
  ggplot(aes(x = epred, y = contrast)) +
  tidybayes::stat_halfeye(normalize = "panels") +
  geom_vline(data = unnest(bf, rope), aes(xintercept = rope))  +
  facet_wrap(~class, scales = "free_x") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())

```

We can see these results, numerically, by calculating the area of the posterior
that is greater than (less than) the upper (lower) bounds of the ROPE. I'm
picking which side to compare to based on the sign of the median.

```{r}

# We are [blank] percent certain that the magnitude of the difference between
# treatments is as least 100 kg/ha
unnest(bf, epred) |>
group_by(class, contrast) |>
  summarize(
    rope = rope_size * sd(epred),
    prob = if_else(median(epred) > 0, 
      mean(epred > rope),
      mean(epred < -rope)
    )
  )

```

Here is what the posterior draws look like for the estimated marginal means of
the treatments.

```{r}

# bf1 |>
# emmeans::emmeans(~ treatment, epred = TRUE) |>
#   tidybayes::gather_emmeans_draws() |>
#     ggplot(aes(.value, fill = treatment)) +
#     geom_density(alpha = 0.7) +
#     theme(
#       panel.grid.minor = element_blank(),
#       panel.grid.major = element_blank()
#     ) 

```

## Bayesian model checking

I've heard I should check pairs plots for convergence related problems. To check
model fit, I'll want to do prior predictive and posterior predictive checks. I'm
not sure if normality of residuals comes into play, and I'm not really worried
about normality of the random effects.

```{r}
# This is proposed as a check for model problems, I' not sure what it means, but
# there is correlation among predictors, which is expected here, so I'm not sure
# it's a problem
# brms::mcmc_plot(bf1, type = "pairs", variable = "^b_", regex = TRUE)
#
# brms::prior_summary(bf1)
#
# brms::brm(form, data = d1, sample_prior = "only") |>
#   brms::pp_check(nsamples = 50)
```
