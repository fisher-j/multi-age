---
title: Hypothesis testing
---

```{=html}
<!-- 
source(knitr::purl("hypothesis_test.qmd", tempfile()))
-->
```

```{r init}
#| echo: false

suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
load("./data_long.RData")
source("./scripts/utils.r")
transectid <- c("site", "treatment", "corner", "azi")

theme_update(
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank()
)
# this allows to adjust each panels coordinate limits
source("./scripts/coord_cartesian_panels.r")

class_labels <- c(
    onehr = "1-hr",
    tenhr = "10-hr",
    hundhr = "100-hr",
    thoushr = "1,000-hr",
    dufflitter = "Duff & litter",
    veg = "Live vegetation"
  )
fuel_class_labeller <- labeller(class = class_labels, fuel_class = class_labels)

```

## Frequentist approach

Exploration of frequentist and Bayesian approaches can lead to more robust 
conclusions.

### Manova and multiple anovas

The often recommended Pillai's Trace Test is robust to the normality assumption.
Follow up with linear discriminant analysis, or multiple one-way anovas
dependidng on research question. Using a Bonferroni correction for rejecting the
null of alpha / m, for m hypothesis, we get an alpha of 0.008 for an alpha of
0.05 and 6 tests.

This suggests that it is unlikely that all treatemnts are equal.

```{r}
load_vars <- c("onehr", "tenhr", "hundhr", "dufflitter", "thoushr", "veg")
tl <- load2("wide", treatment, all_of(load_vars))
myexpr <- expr(cbind(!!!syms(load_vars)) ~ treatment)
test1 <- do.call("manova", list(myexpr, data = quote(tl)))
summary(test1)

```

### Multiple one-way anovas

One way anova (using the welch test) can either assume constant variance or not.
A levene test (using median) indicates onehr, tenhr, and veg may all have
different *variances* between groups.

The one-way anova test results are the same though between equal and unequal
variance assumptions. These tests support the notion that we can't assume that
the mean *vegetatvie* and *onehr* fuel loading are equal across all treatments,
but there isn't such evidence for the other fuel loading classes.

```{r}
#| label: tbl-anovas
#| tbl-cap: Levene tests suggest that variances are unequal across treatments
#|   for all fuel loading classes. Welches tests suggest that veg and onehr
#|   fuels may have different means among treatments.

d <- load2("long", treatment, all_of(load_vars)) |> group_by(class)

d |> nest() |>
  rowwise() |>
  transmute(
    levene = car::leveneTest(load ~ factor(treatment), data)[[3]][1],
    welch_uneq_var = oneway.test(load ~ treatment, data)$p.value,
    welch_eq_var = oneway.test(
      load ~ treatment, var.equal = TRUE, data = data
    )$p.value,
  ) |>
  knitr::kable(digits = 3)
  
```

We can use the Games Howell test for pairwise comparisons to follow up on the
welches test for differences between means when there is unequal variance among
groups. These p-values provide evidence that for onehr fuels, the mean value of
ha is greater than gs and ld, and the mean value for hd is also greater than gs
and ld. Also, for vegetation, gs is greater than ha only. While this test is
robust to the assumptions of normality, some of our data is highly skewed. Also,
because of the nesting of our data, observations are not independent, so our
effective sample size is not what is assumed by this test.

```{r}
#| label: fig-pairwise
#| fig-cap: pair-wise tests using Games-Howell, for unequal variances across
#|   groups. This shows many statistically significant differences, but the
#|   assumption of independence, which is likely to have a significant effect on
#|   our effective sample size.

gh_test <- d |> rstatix::games_howell_test(load ~ treatment) |>
  filter(p.adj.signif != "ns") |>
  rstatix::add_y_position(scales = "free", step.increase = 0.5)

ggpubr::ggboxplot(d, x = "treatment", y = "load", facet.by = "class") +
facet_wrap(~class, scales = "free") +
ggpubr::stat_pvalue_manual(gh_test, label = "p.adj") +
scale_y_continuous(expand = expansion(mult = c(0.05, 0.1)))

```

### Multi-level model {#sec-multi-level-model}

We have transects nested within plot corners, corners nested within plots, and
plots nested within sites. We would like to detect a treatment effect, while
accounting for the non-independence of this nested data structure. The following
model, I believe, captures this grouping structure.

```{r}

form <- load ~ treatment + (1 | site/treatment/corner)

```

This will estimate a group-wise intercept adjustments for each site, plot, and
corner, based on modeled variances for each of these grouping levels.

```{r}

d <- load2("long", site, treatment, corner, all_of(load_vars)) |> 
  group_by(class)

m1 <- d |> nest() |> rowwise() |>
  transmute(
    mod = list(lme4::lmer(form, data = data)),
    emmeans = list(emmeans::emmeans(mod, "treatment")),
    pairs = list(as_tibble(pairs(emmeans, infer = TRUE)))
  )

```

Pairwise comparisons with Tukey adjustment for each of 6 multilevel models
representing different fuel loading classes reveals that the only evidence for
differences in means among treatments is with vegetation between the gs and ha
treatments. Another sizeable difference in means is between gs and ha for the
onehr fuels (@fig-multilevel-p-value).

```{r}
#| label: tbl-multilevel-p-values
#| tbl-cap: Pairwise comparisons among treatments with p-values < 0.05 for 6
#|   multilevel models. Only Veg, gs-ha comparison is statistically significant.

select(m1, pairs) |> unnest(pairs) |>
  filter(p.value <= 0.05) |>
  knitr::kable(digits = 3)

```

```{r}
#| label: fig-multilevel-p-value
#| fig-cap: 95% confidence intervals and pairwise comparisons of means for 6
#|   mixed models representing different fuel loading classes using package
#|   emmeans.

group_map(m1, ~ plot(.x$emmeans[[1]], comparisons = TRUE) + ggtitle(.x$class)) |>
  patchwork::wrap_plots()

```

Hypothesis testing with multi-level models is not as straight forward with
multi-level models. The problem, [explained
here](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#inference-and-confidence-intervals)
is two fold. For GLMMs and unbalanced experimental designs, the null
distribution for the F-statistic may not be F-distributed.

For us, we have a balanced design (I think) and so the F-statistic should be F
distributed *and* degrees of freedom should be clear from the details of the
design. Because of our balanced design, the Kenward-Rogers approach and
"inner-outer" design approach (which is used by `nlme::lme`) give the same
result of 9 DF for all of the pairwise tests.

Using the package `pbkrtest` we can get parametric bootstrap liklihood ratio
statistics and test this statistic in a number of different ways. The PBtest
should probably be the most reliable, but I've included descriptions of the
others from the package documentation for reference. I'm also including an
F-test in which degrees of freedom are estimated with Kenward-Rogers approach.

LRT

:   Assuming that LRT has a chi-square distribution.

PBtest

:   The fraction of simulated LRT-values that are larger or equal to the
    observed LRT value.

Bartlett

:   A Bartlett correction is of LRT is calculated from the mean of the simulated
    LRT-values

Gamma

:   The reference distribution of LRT is assumed to be a gamma distribution with
    mean and variance determined as the sample mean and sample variance of the
    simulated LRT-values.

F

:   The LRT divided by the number of degrees of freedom is assumed to be
    F-distributed, where the denominator degrees of freedom are determined by
    matching the first moment of the reference distribution.

```{r}
#| warning: false
#| label: tbl-pbtest
#| tbl-cap: >
#|   Liklihood ratio tests and parametric boot strap tests of model
#|   significance: whether the model with treatment, fits the data better than
#|   the intercept only model (adjusting for nesting structure).

d <- load2("long", all_of(c(transectid, load_vars)))
dd <- ungroup(d) |> split(~class)


if(file.exists("pmod.Rdata")) {
  load("pmod.Rdata")
} else {
  cluster <- parallel::makeCluster(rep("localhost", parallel::detectCores()))

  pmod <- imap(dd, function(d, i) {
    form <- load ~ treatment + (1 | site/treatment/corner)
    amod <- lme4::lmer(form, d, REML = FALSE)
    nmod <- update(amod, . ~ . -treatment)
    krtest <- pbkrtest::KRmodcomp(amod, nmod) |>
      pluck("test", \(x) slice(x, 1)) |>
      rename(df = ndf) |>
      rownames_to_column("test")
    pbkrtest::PBmodcomp(amod, nmod, cl = cluster) |>
      pluck(summary, "test") |>
      rownames_to_column("test") |>
      bind_rows(krtest) |>
      mutate(class = i, .before = 1) |>
      relocate(c(df, ddf, F.scaling), .after = stat)
  }) |> 
    list_rbind() |>
    filter(test %in% c("LRT", "PBtest"))
 
  parallel::stopCluster(cluster)
  save(pmod, file = "pmod.Rdata")
}

pmod |> knitr::kable(digits = c(NA, NA, 2, 1, 2, 1, 4))

```

### Model checking

Taking a look at residual vs. fitted and qqplots of the model, it looks like our
residuals are not normally distributed and there is not constant variance.

```{r}
#| code-fold: true

# These are functions to plot for each model, residuals vs fitted and normal
# quantiles. The third function is a wrapper to do both.
resid_plot <- function(data) {
  data |>
    ggplot(aes(fitted, resid)) +
    geom_point() +
    facet_wrap(~class, scales = "free") +
    geom_hline(yintercept = 0)
}

qq_plot <- function(data) {
  data |>
    ggplot(aes(sample = resid)) +
    stat_qq() +
    stat_qq_line() +
    facet_wrap(~class, scales = "free")
}

resid_qq_plot <- function(data) {
  data <- unnest(data, c(resid, fitted))
  list(
    a = resid_plot(data),
    b = qq_plot(data)
  )
}

d <- load2("long", all_of(c(transectid, load_vars))) |>
  group_by(class) |> nest() |> rowwise()

```

```{r}
#| label: fig-mod1-diagnostic
#| fig-height: 10
#| fig-cap: Residual vs fitted and normal quantile-quantile plots for a
#|   multi-level model with un-pooled treatment intercepts and partially pooled
#|   (random effects) for nested data. Fit using `lme4` The residuals are not
#|   homogenous.

form <- load ~ treatment + (1 | site/treatment/corner)

mod1 <- d |>
  mutate(
    mod = list(lme4::lmer(form, data)),
    fitted = list(fitted(mod)),
    resid = list(resid(mod, type = "pearson", scaled = TRUE)),
    .keep = "unused"
  ) 
  
resid_qq_plot(mod1) |> patchwork::wrap_plots(ncol = 1)

```

I'll try to control the variance by refitting the model with `nlme::lme` and
using the weights argument. I'll be using the pearson residuals which are
[corrected for heteroscedasticity](https://stackoverflow.com/a/24108158).

I had to use the control argument `sigma = 1` for the model to fit. I'm not sure
why, I read it in the documentation for `nlme::varConstProp`. I'm modeling
variance as a constant proportion of the fitted values of the model. This seems
to have cleaned up the variance.

```{r}
#| label: fig-mod2-diagnostic
#| fig-height: 10
#| fig-cap: Same as @fig-mod1-diagnostic but variance is modeled as a function
#|   fitted values, assuming a linear relationship. Fit with `nlme`. The
#|   (scaled) residuals are more homogenous now.

mod2 <- d |>
  mutate(
    mod = list(nlme::lme(
      fixed = load ~ treatment,
      random = ~ 1 | site/treatment/corner,
      data = data,
      weights = nlme::varConstProp(),
      control = nlme::lmeControl(sigma = 1)
    )),
    fitted = list(fitted(mod)),
    resid = list(resid(mod, type = "pearson")),
    .keep = "unused"
  )

patchwork::wrap_plots(resid_qq_plot(mod2), ncol = 1)

```

I'll compare AIC of the two models to see if one performs better than the other.

first, I want to see if the models produced by lme and lmer are equivalent

```{r}

d |>
  mutate(
    mod1 = list(lme4::lmer(form, data)),
    mod2 = list(nlme::lme(
      fixed = load ~ treatment,
      random = ~ 1 | site/treatment/corner,
      data = data
    )),
    .keep = "unused"
  ) |>
  pivot_longer(-class, names_to = "model") |>
  rowwise() |>
  mutate(s = list(broom.mixed::tidy(value, effect = "fixed"))) |>
  select(class, model, s) |>
  unnest(everything()) |>
  arrange(class, term)

```

they seem equivalent enough, although the random effects variances estimated by
`lmer` are somewhat smaller. Now, lets compare the two `lme` models. I'm fitting
with REML because I'm not changing the fixed effects structure.

```{r}
#| label: tbl-multilivel-aic-compare
#| tbl-cap: >
#|   Comparison of AIC between multilevel models with and without weights to
#|   account for heterogeneity of variance. The models with weights have
#|   consistently lower AIC, indicating better fit.

mod_c <- d |>
  mutate(
    unweighted = list(nlme::lme(
      fixed = load ~ treatment,
      random = ~ 1 | site/treatment/corner,
      data = data,
    )),
    weighted = list(nlme::lme(
      fixed = load ~ treatment,
      random = ~ 1 | site/treatment/corner,
      data = data,
      weights = nlme::varConstProp(),
      control = nlme::lmeControl(sigma = 1),
    )),
    .keep = "unused"
  ) 

mod_c |>
  mutate(aic = list(across(matches("weight"), ~ AIC(.x))))|>
  select(aic)|> unnest(aic) |> knitr::kable()

```

This indicates that the model with modeled variance fits the data better than
the model without.

Does this change our conclusions about the effect of the treatment?

```{r}

d <- load2("long", site, treatment, corner, all_of(load_vars)) |> 
  group_by(class)

m2 <- d |> nest() |> rowwise() |>
  transmute(
    mod = list(nlme::lme(
      fixed = load ~ treatment,
      random = ~ 1 | site/treatment/corner,
      data = data,
      weights = nlme::varConstProp(),
      control = nlme::lmeControl(sigma = 1),
    )),
    emmeans = list(emmeans::emmeans(mod, "treatment")),
    pairs = list(as_tibble(pairs(emmeans, infer = TRUE)))
  )

```

```{r}
#| label: tbl-multilevel-p-values2
#| tbl-cap: >
#|   Pairwise comparisons of treatments (four levels) with p-values 
#|   < 0.05 for six multilevel models with variance modeled as a 
#|   linear relationsihp with the fitted value.

select(m2, pairs) |> unnest(pairs) |>
  filter(p.value <= 0.05) |>
  knitr::kable(digits = 3)

```

```{r}
#| label: fig-multilevel-p-value2
#| fig-cap: 95% confidence intervals and pairwise comparisons of means for 6
#|   mixed models representing different fuel loading classes using a model with
#|   variance modeled as a linear relationsihp with the fitted value.
 
group_map(m2, ~ plot(.x$emmeans[[1]], comparisons = TRUE) + ggtitle(.x$class)) |>
  patchwork::wrap_plots()

```

No, it doesn't seem to make much of a difference.

### Other random effects structures

I'm not sure I'm using the correct random effects specification. The somewhat
confusing thing is that I have a random effects nested above *and* below my
fixed effect. This means that when I specify my random effect using the nesting
notation: `1 | site/treatment/corner`, I'm estimating a variance for
`corner:treatment:site`, `treatment:site`, and `site`. The interaction of
treatment and site here is analagous to a plot effect, of which there are 16.

## Bayesian mode

```{r}
#| include: false
library(tidybayes)
library(brms)
library(bayesplot)
library(ggdist)
```

I'll use `brms` with the same formula I used for the lmm above. First we'll
reload our data.

All the data is nested to facilitate modeling each fuel class separately. We'll
look at the average loading to get an idea of the data.

```{r}

d <- load2("long", all_of(c(transectid, load_vars))) |>
  group_by(class) |> nest() |> rowwise()

```

### Gaussian model

I started out using a Gaussian model for load, with mostly default priors on the
random effects. This was for convenience more than for anything. A model with
positive support only, and more approriate priors would improve the model. This
parameterization also results in the first treatment level (gs) being
interpreted as the global intercept. This implies the unfortunate assumption
that the other levels entail more variability as their priors result from the
combination of the intercept prior as well as the treatment prior, wheres the
prior for the first level only contains the variability in the intercept. Thus,
parameterizing the model by removing the intercept for the main effect would
allow equivalent interpretations of all four treatment levels. I use this
approach below in the Gamma model.

#### Priors

We are using mostly uninformative priors for our un-pooled (fixed effect)
estimates of treatment intercepts. They are all set as normal distributions,
centered at the median of the fuel load with a sd of 2.5 times the sd of the
data. While it is not possible to have negative values here, a current
limitation of the brms package is that you can't put bounds on individual
coefficients, and we would not want to contstrain our treatment effects to be
positive, as they are centered around the mean and will be both positive and
negative.

```{r define-priors}
#| label: fig-priors
#| fig-cap: Normal priors for the intercept are centered at the median and sd of 
#|   2.5 times the sd of the data. For the fixed effect treatment, they are 
#|   centered at zero with the a standard deviation equal to 1.5 sd of the data.
#|   Fixed treatments are relative to the first treatment level (GS) which is 
#|   used as the intercept. 

# add calcualted priors to the data
library(ggdist)

d <- mutate(d, 
  priors = list(
    brms::set_prior(
      str_glue(
        "normal( {round(median(data$load))}, {round(2.5 * sd(data$load))} )"
      ), 
      # lb = 0, 
      class = "b", coef = "Intercept"
    ) +
    brms::set_prior(
      str_glue(
        "normal( 0, {round(1.5 * sd(data$load))} )"
      ), 
      class = "b"
    )
    # brms::set_prior("student_t(3, 0, 2.5)",  lb = 0, class = "sd")
  )
)

# Plot the priors
d |>
  mutate(
    prior_dist = list(tidybayes::parse_dist(priors) |> 
    rename(dist_class = class))
  ) |>
  unnest(prior_dist) |>
  ggplot(aes(ydist = .dist_obj, color = interaction(coef, dist_class))) +
    stat_slab(normalize = "panels", fill = NA) +
    stat_pointinterval(position = position_dodge(width = 0.3, preserve = "single")) +
    geom_text(
      aes(label = format(.dist_obj), y = mean(.dist_obj), x = 0.97, vjust = 0),
      position = position_dodge(width = 0.3), show.legend = FALSE
    ) +
    coord_flip() +
    facet_wrap(~ class, scales = "free_x") +
    scale_color_hue(name = "prior", labels = c("treatment", "intercept", "sd"))

```

#### Model fitting

Now we'll fit the model.

```{r model-spec}
#| message: false

form <- load ~  0 + Intercept + treatment + (1 | site/treatment/corner)

bf2 <- mutate(d,
  mod = list(brms::brm(form, data,
    warmup = 3000,
    iter = 4000,
    cores = 4,
    control = list(adapt_delta = 0.99),
    prior = priors,
    sample_prior = TRUE,
    file = paste0("fits/bf2_", class)
  ))
)

```

#### Prior check

```{r prior-check-functions}
#| code-fold: true

# Plod density of model priors against posterior to get a sense of the
# informativeness of the prior. Average over all fixed effects.

plot_pri_post <- function(mod) {
  tidy_draws(mod) |> 
  select(c(matches("b_"), matches("sd"), matches("sigma"), matches("hu"))) |>
  pivot_longer(everything()) |>
  mutate(
    name = if_else(
      str_starts(name, "prior"),
      name, 
      paste0("posterior_", name)
    )
  ) |>
  separate_wider_regex(name, 
      c(phase = "prior|posterior", "_", name = ".*")
  ) |>
  mutate(name = str_remove(name, "__.*")) |>
  ggplot(aes(x = value, color = phase, fill = phase)) +
  stat_slab(normalize = "panels", alpha = 3/4) +
  stat_pointinterval(
    position = position_dodge(width = 0.3, preserve = "single")
  ) +
  facet_wrap(~ name, scales = "free", labeller = fuel_class_labeller) +
  labs(x = "Parameter value", y = "Density") +
  scale_fill_manual(values = c("gray60", "gray30")) +
  scale_color_manual(values = c("gray60", "gray30"))
}

```

```{r}
#| echo: false
#| fig-label: gaus-pri-post-sd
#| fig-cap: >
#|   Sampled prior  and posterior distributions for  Gaussian model
#|   variables that have priors, for one fuel class. These include the sd
#|   for the random effects as well as sigma: the model residuals, and the fixed
#|   effect of treatment. The plots for the other fuel classes look similar.

plot_pri_post(bf2$mod[[1]]) + ggtitle("1-hr")
# plot_pri_post(bf2$mod[[2]]) + ggtitle("10-hr")
# plot_pri_post(bf2$mod[[3]]) + ggtitle("100-hr")
# plot_pri_post(bf2$mod[[4]]) + ggtitle("duff & litter")
# plot_pri_post(bf2$mod[[5]]) + ggtitle("1000-hr")
# plot_pri_post(bf2$mod[[6]]) + ggtitle("Vegetation")
```

#### Posterior Predictive check (Gaussian)

```{r posterior-check-func}
#| code-fold: true

posterior_predictive_check <- function(models) {
  models <- mutate(models,
    pred = list(tidybayes::add_predicted_draws(data, mod, ndraws = 15)),
    lims = list(tibble(
      xmin = min(
        quantile(data$load, .001),
        quantile(pred$.prediction, .001)
      ),
      xmax = max(
        quantile(data$load, .999),
        quantile(pred$.prediction, .999)
      ))
    )
  )
  ggplot() + 
  geom_line(
    data = unnest(models, data),
    aes(x = load),
    stat = "density", size = 1
  ) +
  geom_line(
    data = unnest(models, pred),
    aes(x = .prediction, group = .draw),
    stat = "density", alpha = 0.15, size = 1
  ) +
  facet_wrap(~class, scales = "free", labeller = fuel_class_labeller) +
  coord_cartesian_panels(
    panel_limits = unnest(select(models, lims), lims)
  )
}

```

Here is a posterior predictive check.

```{r}
#| label: fig-gaus-pp-check
#| fig-cap: Density of the observed data (y) plotted against 10 random 
#|   draws from the posterior predictive distribution.

posterior_predictive_check(bf2)

```

The Gaussian distribution is symmetric and doesn't capture well the peak near
zero and the long right tail of our observed values for most of the fuel
classes. It also dramatically overpredicts negative values (which are absent
from our data, despite the fact that the density smoothing of the observed
values seems to suggest there are some.)

The fact that the model predicts negative values suggests that it is not right
for our data, and could potentially be biasing comparissons between treatments.
A Gamma distribution for the response makes more sense.

### Gamma model

The gamma model has support for only postive values, which makes sense for our
weight per area data. Initial data exploration also revealed that the gamma
appears to be a good fit for our data. This makes sense, as our data is
fundamentally transformed count data, and the gamma distribution is the
continuous generalization of the negative binomial, which is used for modeling
count data.

#### Formula

I'm assuming the outcome is hurdle gamma distributed.

$$ 
\operatorname{HurdleGamma}(y \mid \mu, \text{shape}, \text{hu}) =
\begin{cases}
\text{hu} 
  &\quad\text{if } y = 0, \text{ and}\\
(1 - \text{hu}) \operatorname{Gamma}(y \mid \alpha, \beta)
  &\quad\text{if } y > 0,
\end{cases}
$$

Where $\mu = \frac{\alpha}{\beta}$, and $\text{shape} = \beta$. This is a
mixture model where the proportion of zeros are estimated and the rest of the
observations are assumed to be gamma distributed. Because the gamma distribution
doesn't incldue support for zero, we don't have to worry about any "overlapping"
zero predictions from the gamma portion of the model.

Our model form is as follows:

$$  
\begin{align*}
y_i &\sim \operatorname{HurdleGamma}(\mu, \text{shape}, \text{hu})\\
\log(\mu_i) &= \bar{\alpha} + 
  U_{\text{site}[i]}\sigma_{u} +
  V_{\text{plot}[i]}\sigma_{v} +
  W_{\text{corner}[i]}\sigma_{w} +
  \beta_{treatment[i]}\\
\bar{\alpha} &\sim \operatorname{normal}(M, S)\\
U_j &\sim \operatorname{normal}(0, 1) \qquad \text{for } j=1 \dots 4\\
V_j &\sim \operatorname{normal}(0, 1) \qquad \text{for } j=1 \dots 16\\
W_j &\sim \operatorname{normal}(0, 1) \qquad \text{for } j=1 \dots 64\\
\sigma_u &\sim \operatorname{normal^+}(0, 1)\\
\sigma_v &\sim \operatorname{normal^+}(0, 1)\\
\sigma_w &\sim \operatorname{normal^+}(0, 1)\\
\beta_j &\sim \operatorname{normal}(0, 1) \qquad \text{for } j=1 \dots 4\\
\text{shape} &\sim \operatorname{Gamma}(0.01, 0.01)\\
\text{hu} &\sim \operatorname{beta}(1, 1)
\end{align*}
$$ {#eq-bf4a}

Here is an interpretation for each line of the above model.

1. Observations of transects are assumed to be hurdle gamma distributed given
   the fixed and random intercept, grouping structure.
2. The log fuel loading for a transect is a function of the grand mean plus
   random effects for site, plot, and corner, and fixed effect of treatment.
   Each $U$, $V$, and $W$, is accompanied by a $\sigma$ parameter. This is the
   non-centered parameterization and is equivalent to just defining a
   distribution with a scale of $\sigma$, but factoring out the scale parameter
   leads to better numerical stability in the Markov-chain Monte Carlos
   algorithm.
3. The prior for bar-a, the grand mean, is assumed to be normally distributed
   with mean and SD of $M$ and $S$, repsectively. These are chosen based on the
   mean and 2.5 times the SD of the data converted to produce a log normal
   distribution on the log scale. This should be a minimally informative prior.
4. Each of the four unique site effects come from a normal distribution centered
   on zero.
5. Same as above for 16 plot
6. Same as above for 64 corners.
7. The site effects come from a normal distribution with a SD that is assumed
   to be from a postive constrained, standard normal distribution. On the
   response scale, about 95% of the mass of the joint distribution of this line
   and line 4 ($U_j\sigma_u$ is in the interval $[0.11,8.9]$), which implies
   that we would be supprised to see any site with a load more than 9 times any
   other.
8. Same as 7 for plots.
9. Same as 7 for corners.
10. The fixed effect of treatment on the log scale is assumed to be from a
    standard normal distribution. On the response scale, we would be supprised
    if the effect of any treatment was more than seven times any other.
11. This is the default prior for the shape parameter of the gamma distribution
    in `brms`. It is chosen to be minimally informative.
12. Our prior assumption about the proportion of zeros is also minimally
    informative, it is mostly uniform between 0 and 1.

One problem with these independent prior assumptions is that the multiplicative
effect of the terms on the response scale (due to exponentiation) can lead to 
impossibly large predictiontions, particularly when multiple random or fixed
effect parameters are sampled simultaneosly in their upper tails. For instance,
if the effects of site, plot, and corner where sampled at 2 (on the log scale)
and the effect of treatment, at 1.8, this could lead to a predicted $\mu$ more 
than 2,400 *times* greater than the grand mean.

A better choice would be to use a joint prior, like the Dirichlet, so a prior 
could be set on the total variance and the individual component variances would
vary (in either a eqaul, or unequally weighted fashioin) in their repsective
proportions of that total.

In this regard, with either independent, or independet priors, further work 
could be done on establshing the relative importance of each variance component
and setting priors that reflect these differences. For our nesting structure, 
this means defining the length scale over which fuels are expected to vary 
most/least, from 10 m to 50 m to > 100 m, to thousands of meters.

#### Compute priors

First I'll set my prior. This is complicated because we are now working with a
log link. According to Solomon Kurz, [this is how] to transform your mean and
sd for a normal distribution on the identity scale, to the equivalent normal
distribution on the log scale (a lognormal distribution). This is what we'll
use for our prior on the grand mean ($\bar{\alpha}$).

[this is how]: (https://solomonkurz.netlify.app/blog/2023-05-14-causal-inference-with-gamma-regression-or-the-problem-is-with-the-link-function-not-the-likelihood/#causal-inference-with-bayesian-gamma-regression)

```{r convert-prior}

d <- load2("long", all_of(c(transectid, load_vars))) |>
  group_by(class) |> nest() |> rowwise()

lnp <- function(data) {
  # Desired values
  m <- mean(data)
  s <- 2 * sd(data)
  # use the equations
  mu    <- log(m / sqrt(s^2 / m^2 + 1))
  sigma <- sqrt(log(s^2 / m^2 + 1))
  # output mu and sigma on lognormals own scale
  list(mu = round(mu, 2), sigma = round(sigma, 2))
}

```

```{r define-priors2}
#| warning: false
#| label: fig-priors2
#| fig-cap: >
#|   Normal priors for the grand mean (global intercept) centered at the median 
#|   and sd of 2.5 times the sd of the data.

# data frame to plot the priors
pd <- d |>
  mutate(
    priors = list(brms::set_prior(
      str_glue("normal({mu}, {sigma})", .envir = lnp(data$load))
    )),
    prior_dist = list( tidybayes::parse_dist(priors)),
    lims = list(
      tibble(
        xmin = quantile(exp(prior_dist$.dist_obj), .01),
        xmax = quantile(exp(prior_dist$.dist_obj), .99)
      )
    )
  ) |>
  rename(fuel_class = class)

ggplot(data = unnest(pd, prior_dist, names_repair = "universal")) +
tidybayes::stat_halfeye(
  aes(xdist = exp(.dist_obj)), 
  normalize = "panels"
) +
geom_text(aes(label = format(.dist_obj), x = mean(exp(.dist_obj)), y = 0.97, hjust = 0)) +
facet_wrap(~ fuel_class, scales = "free_x", labeller = fuel_class_labeller) +
coord_cartesian_panels(panel_limits = unnest(select(pd, lims), lims)) +
labs(x = expression(exp(bar(alpha))))

```

#### Prior predictive check

The below code ran with very many divergent transitions and the results did not
really make sense. This may have to do with [a comment I found
here](https://discourse.mc-stan.org/t/prior-predictive-simulation-gives-unreasonable-results-for-lognormal-brms-model/16914/2?u=fisher-j):
independent priors specified on the log link usually don't sample.

An alternative is refitting the whole model with different priors and comparing
the output to infer the prior influence. I'm not going to do that right now.

I will though, sample from the priors while fitting the model in order to plot those along with the posterior for our variables of interest.

```{r}

# prior_only_bf4a <- mutate(bf4a, 
#   mod = list(update(mod, 
#     sample_prior = "only", 
#     cores = 4,
#     warmup = 4000,
#     iter = 5000,
#     control = list(adapt_delta = .99)
#   ))
# )

```

#### Model fitting

Here I actually fit several models. They were each explored to some degree in an
iterative process of model fitting and checking. For the sake of brevity, I
I have focussed on just one of them (`bf4a`) for displaying final results. This
model is the one described mathematically above.

```{r}

bf4a <- mutate(d,
  priors = list(
    set_prior(
      str_glue("normal({mu}, {sigma})", .envir = lnp(data$load)),
      nlpar = "a", coef = "Intercept"
    ) +
    set_prior("normal(0, 1)", nlpar = "a", class = "sd") +
    set_prior("normal(0, 1)", nlpar = "b", coef = "treatmentgs") +
    set_prior("normal(0, 1)", nlpar = "b", coef = "treatmentha") +
    set_prior("normal(0, 1)", nlpar = "b", coef = "treatmenthd") +
    set_prior("normal(0, 1)", nlpar = "b", coef = "treatmentld")
  ),
  mod = list(brms::brm(
    brms::bf(
      load ~ a + b,
      a ~ 1 + (1 | site/treatment/corner),
      b ~ 0 + treatment,
      nl = TRUE),
    data,
    warmup = 4000,
    iter = 5000,
    cores = 4,
    control = list(adapt_delta = 0.99),
    family = brms::hurdle_gamma(),
    prior = priors,
    sample_prior = TRUE,
    file = paste0("fits/bf4a_", class)
  ))
)

```

```{r}
#| message: false
#| code-fold: true

bf3 <- mutate(d,
  priors = list(brms::set_prior(
    str_glue("normal({mu}, {sigma})", .envir = lnp(data$load))
  )),
  mod = list(brms::brm(form, data,
    warmup = 5000,
    iter = 6000,
    cores = 4,
    control = list(adapt_delta = 0.99),
    family = brms::hurdle_gamma(),
    prior = priors,
    file = paste0("fits/bf3_", class)
  ))
)

bf4 <- mutate(d,
  priors = list(
    set_prior(
      str_glue("normal({mu}, {sigma})", .envir = lnp(data$load)),
      nlpar = "a", coef = "Intercept"
    ) +
      set_prior("student_t(3, 0, 0.35)", nlpar = "a", class = "sd") +
      set_prior("student_t(3, 0, 0.35)", nlpar = "b", coef = "treatmentgs") +
      set_prior("student_t(3, 0, 0.35)", nlpar = "b", coef = "treatmentha") +
      set_prior("student_t(3, 0, 0.35)", nlpar = "b", coef = "treatmenthd") +
      set_prior("student_t(3, 0, 0.35)", nlpar = "b", coef = "treatmentld")
  ),
  mod = list(brms::brm(
    brms::bf(
      load ~ a + b,
      a ~ 1 + (1 | site / treatment / corner),
      b ~ 0 + treatment,
      nl = TRUE
    ),
    data,
    warmup = 4000,
    iter = 5000,
    cores = 4,
    control = list(adapt_delta = 0.99),
    family = brms::hurdle_gamma(),
    prior = priors,
    sample_prior = TRUE,
    file = paste0("fits/bf4_", class)
  ))
)


bf5 <- mutate(d,
  priors = list(brms::set_prior(
    str_glue("normal({mu}, {sigma})", .envir = lnp(data$load))
  )),
  mod = list(brms::brm(
    brms::bf(load ~ 0 + Intercept + (1 | site / treatment / corner) + (1 | treatment)),
    data,
    warmup = 4000,
    iter = 5000,
    cores = 4,
    control = list(adapt_delta = 0.99),
    family = brms::hurdle_gamma(),
    prior = priors,
    file = paste0("fits/bf5_", class)
  ))
)

```

#### Model summaries

```{r}
#| warning: false
#| label: tbl-model-summaries
#| tbl-cap: >
#|   Summaries of parameters and dianostics. Rhat is a measure of model
#|   convergence, well mixed chains have an Rhat of 1, bulk and tail ESS are
#|   measures of effective sample size.

my_summary <- function(mod) {
  as_draws_df(mod) |>
    select(starts_with(c("b_", "sd_")), shape, hu) |>
    posterior::summarize_draws(
      "mean", ~posterior::quantile2(.x, c(0.05, .5, .95)), "rhat", "ess_bulk",
      "ess_tail"
    )
}

bf4a |>
  mutate(
    summary = list(my_summary(mod))
  ) |>
  select(summary) |>
  unnest(summary) |>
  mutate(
    variable = str_remove_all(variable, "b_._|_a_")
  ) |>
  knitr::kable(digits = c(NA, NA, 2, 2, 2, 2, 2, 0, 0))

```


#### Prior check

While it was not possible to sample from the prior using the MCMC sampler which
was likely due to the extremely long tails implied by our exponentiated,
independent priors, another way of describing priors is simply sampling random
from random number generators. The results of these random samples are plotted 
against the posterior distributions for the variables that we set priors for.

```{r}
#| label: fig-gamma-pri-post
#| fig-cap: >
#|   Posterior and prior distributions for variables we set priors for, for one
#|   fuel class model.


plot_pri_post(bf4a$mod[[1]]) + ggtitle("1-hr")
# plot_pri_post(bf4a$mod[[2]]) + ggtitle("10-hr")
# plot_pri_post(bf4a$mod[[3]]) + ggtitle("100-hr")
# plot_pri_post(bf4a$mod[[4]]) + ggtitle("duff & litter")
# plot_pri_post(bf4a$mod[[5]]) + ggtitle("1000-hr")
# plot_pri_post(bf4a$mod[[6]]) + ggtitle("Vegetation")
```

#### Posterior predictive check

```{r}
#| label: fig-gamma-pp-check
#| fig-cap: Density of the observed data (y) plotted against 10 random draws
#|   from the posterior predictive distribution.
#| echo: false

posterior_predictive_check(bf4a)
```

The gamma model fits the data better. There are no predictions below zero
anymore. It does seem like the gamma distribution tends to predict higher
densities of lower values than we observed, as seen in the plots for the tenhr,
thoushr, and veg fuel classes. But generally, the predictions appear to agree
with the observed data pretty well.

#### Expected value of the posterior predictive

```{r predict-func}

# get posterior predictions for treatments for models for all fuel classes, 
# ignoring random effects. These are predictions for the expected value across
# all sites.
predict_posterior_expected <- function(data, plot = TRUE, re_formula = NA) {
  newdata <- tidyr::expand(data$data[[1]], nesting(treatment))
  data <- mutate(data,
    pred = list(
      tidybayes::epred_draws(mod, newdata, re_formula = re_formula, 
        value = "pred")
    ),
    lims = list(
      tibble(xmin = 0, xmax = quantile(pred$pred, .995))
    )
  )
  if (plot) {
    p <- data |> unnest(pred) |>
      ggplot(aes(pred, treatment)) +
      tidybayes::stat_halfeye(normalize = "panels") +
      facet_wrap(~class, scales = "free_x", labeller = fuel_class_labeller) +
      coord_cartesian_panels(panel_limits = unnest(select(data, lims), lims)) +
      scale_y_discrete(labels = toupper) +
      labs(x = expression(Load~(Mg%.%ha^-1)), y = "Treatment")
    print(p)
  }
  invisible(data)
}

```

```{r}
#| label: fig-expected-predictions
#| fig-cap: >
#|   Posterior expected predictions, with no random effects. This reprsents the
#|   expected average conditions across all sites. The point estimate is the
#|   mode. Units are mg ha-1. Upper and lower limits are the 95% credible
#|   intervals.
#| echo: false

expected_predictions <- predict_posterior_expected(bf4a)

```

These are the expected predictions, or predictions for the mean. It only
includes the uncertainty in the mean and not the variance in predictions
estimated by the model.

There is quite a bit of uncertainty about the mean all around, but there is a
notable difference in that uncertainty among treatments for the onehr and veg
fuel classes.

Table @tbl-expected-predictions shows these data in a tabular format. I'm using
the highest density continuous interval because, while its hard to see in
@fig-expected-predictions, the highest desity region is actually slightly
discontinuous.

```{r}
#| label: tbl-expected-predictions
#| tbl-cap: Tabular data associated with @fig-expected-predictions. Upper and
#|   lower pertain to the 95% highest density *continuous* interval.

expected_predictions |>
  mutate(
    summary = list(tidybayes::mode_hdci(pred))
  ) |> select(summary) |> unnest(summary) |>
  select(treatment, prediction = pred, lower = .lower, upper = .upper) |>
  knitr::kable(digits = 1)

```

```{r comparisons-func}
#| code-fold: true

predict_expected_contrasts <- function(data, rope_size, plot = TRUE, 
                                       re_formula = NA) {
  # Assume treatment levels are the same for all models: they are.
  newdata <- expand(data$data[[1]], nesting(treatment))
  d <- data |>
    mutate(
      pred = list(
        tidybayes::epred_draws(mod, newdata, re_formula = NA, value = "pred") |>
        tidybayes::compare_levels(pred, by = treatment) |>
        select(contrast = treatment, pred) 
      ),
      rope = rope_size * sd(data$load),
      lims = list(
        tibble(xmin = quantile(pred$pred, .001), xmax = quantile(pred$pred, .999))
      )
    )
  if (plot) {
    p <- d |>
      unnest(c(pred)) |>
      ggplot(aes(x = pred, y = contrast)) +
      tidybayes::stat_halfeye(normalize = "panels") +
      geom_vline(aes(xintercept = rope))  +
      geom_vline(aes(xintercept = -rope))  +
      facet_wrap(~class, scales = "free_x", labeller = fuel_class_labeller) +
      coord_cartesian_panels(
        panel_limits = unnest(select(d, lims), lims)
      ) +
      scale_y_discrete(labels = toupper) +
      labs(x = expression(Load~(Mg%.%ha^-1)), y = "Treatment")
    print(p)
  }
  invisible(d)
}

```

```{r}
#| label: fig-compare-expected
#| fig-cap: 
#|   Differences between expected values for each treatment, with 95% continuous
#|   interval shown.
#| echo: false

expected_contrasts <- predict_expected_contrasts(bf4a, 0)

```

```{r}
#| label: tbl-expected-contrasts
#| tbl-cap: >
#|   Posterior expected predictions of pairwise differences in means, with no
#|   random effects. This reprsents the expected average conditions across all
#|   sites. Units are Mg ha-1. The point estimate is the mode. Upper and lower
#|   limits are the 95% credible intervals. Prob is the probability that the
#|   predicted difference matches the sign of its medianâ€“the probability that it
#|   is not zero.
#| echo: false

expected_contrasts |>
  mutate(
    summary = list(tidybayes::mode_hdci(pred)),
    pred = list(group_by(pred, contrast)),
    prob = list(summarize(pred,
      prob = if_else(median(pred) > 0, 
        mean(pred > rope),
        mean(pred < rope)
      )
    )),
    summary = list(left_join(summary, prob, by = "contrast"))
  ) |> select(summary) |> unnest(c(summary)) |>
  select(contrast, prediction = pred, lower = .lower, upper = .upper, prob) |>
  knitr::kable(digits = 2)

```

#### R squared

```{r}
#| label: tbl-R2
#| tbl-cap: >
#|   This version of R2 is recommended by Gelman et al. (2019). It is defined as
#|   the variance in predictions divided by the variance in predictions plus
#|   the expected variance of the errors.

bf4a |>
mutate(
  R_squared = list(as_tibble(bayes_R2(mod)))
) |>
select(R_squared) |>
unnest(R_squared) |>
knitr::kable(digits = 2)

```

## Final

### Model formula and prior explanation

The final model is described in @eq-bf4a. The description of the meaning of the
priors in the following list should probably be included for explanation.

### fiting and diagnostics

- The actual code used to fit the model in `brms` should be included in an
  appendix. 
- @fig-gamma-pri-post should be included as an example of the connection between
  our priors and posterior distributions.
- We should summarize rhat and tail ess for all models as shown in @tbl-model-summaries
  - R-hat values were all below 1.005
  - Bulk and tail effective sample sizes were generally above 1000, with the
    exception of plot and corner SD's for 1000-hr and Live vegetation fuels,
    which were above 640.
  - These should be adequate effective sample sizes for robust inference.
- @fig-gamma-pp-check should be included to show model fit 
- @tbl-R2 gives an interpretation of R-squared for bayesian models for each of
  our models.

### Results

#### Predicted means

- The posterior effect of treatment is summarized in @fig-expected-predictions.
- For 1-hr fuels, treatments HD and HA show patterns of higher loading. This may
  have to do with more fine fuel inputs from higher retention. This could lead
  to increased fire behavior, which could be of benefit if attempting to burn
  under marginal conditions.
- The HD treatment shows the potential for higher loading of duff and litter.
  This could be due to the same effect as for 1-hr fuels, but the HA treatement
  does not show a similar trend in this case. This could be explained by a
  higher probability of sampling near a trees base, where duff and litter load
  are known to be higher.
- An obvious trend in live vegetation is revealed in the GS treatment, where
  tree sprouts and brush was able to regenerate without any competition. The
  potentially reduced live vegetation in the HA treatment is unexpected.

#### Predicted contrasts

- Pairwise comparison of treatment efffect is sumarized in @fig-compare-expected
- Contrasts take into account how expected predictions co-vary and is a better
  indicato of potential differences between treatments.
- If we take the 95% credible interval as the measure of "significant
  differences" between treatments, then the HA treatment would be considered to
  result in greater fuel load than the GS treatment, and the opposite trend
  would be true for live vegetation: HA is greater than GS.
- The posterior distributions of contrasts reveal more potential differences
  with somewhat reduced certainty for 1-hr, Duff & litter, and Live vegetation.
  - It is also likely that the LD treatment has lower 1-hr fuel loading than HA.
  - Similar to the HA treatment, the HD treatment also appears to have greater
    loading than GS.
  - For Duff & litter, the HD treatment may have greater load than HA
  - For live vegetation, LD may have greater load than HA.
- For 10, 100, and 1000-hr fuels, there is no real detectable differences
  between treatments.


# TODO:

- ICC
