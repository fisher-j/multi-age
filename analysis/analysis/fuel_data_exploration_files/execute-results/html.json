{
  "hash": "ce6f16568a079ab7de0fb8ef08a20629",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Fuel data exploration\n---\n\n::: {.cell}\n\n:::\n\n\nOur main questions regarding fuel loading are:\n\n1. Are there differences in fuel loading between treatments?\n2. If so, what fuel loading components differ, and in which treatments?\n3. What is the magnitude of the difference (for each fuel class) by treatment?\n\nWe have a range of response variables that include loading in several different\nclasses of surface fuels. Some of these may be correlated with each other, which\nmay have an effect on our interpretation of differences between treatments. For\ninstance, if duff-litter load is negatively correlated with vegetation, even if\nwe don't see differences between treatment in either of these variables\nseparately, it might be found that for any given level of vegetation loading, one\ntreatment, or another may have consistently higher levels of duff-litter.\nCapturing differences between treatments in terms of interactions like these\nrequires different tools than assessing any one variable on its own.\n\nIn order to assess potential outliers, co-linearity, interactions, and other\nproblems with statistical tests, we'll first conduct some data exploration as\noutlined by @zuurProtocolDataExploration2010.\n\nFor reference, here is a list of the fuel loading (response) variables of\ninterest.\n\n1. duff/litter load\n2. woody vegetation\n3. herbaceous vegetation\n4. one hr fuels\n5. ten hr fuels\n6. hundred hr fuels\n7. coarse woody debris\n\nIt's important to note that variables 1-3 were measured twice per transect and\nthe rest were measured once per transect. All analysese were conducted at the\ntransect level--variables 1-3 were analyzed at their transect average (average\nof two observations per transect.) This was done to simplify the analysis.\n\n## Basic Summary\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# function to access mostly raw data, but with combined veg and thoushr fuels.\n# and select the output variables (`...`)\nload2 <- function(shape = \"wide\", ...) {\n  load_vars <- c(\"onehr\", \"tenhr\", \"hundhr\", \"dufflitter\", \"thoushr\", \"veg\")\n  tl <- pivot_wider(total_load, names_from = class, values_from = load) |>\n    mutate(\n      thoushr = rowSums(pick(c(thoushr_s, thoushr_r)), na.rm = TRUE),\n      veg = rowSums(pick(c(woody, herb)), na.rm = TRUE),\n      .keep = \"unused\"\n    ) |>\n    mutate(treatment = forcats::fct_relevel(treatment, c(\"gs\", \"ld\", \"ha\", \"hd\")))\n  if (!missing(...)) tl <- select(tl, ...)\n  if (shape == \"long\") {\n    tl <- pivot_longer(tl,\n      -any_of(c(\"site\", \"treatment\", \"corner\", \"azi\")),\n      names_to = \"class\",\n      values_to = \"load\"\n    )\n    load_vars <- load_vars[load_vars %in% tl$class]\n    tl <- mutate(tl, class = factor(class, levels = load_vars))\n  }\n  tl\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nload2(\"long\", everything()) |>\n  filter(!(load > 190 & class == \"veg\" | load > 400 & class == \"thoushr\")) |>\n  ggplot(aes(treatment, load)) +\n  geom_boxplot() +\n  facet_wrap(~class, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![Two outliers were removed to aide in interpretability--600 and 199 t/ha in ha-thoushr and gs-veg, respectively.](fuel_data_exploration_files/figure-html/fig-raw-data-1.png){#fig-raw-data width=672}\n:::\n:::\n\n\nHere is a table summary of our raw data.\n\n\n::: {#tbl-raw-data .cell tbl-cap='Average (sd) transect level load (Mg ha^-1^) for six fuel class categories in four different overstory harvest techniques.'}\n\n```{.r .cell-code}\nspf <- \"%.1f\"\nload2(\"long\", all_of(c(transectid, load_vars))) |>\n  group_by(class, treatment) |>\n  summarize(\n    avg_load = mean(load),\n    sd_load = sd(load)\n  ) |>\n  mutate(\n    load = paste0(sprintf(spf, avg_load), \" (\", sprintf(spf, sd_load), \")\"),\n    .keep = \"unused\"\n  ) |>\n  pivot_wider(names_from = treatment, values_from = load) |>\n  knitr::kable()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'class'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n\n\n|class      |gs          |ld          |ha           |hd          |\n|:----------|:-----------|:-----------|:------------|:-----------|\n|onehr      |0.6 (0.6)   |0.7 (0.5)   |1.2 (0.8)    |1.0 (0.6)   |\n|tenhr      |3.7 (2.8)   |3.4 (2.0)   |3.2 (3.2)    |2.9 (1.6)   |\n|hundhr     |11.8 (8.6)  |9.5 (8.5)   |9.7 (9.0)    |9.5 (7.6)   |\n|dufflitter |48.4 (31.1) |44.9 (18.3) |40.2 (22.0)  |55.0 (28.9) |\n|thoushr    |44.9 (57.0) |35.3 (52.4) |41.6 (105.6) |39.5 (45.1) |\n|veg        |38.2 (42.3) |21.2 (16.0) |12.9 (14.6)  |17.3 (16.1) |\n\n\n:::\n:::\n\n\n## Outliers\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncleavland_plot <- function(data, title, load_var = load) {\n  data |>\n    group_by(site, treatment) |>\n    mutate(replicate_mean_load = mean({{load_var}}, na.rm = TRUE)) |>\n    ggplot(\n      aes(\n        {{load_var}},\n        fct_reorder(\n          interaction(site, treatment, sep = \" \"),\n          {{load_var}},\n          .na_rm = TRUE\n        ),\n        color = treatment\n      )\n    ) +\n    geom_jitter(width = 0, height = 0.2) +\n    labs(x = expression(Load~(Mg%.%ha^-1)), y = \"Data order\", title = title)\n}\n```\n:::\n\n::: {#fig-fwd .cell}\n::: {.cell-output-display}\n![One hour fuels](fuel_data_exploration_files/figure-html/fig-fwd-1.png){#fig-fwd-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Ten hour fuels](fuel_data_exploration_files/figure-html/fig-fwd-2.png){#fig-fwd-2 width=672}\n:::\n\n::: {.cell-output-display}\n![Hundred hour fuels](fuel_data_exploration_files/figure-html/fig-fwd-3.png){#fig-fwd-3 width=672}\n:::\n\nData distribution of loading for fine woody debris classes. Data are sorted by mean loading within each replicate. Jitter has been added to aid in visual interpretation.\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Sum of coarse woody (>7.64 cm, sound and rotten wood combined) fuel loading for transects. The y-axis is sorted by mean CWD loading for each replicate.](fuel_data_exploration_files/figure-html/fig-cwd-1.png){#fig-cwd width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Combined duff and litter loading at each station along trancects. Y-axis is sorted as in @fig-cwd.](fuel_data_exploration_files/figure-html/fig-dufflitter-1.png){#fig-dufflitter width=672}\n:::\n:::\n\n::: {#fig-veg .cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Woody vegetation](fuel_data_exploration_files/figure-html/fig-veg-1.png){#fig-veg-1 width=672}\n:::\n\n::: {.cell-output-display}\n![herbaceous vegetation](fuel_data_exploration_files/figure-html/fig-veg-2.png){#fig-veg-2 width=672}\n:::\n\nVegetation fuel loading for each station along transects, including\n  live and dead fuels attached to live vegetation. Y-axis is sorted as in\n  @fig-cwd.\n:::\n\n\n## Normality {#sec-Normality}\n\nFor further testing, I will summarize the data somewhat, by combining vegetation\nloading (woody and herb), and coarse woody loading (sound and rotten) into just\ntwo loading metrics. Now we have the following response variables:\n\n1. dufflitter\n2. onehr\n3. tenhr\n4. hundhr\n5. thoushr\n6. veg\n\nWhen using manova to test for difference between groups with multiple response\nvariables, it is important that the response variables are multivariate normally\ndistributed. Unfortunately, it would appear that we have a probelem with\nnormality. The raw data for each loading variable is clearly not normally\ndistributed @fig-qqplot-naive.\n\n\n::: {.cell fig-heigth='8'}\n\n```{.r .cell-code}\nmyqqplot <- function(data, var) {\n  data |>\n  ggplot(aes(sample = {{ var }})) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_grid(class ~ treatment, scales = \"free\") +\n  labs(\n    x = \"Theoretical quantiles\", y = \"Sample quantiles\",\n    title = \"Normal Q-Q Plot\"\n  )\n}\n\nload2(\"long\", treatment, all_of(load_vars)) |>\n  myqqplot(load)\n```\n\n::: {.cell-output-display}\n![Naive qq plot of loading variables. This doesn't take into the fact that our data is nested. You could say this is based on a simple model where all observations are independent.](fuel_data_exploration_files/figure-html/fig-qqplot-naive-1.png){#fig-qqplot-naive width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# This code creates a dataframe with nested columns that include the original\n# data and calculated coordinates of a superimposed normal distribution.\nbins <- 16\nhist_dat <- load2(\"long\", treatment, all_of(load_vars)) |>\n  drop_na() |>\n  # Facet grid each column (class) has same scale, find limits to calculate bin\n  # width, limits are either implied by the constructed normal curce, or the raw\n  # data\n  group_by(class) |>\n  mutate(\n    xmin = min(c(mean(load) - 3 * sd(load), load)),\n    xmax = max(c(mean(load) + 3 * sd(load), load))\n  ) |>\n  group_by(treatment, class) |>\n  nest(data = load) |>\n  # generate data for a normal curve with mean and sd from observed data to\n  # cover 3 sd.\n  mutate(\n    norm_x = map(data, function (d) {\n      seq(\n        from = mean(d$load) - 3 * sd(d$load), \n        to = mean(d$load) + 3 * sd(d$load), \n        length.out = 100\n      )\n    }),\n    # scale curve to expected binwidth based on plot layout (same x scale across\n    # all fuel classes) multiplied by the number of observations. The histogram\n    # and normal curve should represent the same total area.\n    norm_y = map(data, function (d) {\n      dens <- dnorm(unlist(norm_x), mean = mean(d$load), sd = sd(d$load))\n      dens * ((xmax - xmin) / bins) * nrow(d)\n    })\n  )\n\n# # here is the code to plot the above data. I decided not to use this after all,\n# # but it was a lot of work, so I'm keeping it for posterity\n# ggplot(filter(hist_dat, class %in% load_vars[1:3] )) +\n#   geom_histogram(data = \\(x) unnest(x, data), aes(x = load), bins = bins) +\n#   geom_line(data = \\(x) unnest(x, c(norm_x, norm_y)), aes(norm_x, norm_y)) +\n#   facet_grid(treatment ~ class, scales = \"free\") +\n#   labs(y = \"count\", x = \"Load Mg/ha\")\n#\n# ggplot(filter(hist_dat, class %in% load_vars[4:6] )) +\n#   geom_histogram(data = \\(x) unnest(x, data), aes(x = load), bins = bins) +\n#   geom_line(data = \\(x) unnest(x, c(norm_x, norm_y)), aes(norm_x, norm_y)) +\n#   facet_grid(treatment ~ class, scales = \"free\") +\n#   labs(y = \"count\", x = \"Load Mg/ha\")\n```\n:::\n\n::: {#fig-hist .cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nwalk(list(c(1:3), c(4:6)),\n  function(x) {\n    p <- load2(\"long\") |>\n      filter(class %in% load_vars[x]) |>\n      ggplot(aes(load)) +\n      geom_histogram(bins = 16) +\n      facet_grid(treatment ~ class, scales = \"free\") +\n      labs(y = \"count\", x = \"Load Mg/ha\")\n    print(p)\n  }\n)\n```\n\n::: {.cell-output-display}\n![](fuel_data_exploration_files/figure-html/fig-hist-1.png){#fig-hist-1 width=672}\n:::\n\n::: {.cell-output-display}\n![](fuel_data_exploration_files/figure-html/fig-hist-2.png){#fig-hist-2 width=672}\n:::\n\nHistotrams for the fuel loading response variables.\n:::\n\n\n### Box-Cox transformation {#sec-box-cox-transformation}\n\nI would like to look at the effect of this transformation on the response data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxcox <- function(y, lambda1, lambda2) {\n  if (lambda1 == 0) {\n    log(y + lambda2)\n  } else {\n    ((y + lambda2)^lambda1 - 1) / lambda1\n  }\n}\n\nload2(\"long\", treatment, all_of(load_vars)) |>\n  group_by(class) |> nest() |> rowwise() |>\n  mutate(\n    lambda = list(suppressMessages(geoR::boxcoxfit(data$load, lambda2 = TRUE)$lambda)),\n    load_bc = list(boxcox(data$load, lambda[1], lambda[2]))\n  ) |>\n  unnest(c(data, load_bc)) |> ungroup() |>\n  myqqplot(load_bc)\n```\n\n::: {.cell-output-display}\n![](fuel_data_exploration_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nWhile, this looks somewhat more normal, the zeros end up being a little strange.\nI applied a separte transformation for each fuel class, but all treatments\nwithin a fuel class have the same transformation.\n\nFor MANOVA we are concered with the within group multivariate normality, the\nassumption does not appear to be met here either (@fig-qqplot-model). The code\noutput below indicates the rows with the greatest deviation from normal.\n\n\n::: {#fig-qqplot-model .cell}\n\n```{.r .cell-code}\ntl <- load2(\"wide\", treatment, all_of(load_vars))\nload_mod <- lm(as.matrix(tl[-1]) ~ treatment, data = tl)\nm_dist <- heplots::cqplot(load_mod)\n\nload2(\"wide\", everything())[order(m_dist, decreasing = TRUE)[1:10], ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 10\n   site    treatment corner   azi dufflitter onehr  tenhr hundhr thoushr    veg\n   <chr>   <fct>     <chr>  <dbl>      <dbl> <dbl>  <dbl>  <dbl>   <dbl>  <dbl>\n 1 waldos  ha        nw        90       23.2 1.27  10.7    21.2    593.   16.2 \n 2 waldos  gs        ne       270      131.  1.72   6.46   16.1     36.3 199.  \n 3 whiskey ha        n        225       27.1 2.76  13.3    45.2     32.1   8.5 \n 4 waldos  gs        se       270       54.1 0.504 11.6    34.8    280.    0.9 \n 5 whiskey gs        n        225       27.1 0.124  2.09    8.56     0   121.  \n 6 waldos  ha        se       270      112.  2.32   0.930   2.54     0    46.1 \n 7 waldos  ha        se         0       15.5 3.34   6.53   13.7     17.7   6.31\n 8 waldos  gs        se         0       38.7 1.08   7.31   37.3     17.6   4.23\n 9 waldon  gs        e        225      112.  0.711  8.98   14.7     89.6  34.6 \n10 whiskey ld        s        315       77.3 0.390  5.38   31.9     59.8  74.2 \n```\n\n\n:::\n\n```{.r .cell-code}\ntreatments = c(\"gs\", \"ha\", \"ld\", \"hd\")\npar(mfrow = c(2, 2))\ninvisible(lapply(treatments, \n  \\(x) heplots::cqplot(filter(tl, treatment == x)[-1], main = x)\n))\n```\n\n::: {.cell-output-display}\n![](fuel_data_exploration_files/figure-html/fig-qqplot-model-1.png){#fig-qqplot-model-1 width=672}\n:::\n\n::: {.cell-output-display}\n![](fuel_data_exploration_files/figure-html/fig-qqplot-model-2.png){#fig-qqplot-model-2 width=672}\n:::\n\nPlot A assessed the multivariate normality of residuals given the model where all loading variabels are a function of the treatment group.\n\n:::\n\n\nA number of different tests of multivariate normaliy also confirm the lack of\nevidence for meeting this assumption (@tbl-norm-test).\n\n\n::: {#tbl-norm-test .cell tbl-cap='Several different tests of multivariate normality indicate a lack of evidence to support this assumption.'}\n\n```{.r .cell-code}\nall_mvn_tests <- function(data) {\n  c(\"mardia\", \"hz\", \"royston\", \"dh\", \"energy\") |>\n  map(\\(x) MVN::mvn(data = data, subset = \"treatment\", mvnTest = x)) |>\n  map(\\(x) x$multivariateNormality) |>\n  map(\\(x) bind_rows(x, .id = \"treatment\")) |>\n  map_dfr(\\(x) \n    filter(x, Test != \"MVN\") |> \n    mutate( across(where(is.factor), \\(f) as.numeric(as.character(f)))) |> \n    select(1:2, statistic = 3, `p value`, Result = last_col())\n  )\n}\n\nall_mvn_tests(load2(\"wide\", treatment, all_of(load_vars))) |>\n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|treatment |Test            | statistic| p value|Result |\n|:---------|:---------------|---------:|-------:|:------|\n|gs        |Mardia Skewness |  156.2386|  0.0000|NO     |\n|gs        |Mardia Kurtosis |    4.1306|  0.0000|NO     |\n|ld        |Mardia Skewness |  122.0012|  0.0000|NO     |\n|ld        |Mardia Kurtosis |    3.2534|  0.0011|NO     |\n|ha        |Mardia Skewness |  198.9055|  0.0000|NO     |\n|ha        |Mardia Kurtosis |    5.8903|  0.0000|NO     |\n|hd        |Mardia Skewness |   85.1671|  0.0072|NO     |\n|hd        |Mardia Kurtosis |    0.7268|  0.4673|YES    |\n|gs        |Henze-Zirkler   |    1.1979|  0.0000|NO     |\n|ld        |Henze-Zirkler   |    1.1036|  0.0001|NO     |\n|ha        |Henze-Zirkler   |    1.2792|  0.0000|NO     |\n|hd        |Henze-Zirkler   |    1.0978|  0.0001|NO     |\n|gs        |Royston         |   79.7755|  0.0000|NO     |\n|ld        |Royston         |   65.9480|  0.0000|NO     |\n|ha        |Royston         |  105.5921|  0.0000|NO     |\n|hd        |Royston         |   51.6106|  0.0000|NO     |\n|gs        |Doornik-Hansen  |   45.8165|  0.0000|NO     |\n|ld        |Doornik-Hansen  |   38.4513|  0.0001|NO     |\n|ha        |Doornik-Hansen  |   57.3566|  0.0000|NO     |\n|hd        |Doornik-Hansen  |   65.4678|  0.0000|NO     |\n|gs        |E-statistic     |    1.9038|  0.0000|NO     |\n|ld        |E-statistic     |    1.7881|  0.0000|NO     |\n|ha        |E-statistic     |    2.1409|  0.0000|NO     |\n|hd        |E-statistic     |    1.6312|  0.0000|NO     |\n\n\n:::\n:::\n\n\n### Other distributions {#sec-other-distributions}\n\nIf our data is not normally distributed, then what distribution is it? I'm going\nto assume what we are interested in the distribution of data within groups\n(treatments).\n\nI attemped to model the distribution of our conditional response data (fuel size\nclass by treatment), but it mostly didn't work.\n\nOur data is non-negative (contains zeros) continuous (for the most part) and\nhighly variable in terms of skew and kurtosis. The presence of zeros, makes\nusing the Gamma distribution more difficult. One possibility is a hurdle gamma,\nor  Zero-adjusted Gamma. \n\nThe following show where our data lies in terms of kurtosis and skewness\ncompared to other common distributions. It looks somewhat Gamma-ish?\n\n\n::: {#fig-skew-kurt .cell .column-page layout-ncol=\"2\"}\n\n```{.r .cell-code}\nd <- load2(\"long\", treatment, all_of(load_vars)) |> \n  split(~class) |> map(~split(.x, ~treatment))\n\n# imap_dfr(d, \\(x, y) tibble(class = y, treatment = names(x) )) |>\n#   with(paste0(\"Treatment \", treatment, \", fuel class \", class)) |>\n#   cat(sep = \"\\n\")\n\n# par(mfrow = c(2,2))\nwalk(d, ~ walk(.x, \\(x) fitdistrplus::descdist(x$load, boot = 111)))\n```\n\n::: {.cell-output-display}\n![Treatment gs, fuel class onehr](fuel_data_exploration_files/figure-html/fig-skew-kurt-1.png){#fig-skew-kurt-1 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment ha, fuel class onehr](fuel_data_exploration_files/figure-html/fig-skew-kurt-2.png){#fig-skew-kurt-2 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment hd, fuel class onehr](fuel_data_exploration_files/figure-html/fig-skew-kurt-3.png){#fig-skew-kurt-3 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment ld, fuel class onehr](fuel_data_exploration_files/figure-html/fig-skew-kurt-4.png){#fig-skew-kurt-4 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment gs, fuel class tenhr](fuel_data_exploration_files/figure-html/fig-skew-kurt-5.png){#fig-skew-kurt-5 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment ha, fuel class tenhr](fuel_data_exploration_files/figure-html/fig-skew-kurt-6.png){#fig-skew-kurt-6 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment hd, fuel class tenhr](fuel_data_exploration_files/figure-html/fig-skew-kurt-7.png){#fig-skew-kurt-7 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment ld, fuel class tenhr](fuel_data_exploration_files/figure-html/fig-skew-kurt-8.png){#fig-skew-kurt-8 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment gs, fuel class hundhr](fuel_data_exploration_files/figure-html/fig-skew-kurt-9.png){#fig-skew-kurt-9 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment ha, fuel class hundhr](fuel_data_exploration_files/figure-html/fig-skew-kurt-10.png){#fig-skew-kurt-10 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment hd, fuel class hundhr](fuel_data_exploration_files/figure-html/fig-skew-kurt-11.png){#fig-skew-kurt-11 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment ld, fuel class hundhr](fuel_data_exploration_files/figure-html/fig-skew-kurt-12.png){#fig-skew-kurt-12 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment gs, fuel class dufflitter](fuel_data_exploration_files/figure-html/fig-skew-kurt-13.png){#fig-skew-kurt-13 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment ha, fuel class dufflitter](fuel_data_exploration_files/figure-html/fig-skew-kurt-14.png){#fig-skew-kurt-14 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment hd, fuel class dufflitter](fuel_data_exploration_files/figure-html/fig-skew-kurt-15.png){#fig-skew-kurt-15 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment ld, fuel class dufflitter](fuel_data_exploration_files/figure-html/fig-skew-kurt-16.png){#fig-skew-kurt-16 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment gs, fuel class thoushr](fuel_data_exploration_files/figure-html/fig-skew-kurt-17.png){#fig-skew-kurt-17 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment ha, fuel class thoushr](fuel_data_exploration_files/figure-html/fig-skew-kurt-18.png){#fig-skew-kurt-18 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment hd, fuel class thoushr](fuel_data_exploration_files/figure-html/fig-skew-kurt-19.png){#fig-skew-kurt-19 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment ld, fuel class thoushr](fuel_data_exploration_files/figure-html/fig-skew-kurt-20.png){#fig-skew-kurt-20 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment gs, fuel class veg](fuel_data_exploration_files/figure-html/fig-skew-kurt-21.png){#fig-skew-kurt-21 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment ha, fuel class veg](fuel_data_exploration_files/figure-html/fig-skew-kurt-22.png){#fig-skew-kurt-22 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment hd, fuel class veg](fuel_data_exploration_files/figure-html/fig-skew-kurt-23.png){#fig-skew-kurt-23 width=480}\n:::\n\n::: {.cell-output-display}\n![Treatment ld, fuel class veg](fuel_data_exploration_files/figure-html/fig-skew-kurt-24.png){#fig-skew-kurt-24 width=480}\n:::\n\nSkewness and kurtosis for fuel classes within treatments.\n:::\n\n\n### Zero-adjusted Gamma\n\nI'll try using the gamlss package, which fits models \"where all the parameters\nof the assumed distribution for the response can be modelled as additive\nfunctions of the explanatory variables.\" \n\nI'm not really sure how it works, but I know if will allow me to fit a model\nassuming a gamma distribution, while modeling the zeros separately. First I fit\na model, then I get the estimated distribution parameters, then I plot the\ndensity curve over a histrogram (scaled to density). This looks promising to me.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Zero adjusted Gamma distribution fit to histograms\nplot_zaga <- function(i, d) {\n  m1 <- gamlss::gamlssML(d, family = gamlss.dist::ZAGA())\n  hist(d, prob = TRUE, main = i, breaks = 20)\n  curve(gamlss.dist::dZAGA(x, mu = m1$mu, sigma = m1$sigma, nu = m1$nu), from = min(d), to = max(d), add = TRUE)\n}\n\nplot_zaga_class <- function(data, class_name) {\n  if(missing(class_name)) {\n    class_name <- as.character(substitute(data))\n    class_name <- class_name[length(class_name)]\n  }\n  par(mfrow = c(2,2))\n  iwalk(data, ~plot_zaga(.y, .x))\n  mtext(class_name, cex = 1.6, side = 3, line = -2, outer = TRUE)\n}\n\nd2 <- d |> map(~ map(.x, \"load\"))\niwalk(d2, ~plot_zaga_class(.x, .y))\n```\n\n::: {.cell-output-display}\n![](fuel_data_exploration_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](fuel_data_exploration_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](fuel_data_exploration_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](fuel_data_exploration_files/figure-html/unnamed-chunk-6-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](fuel_data_exploration_files/figure-html/unnamed-chunk-6-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](fuel_data_exploration_files/figure-html/unnamed-chunk-6-6.png){width=672}\n:::\n:::\n\n\n### Poisson fit of count data {#sec-poisson-fit-of-count-data}\n\nAll of the woody debris can be viewed as count data, and mean diameter. The mean\ndiameter implies a distribution, which we actually have for coarse woody, but\nnot for FWD.\n\nI wonder If I can model these counts as a Poisson process.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncwd_counts <- cwd |> \n  group_by(site, treatment, corner, azi) |>\n  summarize(count = sum(count), .groups = \"drop\")\n\npar(mfrow = c(2,2))\ncwd_counts |>\n  group_by(treatment) |>\n  group_walk( function (data, group) {\n    n <- data$count\n    hist(n, main = group, prob = TRUE)\n    lines(0:max(n), dpois(0:max(n), mean(n)))\n  })\n```\n\n::: {.cell-output-display}\n![](fuel_data_exploration_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThat didn't look so great, so I also tried using a Zero-inflated Poisson\ndistribution, but the model fit an extremely small value for the parameter that\ncontrols the probability of zero (referred to here as sigma) and so was\neffectively the same as the Poisson fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,2))\ncwd_counts |> \n  group_by(treatment) |> nest() |>\n  transmute(\n    sigma = map_dbl(data, \n      ~gamlss::gamlssML(.x$count, family = gamlss.dist::ZIP())$sigma\n    )\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n# Groups:   treatment [4]\n  treatment    sigma\n  <chr>        <dbl>\n1 gs        2.61e-10\n2 ha        1.61e-10\n3 hd        2.42e-10\n4 ld        2.71e-10\n```\n\n\n:::\n:::\n\n\n## Homogeneity of variance {#sec-homogeneity-of-variance}\n\nThere seem to be some pretty big differences in the variance between treatments.\nThis is likely to do with outliers. For linear regression, it is recommended\nthat maximum variance ration should be below 4.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmax_var <- load2(\"long\", everything()) |>\n  group_by(class, treatment) |>\n  summarize(var = sd(load)^2, load = max(load), .groups = \"drop_last\") |>\n  summarize(\n    max_var_rat = paste(\"Max. var. ratio: \", round(max(var) / min(var))),\n    x = 1.1, y = max(load) * 1.1, .groups = \"drop\")\n\nload2(\"long\", everything()) |>\n  ggplot(aes(treatment, load)) +\n  geom_boxplot() +\n  geom_text(data = max_var, aes(x, y, label = max_var_rat, hjust = \"inward\")) +\n  facet_wrap(~class, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](fuel_data_exploration_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Zeros\n\nWe do have zeros, which is important if we want to employ a glm like Gamma,\nwhich is only defined for positive values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload2(\"long\", everything()) |>\n  group_by(class, treatment) |>\n  summarize(zeros = sum(load == 0), percent = zeros / n(), .groups = \"drop\") |>\n  ggplot(aes(treatment, percent)) +\n  geom_col() +\n  geom_text(\n    aes(label = if_else(zeros == 0, NA, zeros), y = percent / 2),\n    color = \"gray70\", na.rm = TRUE\n  ) +\n  facet_wrap(~class) +\n  scale_y_continuous(labels = scales::percent)\n```\n\n::: {.cell-output-display}\n![](fuel_data_exploration_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Correlation of response variables\n\nI'm not sure if it's important, but I was curious if the various fuel loading\nclasses were correlated with each other. Either across the board, or within a\ngiven treatment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressMessages(GGally::ggpairs(load2(\"wide\", all_of(load_vars))))\n```\n\n::: {.cell-output-display}\n![Correlation among the response variables (fuel classes).](fuel_data_exploration_files/figure-html/fig-response-corr-1.png){#fig-response-corr width=672}\n:::\n:::\n\n\n## Independence\n\nBecause of how are data were collected they are not independent. The current data\nis summarized at the transect level. At that level. We have two transects at\neach corner. Because of spatial autocorrelation, these may be correlated with\neach other. Corners (and thus transects) are nested within plots, and plots are\nwithin treatments. Each plot received a different treatment. What I'm not clear\nabout is: should I include a random variable for plots, if I'm including a\nfixed effect for treatment?\n\nThere are also question about at what level to summarize/model the data. I've\nalready averaged stations within transects for several variable that were\ncollected at the station level (two within each transect). What are the\ntrade-offs for either averaging at the corner level, or alternatively,\nanalyzing our raw station data instead of averaging.\n\n",
    "supporting": [
      "fuel_data_exploration_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}